{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Race model\n",
    "> May 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp multi_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethos/git/gh-pages-example/gh_pages_example/model_utils.py:299: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if (ind not in allowed_inds) and (str(ind) not in allowed_inds):\n",
      "/home/ethos/git/gh-pages-example/gh_pages_example/methods.py:260: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  ergodic = np.array(V.transpose(0, 2, 1)[y], dtype=float)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| export\n",
    "from gh_pages_example.conditions import *\n",
    "from gh_pages_example.data_utils import *\n",
    "from gh_pages_example.methods import *\n",
    "from gh_pages_example.models import *\n",
    "from gh_pages_example.model_utils import *\n",
    "from gh_pages_example.payoffs import *\n",
    "from gh_pages_example.plot_utils import *\n",
    "from gh_pages_example.types import *\n",
    "from gh_pages_example.utils import *\n",
    "\n",
    "import copy\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import chaospy\n",
    "import fastcore.test\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from nbdev.showdoc import *\n",
    "import nptyping\n",
    "import numpy as np\n",
    "import pandas\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that $W1/s1 < W2/s2  < W1 < W2$, we can mark which recurrent states lead to each lab type being safe.\n",
    "\n",
    "L1 safe when:\\\n",
    "AS-AS\\\n",
    "AU-AS\\\n",
    "S1-AS\\\n",
    "S2-AS\\\n",
    "AS-S1\\\n",
    "S1-S1\n",
    "\n",
    "Intuitively, L1 cannot be safe if they choose AU or S2 (they always make the breakthrough first and do so unsafely). When choosing S1, they are only safe if they can make their breakthrough before L2, which given our above constraints requires that L2 is safe in their first phase.\n",
    "\n",
    "L2 safe when:\\\n",
    "AS-AS\\\n",
    "AS-S1\\\n",
    "AS-S2\\\n",
    "AS-AU\\\n",
    "S2-AU\\\n",
    "S2-S2\n",
    "\n",
    "Intuitively, L2 cannot be safe if they choose AU or S1 (when choosing S1, they cannot reach their breakthrough before L2 given our selected region of the parameter space, so will always switch to being unsafe). When choosing S2, they will only switch to be safe if L1 reaches their breakthrough first, which requires them to be unsafe in the first phase.\n",
    "\n",
    "Notice that only the following recurrent states lead to both lab types being safe:\\\n",
    "AS-AS\\\n",
    "AS-S1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose the following method of aggregating the harms:\n",
    "$\n",
    "Harm = Pr(both safe) * 0 + Pr(L1 safe only) * (1-p_2) * B2 / (B1 + B2) + Pr(L2 safe only) * (1-p_1) * B1 / (B1 + B2) + pr(both unsafe) * (1-p_{both})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def market_share(t_1, t_2, gamma):\n",
    "   \"\"\"Compute market share of the player who arrives at the discovery at time `t_1`.\"\"\"\n",
    "   dt = np.abs(t_1 - t_2)\n",
    "   market_shares = np.where(t_2 <= t_1,\n",
    "                   0.5*gamma**dt,\n",
    "                   1 - 0.5*gamma**dt)\n",
    "   return market_shares\n",
    "def market_value(b, delta, t, w):\n",
    "   \"\"\"Compute the net present value of the market with monetary value `b` to a\n",
    "   player who enters the market at time `t` given their discount rate, `delta`.\n",
    "   \n",
    "   A negative delta is nonsensical so we simply assume a very large benefit\n",
    "   no matter the value of t. Likewise if delta=1, instead of assuming infinite\n",
    "   payoffs, we assume a large value for the prize, capped at b*100\"\"\"\n",
    "   return np.where((0 < delta) & (delta < 1),\n",
    "                  #  b * delta**t / (1 - delta),\n",
    "                  #  100 * b,\n",
    "                  w / t * b,\n",
    "                  w / t * b)\n",
    "def catch_up_fn(t, w, s_p1, s_p2):\n",
    "   \"\"\"Compute the spillover adjusted arrival time if a player from the other\n",
    "   layer makes a breakthrough at time `t`.\"\"\"\n",
    "   return np.where(s_p2 > 0,\n",
    "                   (t + ((w - t * s_p1) / s_p2)),\n",
    "                   1e100)\n",
    "\n",
    "def build_payoffs_multi_race_v1(models):\n",
    "   \"\"\" \n",
    "   Builds the payoffs for the multi race model of AI development.\n",
    "   \n",
    "   Note: This model assumes only two layers interact with one another at a time.\n",
    "   Currently, we've hardcoded these parameters, but in future we could refactor\n",
    "   the parameters to be 2d arrays instead of 1d arrays and use them as needed.\n",
    "\n",
    "   Note: The interaction should only be between two layers at a time.\n",
    "   More complex interactions should be decomposed into simpler ones.\n",
    "   \"\"\"\n",
    "   # Extract the parameters from the models dictionary.\n",
    "   names1 = [\"B_1\", \"B_2\", \"s_1\", \"s_2\", \"p_1\", \"p_2\", \"p_both\"]\n",
    "   names2 = [ \"W_1\", \"W_2\", \"gamma_1\", \"gamma_2\", \"delta_1\", \"delta_2\", \"alpha_1\", \"alpha_2\"]\n",
    "   B_1, B_2, s_1, s_2, p_1, p_2, p_both = [models[k] for k in names1]\n",
    "   W_1, W_2, gamma_1, gamma_2, delta_1, delta_2, alpha_1, alpha_2  = [models[k] for k in names2]\n",
    "   profile = models[\"profile\"]\n",
    "   strategy_profile = [int(a) for a in profile.split(\"-\")][::-1]\n",
    "   M = np.shape(B_1)[0]\n",
    "   N = len(strategy_profile)\n",
    "   \n",
    "   # The following collections of parameters are specified per strategy. \n",
    "   # Each layer allows the following strategies:\n",
    "   # safe, unsafe, safe_phase_1, safe_phase_2\n",
    "   mapping_s_to_irt = [W_1, W_1/s_1, W_1, W_1/s_1,\n",
    "                       W_2, W_2/s_2, W_2, W_2/s_2] # short for mapping_strategy_to_isolated_research_time\n",
    "   ones = np.ones(M)\n",
    "   mapping_s_to_phase1_speed = [ones, s_1, ones, s_1,\n",
    "                                ones, s_2, ones, s_2]\n",
    "   mapping_s_to_phase2_speed = [alpha_1, alpha_1 * s_1, alpha_1 * s_1, alpha_1,\n",
    "                                alpha_2, alpha_2 * s_2, alpha_2 * s_2, alpha_2]\n",
    "   maping_s_to_w = [W_1, W_1, W_1, W_1,\n",
    "                    W_2, W_2, W_2, W_2]\n",
    "   mapping_s_to_gamma = [gamma_1, gamma_1, gamma_1, gamma_1,\n",
    "                         gamma_2, gamma_2, gamma_2, gamma_2]\n",
    "   mapping_s_to_delta = [delta_1, delta_1, delta_1, delta_1,\n",
    "                         delta_2, delta_2, delta_2, delta_2]\n",
    "   mapping_s_to_b = [B_1, B_1, B_1, B_1,\n",
    "                     B_2, B_2, B_2, B_2]\n",
    "   mapping_s_to_alpha = [alpha_1, alpha_1, alpha_1, alpha_1,\n",
    "                         alpha_2, alpha_2, alpha_2, alpha_2]\n",
    "   \n",
    "   # The following collection of parameters are specified per player\n",
    "   # We usually do this to make broadcasting our functions over arrays easier\n",
    "   isolated_research_times = np.array([mapping_s_to_irt[a - 1] for a in strategy_profile]).T\n",
    "   phase_1_speeds = np.array([mapping_s_to_phase1_speed[a - 1] for a in strategy_profile]).T\n",
    "   phase_2_speeds = np.array([mapping_s_to_phase2_speed[a - 1] for a in strategy_profile]).T\n",
    "   research_distances = np.array([maping_s_to_w[a - 1] for a in strategy_profile]).T\n",
    "   contestability_rates = np.array([mapping_s_to_gamma[a - 1] for a in strategy_profile]).T\n",
    "   discount_rates = np.array([mapping_s_to_delta[a - 1] for a in strategy_profile]).T\n",
    "   benefits = np.array([mapping_s_to_b[a - 1] for a in strategy_profile]).T\n",
    "   safety_boosts = np.array([mapping_s_to_alpha[a - 1] for a in strategy_profile]).T\n",
    "\n",
    "   # Define layer membership for each player\n",
    "   layer_membership = np.zeros((M, N))\n",
    "   for i, a in enumerate(strategy_profile):\n",
    "      # First 4 strategies belong to layer 1, the rest belong to layer 2\n",
    "      layer_membership[:, i] = 1 if a > 4 else 0\n",
    "\n",
    "   # Determine leader and leader arrival time\n",
    "   leader = np.argmin(isolated_research_times, axis=1)\n",
    "   is_leader = np.zeros((M, N), dtype=bool)\n",
    "   is_leader[np.arange(M), leader] = True\n",
    "   leader_layer = layer_membership[np.arange(M), leader][:, None]\n",
    "   arrival_time_leader = np.min(isolated_research_times, axis=1, keepdims=True)\n",
    "   \n",
    "\n",
    "   # Compute times until each player achieves their AI breakthrough given their strategies\n",
    "   arrival_times = isolated_research_times\n",
    "   on_late_layer = layer_membership != leader_layer\n",
    "   on_leader_layer = layer_membership == leader_layer\n",
    "   \n",
    "   catch_up_times =  catch_up_fn(arrival_time_leader,\n",
    "                                 research_distances,\n",
    "                                 phase_1_speeds,\n",
    "                                 phase_2_speeds)\n",
    "   arrival_times = np.where(on_late_layer, catch_up_times, arrival_times)\n",
    "   arrival_times_follower_layer = arrival_times[on_late_layer].reshape((M, 2))\n",
    "   \n",
    "   # Revise arrival times for players on the leading layer who are behind the other layer   \n",
    "   arrival_time_follower = np.min(arrival_times_follower_layer, axis=1, keepdims=True)\n",
    "   after_follower = arrival_times > arrival_time_follower\n",
    "   catch_up_times_crossover = catch_up_fn(arrival_time_follower,\n",
    "                                          research_distances,\n",
    "                                          phase_1_speeds,\n",
    "                                          phase_2_speeds)\n",
    "   arrival_times = np.where(on_leader_layer & after_follower, catch_up_times_crossover, arrival_times)\n",
    "   \n",
    "   # Compute times until each player would achieve a safe AI given their strategies\n",
    "   safe_times = research_distances\n",
    "   safe_time_catch_up = catch_up_fn(arrival_time_leader,\n",
    "                                    research_distances,\n",
    "                                    phase_1_speeds,\n",
    "                                    safety_boosts)\n",
    "   safe_times = np.where(~on_leader_layer, safe_time_catch_up, safe_times)\n",
    "   # Revise safe times for players on the leading layer who are behind the other layer   \n",
    "   safe_times_crossover = catch_up_fn(arrival_time_follower,\n",
    "                                      research_distances,\n",
    "                                      phase_1_speeds,\n",
    "                                      safety_boosts)\n",
    "   safe_times = np.where(on_leader_layer & after_follower, safe_times_crossover, safe_times)\n",
    "   \n",
    "   # Record which players and layers are unsafe\n",
    "   unsafe_players = safe_times > arrival_times\n",
    "   unsafe_layer1 = np.any(unsafe_players[:, layer_membership[0, :] == 0], axis=1, keepdims=True)\n",
    "   unsafe_layer2 = np.any(unsafe_players[:, layer_membership[0, :] == 1], axis=1, keepdims=True)\n",
    "   \n",
    "   # Match arrival times to the rival's arrival times for calculating market share\n",
    "   rival_indices = [i+1 if i % 2 == 0 else i-1 for i in range(N)]\n",
    "   arrival_times_rival = arrival_times[:, rival_indices]\n",
    "   \n",
    "   # Determine harms for each player\n",
    "   unsafe_scenario_1 = unsafe_players & unsafe_layer1 & ~unsafe_layer2\n",
    "   unsafe_scenario_2 = unsafe_players & ~unsafe_layer1 & unsafe_layer2\n",
    "   unsafe_scenario_3 = unsafe_players & unsafe_layer1 & unsafe_layer2\n",
    "   harms = np.zeros((M, N))\n",
    "   harms = np.where(unsafe_scenario_1, 1 - p_1[:, None], harms)\n",
    "   harms = np.where(unsafe_scenario_2, 1 - p_2[:, None], harms)\n",
    "   harms = np.where(unsafe_scenario_3, 1 - p_1[:, None] * p_2[:, None], harms)\n",
    "   \n",
    "   # Given arrival times and safe times, as well as the equations for market\n",
    "   # value, market share, and risk, we can now compute the payoffs.\n",
    "   market_shares = market_share(arrival_times, arrival_times_rival, contestability_rates)\n",
    "   market_values = market_value(benefits, discount_rates, arrival_times, research_distances)\n",
    "   payoff_values = market_shares * market_values * (1 - harms)\n",
    "\n",
    "   # Helpful assertions:\n",
    "   \n",
    "   # Check that it is impossible for any player to have made their breakthrough\n",
    "   # in phase 1 without covering their research distance (as this must be true\n",
    "   # by construction, this only happens if we have a bug in the code).\n",
    "   \n",
    "   assert np.all(research_distances >= np.round(arrival_time_leader * phase_1_speeds, 3))\n",
    "   \n",
    "   payoffs = {f\"P{i+1}\": payoff_values[:, i] for i in range(N)}\n",
    "   return {\"payoffs\": payoffs,\n",
    "           \"arrival_times\": arrival_times,\n",
    "           \"safe_times\": safe_times}\n",
    "\n",
    "@method(build_payoffs, \"multi-race-v1\")\n",
    "def build_payoffs(models):\n",
    "   \"\"\"Payoff matrix for the Multi Race game.\"\"\"\n",
    "   profiles = models[\"profiles_filtered\"]\n",
    "\n",
    "   models[\"payoffs\"] = {}\n",
    "   models[\"arrival_times\"] = {}\n",
    "   models[\"safe_times\"] = {}\n",
    "   for profile in tqdm.tqdm(profiles):\n",
    "      models[\"profile\"] = profile\n",
    "      # Build payoffs for the current profile\n",
    "      data = build_payoffs_multi_race_v1(models)\n",
    "      models[\"payoffs\"][profile] = data[\"payoffs\"]\n",
    "      models[\"arrival_times\"][profile] = data[\"arrival_times\"]\n",
    "      models[\"safe_times\"][profile] = data[\"safe_times\"]\n",
    "\n",
    "   return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_harm_baseline(models):\n",
    "    \"\"\"Compute a random harm baseline for comparison\"\"\"\n",
    "    names = [\"p_1\", \"p_2\", \"p_both\", \"B_1\", \"B_2\", \"recurrent_states\"]\n",
    "    p_1, p_2, p_both, B_1, B_2, recurrent_states = [models[k] for k in names]\n",
    "    # l1_safe = [\"AS-AS\", \"AU-AS\", \"S1-AS\", \"S2-AS\", \"AS-S1\", \"S1-S1\"]\n",
    "    l1_safe = [\"5-1\", \"6-1\", \"7-1\", \"8-1\", \"5-3\", \"6-3\"]\n",
    "    # l2_safe = [\"AS-AS\", \"AS-S1\", \"AS-S2\", \"AS-AU\", \"S2-AU\", \"S2-S2\"]\n",
    "    l2_safe = [\"5-1\", \"5-3\", \"5-4\", \"5-2\", \"8-2\", \"8-4\"]\n",
    "    both_safe = set(l1_safe) & set(l2_safe)\n",
    "    l1_safe_only = set(l1_safe) - both_safe\n",
    "    l2_safe_only = set(l2_safe) - both_safe\n",
    "    both_unsafe = set(recurrent_states) - set(l1_safe) - set(l2_safe)\n",
    "    S = len(recurrent_states)\n",
    "    S_L1 = len(l1_safe_only)\n",
    "    S_L2 = len(l2_safe_only)\n",
    "    S_0 = len(both_unsafe)\n",
    "    harm_baseline = (S_L1/S * (1 - p_2) * B_2 / (B_1 + B_2)\n",
    "                     + S_L2/S * (1 - p_1) * B_1 / (B_1 + B_2)\n",
    "                     + S_0/S * (1 - p_both))\n",
    "    return {**models, \"harm_baseline\": harm_baseline}\n",
    "\n",
    "def compute_harm(models):\n",
    "    \"\"\"Compute the harm given the frequency of each state.\"\"\"\n",
    "    names = [\"p_1\", \"p_2\", \"p_both\", \"B_1\", \"B_2\", \"recurrent_states\"]\n",
    "    # TODO: recurrent_states should be stored as recurrent_states.\n",
    "    # Currently, it is stored as strategy_set.\n",
    "    p_1, p_2, p_both, B_1, B_2, recurrent_states = [models[k] for k in names]\n",
    "    ergodic = models[\"ergodic\"]\n",
    "    # l1_safe = [\"AS-AS\", \"AU-AS\", \"S1-AS\", \"S2-AS\", \"AS-S1\", \"S1-S1\"]\n",
    "    l1_safe = [\"5-1\", \"6-1\", \"7-1\", \"8-1\", \"5-3\", \"6-3\"]\n",
    "    # l2_safe = [\"AS-AS\", \"AS-S1\", \"AS-S2\", \"AS-AU\", \"S2-AU\", \"S2-S2\"]\n",
    "    l2_safe = [\"5-1\", \"5-3\", \"5-4\", \"5-2\", \"8-2\", \"8-4\"]\n",
    "    both_safe = set(l1_safe) & set(l2_safe)\n",
    "    l1_safe_only = set(l1_safe) - both_safe\n",
    "    l2_safe_only = set(l2_safe) - both_safe\n",
    "    both_unsafe = set(recurrent_states) - set(l1_safe) - set(l2_safe)\n",
    "    unsafe_states = [i for i, state in enumerate(recurrent_states)\n",
    "                     if state in both_unsafe]\n",
    "    l1_safe_only_states = [i for i, state in enumerate(recurrent_states)\n",
    "                           if state in l1_safe_only]\n",
    "    l2_safe_only_states = [i for i, state in enumerate(recurrent_states)\n",
    "                           if state in l2_safe_only]\n",
    "    pr_both_unsafe = np.sum(ergodic[:, unsafe_states],axis=1)\n",
    "    pr_l1_safe_only = np.sum(ergodic[:, l1_safe_only_states],axis=1)\n",
    "    pr_l2_safe_only = np.sum(ergodic[:, l2_safe_only_states],axis=1)\n",
    "    harm = (pr_l1_safe_only * (1 - p_2) * B_2 / (B_1 + B_2)\n",
    "            + pr_l2_safe_only * (1 - p_1) * B_1 / (B_1 + B_2)\n",
    "            + pr_both_unsafe * (1 - p_both))\n",
    "    harm_l1 = pr_both_unsafe * (1 - p_both) + pr_l2_safe_only * (1 - p_1)\n",
    "    harm_l2 = pr_both_unsafe * (1 - p_both) + pr_l1_safe_only * (1 - p_2)\n",
    "    return {**models, \"harm\": harm, \"harm_l1\": harm_l1, \"harm_l2\": harm_l2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_arrival_times(models):\n",
    "    \"\"\"Compute the arrival times for each player in each profile.\"\"\"\n",
    "    arrival_times = models[\"arrival_times\"]\n",
    "    arrival_times_dict_1 = {f\"arrival_times_player_1_profile_{k}\": d[:, 0]\n",
    "                            for k, d in arrival_times.items()}\n",
    "    arrival_times_dict_2 = {f\"arrival_times_player_2_profile_{k}\": d[:, 1]\n",
    "                            for k, d in arrival_times.items()}\n",
    "    arrival_times_dict_3 = {f\"arrival_times_player_3_profile_{k}\": d[:, 2]\n",
    "                            for k, d in arrival_times.items()}\n",
    "    arrival_times_dict_4 = {f\"arrival_times_player_4_profile_{k}\": d[:, 3]\n",
    "                            for k, d in arrival_times.items()}\n",
    "    \n",
    "    # We usually only care about the minimum arrival times for each layer.\n",
    "    arrival_times_profiles_layer_1 = {f\"arrival_times_L1_profile_{k}\": np.min(d[:, :2], axis=1)\n",
    "                             for k, d in arrival_times.items()}\n",
    "    arrival_times_profiles_layer_2 = {f\"arrival_times_L2_profile_{k}\": np.min(d[:, 2:4], axis=1)\n",
    "                             for k, d in arrival_times.items()}\n",
    "    \n",
    "    # We can also record the arrival times of laggards in each layer\n",
    "    arrival_times_layer_1_laggards = {f\"arrival_times_L1_laggards_profile_{k}\": np.max(d[:, :2], axis=1)\n",
    "                                      for k, d in arrival_times.items()}\n",
    "    arrival_times_layer_2_laggards = {f\"arrival_times_L2_laggards_profile_{k}\": np.max(d[:, 2:4], axis=1)\n",
    "                                      for k, d in arrival_times.items()}\n",
    "    \n",
    "    # We can also record the average arrival times for the layer given the\n",
    "    # ergodic distribution of the model\n",
    "    recurrent_states = models[\"recurrent_states\"]\n",
    "    ergodic = models[\"ergodic\"]\n",
    "    arrival_times_layer_1_mean = np.zeros(ergodic.shape[0])\n",
    "    arrival_times_layer_2_mean = np.zeros(ergodic.shape[0])\n",
    "    for i, rs in enumerate(recurrent_states):\n",
    "        s = rs.split(\"-\")\n",
    "        state = \"-\".join([s[0], s[0], s[1], s[1]])\n",
    "        arrival_times_layer_1_mean += ergodic[:, i] * arrival_times_profiles_layer_1[f\"arrival_times_L1_profile_{state}\"]\n",
    "        arrival_times_layer_2_mean += ergodic[:, i] * arrival_times_profiles_layer_2[f\"arrival_times_L2_profile_{state}\"]\n",
    "    \n",
    "    return {**models,\n",
    "            \"arrival_times_layer_1_mean\": arrival_times_layer_1_mean,\n",
    "            \"arrival_times_layer_2_mean\": arrival_times_layer_2_mean,\n",
    "            **arrival_times_profiles_layer_1,\n",
    "            **arrival_times_profiles_layer_2,\n",
    "            **arrival_times_layer_1_laggards,\n",
    "            **arrival_times_layer_2_laggards,\n",
    "            **arrival_times_dict_1,\n",
    "            **arrival_times_dict_2,\n",
    "            **arrival_times_dict_3,\n",
    "            **arrival_times_dict_4}\n",
    "    \n",
    "def compute_safe_times(models):\n",
    "    \"\"\"Compute the safe times for each player in each profile.\"\"\"\n",
    "    safe_times = models[\"safe_times\"]\n",
    "    safe_times_dict_1 = {f\"safe_times_player_1_profile_{k}\": d[:, 0]\n",
    "                         for k, d in safe_times.items()}\n",
    "    safe_times_dict_2 = {f\"safe_times_player_2_profile_{k}\": d[:, 1]\n",
    "                         for k, d in safe_times.items()}\n",
    "    safe_times_dict_3 = {f\"safe_times_player_3_profile_{k}\": d[:, 2]\n",
    "                         for k, d in safe_times.items()}\n",
    "    safe_times_dict_4 = {f\"safe_times_player_4_profile_{k}\": d[:, 3]\n",
    "                         for k, d in safe_times.items()}\n",
    "    \n",
    "    # We usually only care about the minimum safe times for each layer.\n",
    "    safe_times_profiles_layer_1 = {f\"safe_times_L1_profile_{k}\": np.min(d[:, :2], axis=1)\n",
    "                             for k, d in safe_times.items()}\n",
    "    safe_times_profiles_layer_2 = {f\"safe_times_L2_profile_{k}\": np.min(d[:, 2:4], axis=1)\n",
    "                             for k, d in safe_times.items()}\n",
    "    \n",
    "    # We can also record the safe times of laggards in each layer\n",
    "    safe_times_layer_1_laggards = {f\"safe_times_L1_laggards_profile_{k}\": np.max(d[:, :2], axis=1)\n",
    "                                   for k, d in safe_times.items()}\n",
    "    safe_times_layer_2_laggards = {f\"safe_times_L2_laggards_profile_{k}\": np.max(d[:, 2:4], axis=1)\n",
    "                                   for k, d in safe_times.items()}\n",
    "    \n",
    "    # We can also record the average safe times for the layer given the\n",
    "    # ergodic distribution of the model\n",
    "    recurrent_states = models[\"recurrent_states\"]\n",
    "    ergodic = models[\"ergodic\"]\n",
    "    safe_times_layer_1_mean = np.zeros(ergodic.shape[0])\n",
    "    safe_times_layer_2_mean = np.zeros(ergodic.shape[0])\n",
    "    for i, rs in enumerate(recurrent_states):\n",
    "        s = rs.split(\"-\")\n",
    "        state = \"-\".join([s[0], s[0], s[1], s[1]])\n",
    "        safe_times_layer_1_mean += ergodic[:, i] * safe_times_profiles_layer_1[f\"safe_times_L1_profile_{state}\"]\n",
    "        safe_times_layer_2_mean += ergodic[:, i] * safe_times_profiles_layer_2[f\"safe_times_L2_profile_{state}\"]\n",
    "    \n",
    "    return {**models,\n",
    "            \"safe_times_layer_1_mean\": safe_times_layer_1_mean,\n",
    "            \"safe_times_layer_2_mean\": safe_times_layer_2_mean,\n",
    "            **safe_times_profiles_layer_1,\n",
    "            **safe_times_profiles_layer_2,\n",
    "            **safe_times_layer_1_laggards,\n",
    "            **safe_times_layer_2_laggards,\n",
    "            **safe_times_dict_1,\n",
    "            **safe_times_dict_2,\n",
    "            **safe_times_dict_3,\n",
    "            **safe_times_dict_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_state_color(state, recurrent_states):\n",
    "    \"\"\"Map state to a color for plotting purposes.\"\"\"\n",
    "    l1_safe = [\"AS-AS\", \"AU-AS\", \"S1-AS\", \"S2-AS\", \"AS-S1\", \"S1-S1\"]\n",
    "    l2_safe = [\"AS-AS\", \"AS-S1\", \"AS-S2\", \"AS-AU\", \"S2-AU\", \"S2-S2\"]\n",
    "    both_safe = set(l1_safe) & set(l2_safe)\n",
    "    l1_safe_only = set(l1_safe) - both_safe\n",
    "    l2_safe_only = set(l2_safe) - both_safe\n",
    "    both_unsafe = set(recurrent_states) - set(l1_safe) - set(l2_safe)\n",
    "    if state in both_unsafe:\n",
    "        return \"#ff9000\"\n",
    "    if state in l1_safe_only:\n",
    "        return \"#FFD700\"\n",
    "    if state in l2_safe_only:\n",
    "        return \"#FFD700\"\n",
    "    if state in both_safe:\n",
    "        return \"#90EE90\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dilemma_zone_multi_race(table, var='AU_frequency'):\n",
    "    \"\"\"Plot the dilemma zone for the multi\"\"\"\n",
    "    heatmap, ax, im = plot_heatmap(table,\n",
    "                ylabel='Risk of layer 1 firms, r_1',\n",
    "                xlabel='Speed of unsafe layer 1 firms, s_1',\n",
    "                zlabel=var,\n",
    "                cmap='inferno',\n",
    "                )\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_times(df):\n",
    "    \"\"\"\n",
    "    Create lineplots for the mean arrival and safe times as we vary s_1and r_1\"\"\"\n",
    "    \n",
    "    df_plot = df\n",
    "    df_plot[\"r_1\"] = np.round(df_plot[\"r_1\"], decimals=2)\n",
    "    set_r_1 = select_unique_values(df_plot[\"r_1\"])\n",
    "    \n",
    "    figs = []\n",
    "    titles = ['Mean Arrival Times for Layer 1',\n",
    "              'Mean Arrival Times for Layer 2',\n",
    "              'Mean Time until Safe AI for Layer 1',\n",
    "              'Mean Time until Safe AI for Layer 2']\n",
    "    for i, var in enumerate([\"arrival_times_layer_1_mean\",\n",
    "                             \"arrival_times_layer_2_mean\",\n",
    "                             \"safe_times_layer_1_mean\",\n",
    "                             \"safe_times_layer_2_mean\"]):\n",
    "        fig = px.line(df_plot[df_plot[\"r_1\"].isin(set_r_1)],\n",
    "                        x=\"s_1\",\n",
    "                        y=var,\n",
    "                        facet_col=\"r_1\")\n",
    "\n",
    "        fig.update_layout(title=titles[i],\n",
    "                            xaxis_title='Speed of Layer 1 Firms',\n",
    "                            yaxis_title='Time',\n",
    "                            )\n",
    "        figs.append(fig)\n",
    "        fig.show()\n",
    "\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_df_into_states(df):\n",
    "       \"\"\"\n",
    "       Reshape dataframe so that the frequency columns are melted into a signle\n",
    "       column where each row is a possible state and select outcomes relevant to\n",
    "       each state.\n",
    "       \"\"\"\n",
    "       columns = df.columns\n",
    "       frequency_columns = [col for col in columns if col.endswith(\"_frequency\")]\n",
    "       frequency_columns = [col for col in frequency_columns if len(col.split(\"-\")) == 2]\n",
    "       arrival_time_cols = [col for col in columns if col.startswith(\"arrival_times_L\")]\n",
    "       safe_time_cols = [col for col in columns if col.startswith(\"safe_times_L\")]\n",
    "\n",
    "       df_reshaped = df.melt(id_vars=[\"s_1\", \"s_2\", \n",
    "                                   \"p_1\", \"p_2\",\n",
    "                                   \"r_1\", \"r_2\",\n",
    "                                   \"alpha_1\", \"alpha_2\",\n",
    "                                   \"gamma_1\", \"gamma_2\",\n",
    "                                   \"B_1\", \"B_2\",\n",
    "                                   \"W_1\", \"W_2\",\n",
    "                                   *arrival_time_cols,\n",
    "                                   *safe_time_cols],\n",
    "                            value_vars=frequency_columns,\n",
    "                            var_name=\"state\",\n",
    "                            value_name=\"frequency\")\n",
    "       df_reshaped[\"state\"] = df_reshaped[\"state\"].str.replace(\"_frequency\", \"\")\n",
    "       # I need to find the arrival time relevant to the state we are melting to\n",
    "       # I can do this by finding the state in the arrival_time_cols\n",
    "       # and then extracting the arrival time from the corresponding column\n",
    "       df_reshaped[\"profile\"] = [\"-\".join([str.split(state, \"-\")[0], str.split(state, \"-\")[0],\n",
    "                                          str.split(state, \"-\")[1], str.split(state, \"-\")[1]])\n",
    "              for state in df_reshaped.state]\n",
    "       df_reshaped[\"arrival_time_layer_1\"] = [df_reshaped[f\"arrival_times_L1_profile_{profile}\"][i]\n",
    "                                              for i, profile in enumerate(df_reshaped.profile)]\n",
    "       df_reshaped[\"arrival_time_layer_2\"] = [df_reshaped[f\"arrival_times_L2_profile_{profile}\"][i]\n",
    "                                              for i, profile in enumerate(df_reshaped.profile)]\n",
    "       df_reshaped[\"safe_time_layer_1\"] = [df_reshaped[f\"safe_times_L1_profile_{profile}\"][i]\n",
    "                                           for i, profile in enumerate(df_reshaped.profile)]\n",
    "       df_reshaped[\"safe_time_layer_2\"] = [df_reshaped[f\"safe_times_L2_profile_{profile}\"][i]\n",
    "                                           for i, profile in enumerate(df_reshaped.profile)]\n",
    "       return df_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df_fn(df):\n",
    "    # Round these columns to make sure we can match them to the selected values in case of floating point errors.\n",
    "    df.s_1 = np.round(df.s_1, decimals=2)\n",
    "    df.p_1 = np.round(df.p_1, decimals=2)\n",
    "    df.s_2 = np.round(df.s_2, decimals=2)\n",
    "    df.p_2 = np.round(df.p_2, decimals=2)\n",
    "    df[\"r_1\"] = 1 - df[\"p_1\"]\n",
    "    df[\"r_2\"] = 1 - df[\"p_2\"]\n",
    "    df.r_1 = np.round(df.r_1, decimals=2)\n",
    "    df.r_2 = np.round(df.r_2, decimals=2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_results(models):\n",
    "    return thread_macro(models,\n",
    "                         create_profiles,\n",
    "                         apply_profile_filters,\n",
    "                         build_payoffs,\n",
    "                         build_transition_matrix,\n",
    "                         find_ergodic_distribution,\n",
    "                         compute_harm,\n",
    "                         compute_arrival_times,\n",
    "                         compute_safe_times,\n",
    "                         )\n",
    "\n",
    "def run_sim_helper(models):\n",
    "    return thread_macro(models,\n",
    "                       get_results,\n",
    "                       results_to_dataframe_egt,\n",
    "                       process_df_fn,\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_figs(models):\n",
    "\n",
    "    results = get_results(models)\n",
    "    df = thread_macro(results, results_to_dataframe_egt, process_df_fn)\n",
    "    \n",
    "    # It is important that we only include plotting functions which\n",
    "    # don't need us to filter data by a specific value, since this should\n",
    "    # be specified in the original models dictionary.\n",
    "    \n",
    "    figs = []\n",
    "\n",
    "    # For many of our plots, we need to create a plot for unique value of at\n",
    "    # least one variable.\n",
    "    set_s_1 = select_unique_values(df.s_1)\n",
    "    set_p_1 = select_unique_values(df.p_1)\n",
    "    set_s_2 = select_unique_values(df.s_2)\n",
    "    set_p_2 = select_unique_values(df.p_2)\n",
    "    \n",
    "    # # Strategy frequencies\n",
    "    # for selected_s_1 in set_s_1:\n",
    "    #     for selected_p_2 in set_p_2:\n",
    "    #         for selected_s_2 in set_s_2:\n",
    "    #             df_plot = df[(df[\"s_1\"] == selected_s_1)\n",
    "    #                                                   & (df[\"p_2\"] == selected_p_2)\n",
    "    #                                                   & (df[\"s_2\"] == selected_s_2)]\n",
    "    #             fig = plot_strategy_distribution(df_plot,\n",
    "    #                                                results[\"recurrent_states\"],\n",
    "    #                                                x=\"p_1\",\n",
    "    #                                                x_label=\"p_1\",\n",
    "    #                                                thresholds=None,\n",
    "    #                                                cmap=plt.colormaps[\"tab20\"]\n",
    "    #                                                )\n",
    "    #             # figs.append(fig)\n",
    "                \n",
    "    #             fig = go.Figure()\n",
    "    #             player = 4\n",
    "    #             profiles = results[\"payoffs\"].keys()\n",
    "    #             for profile in profiles:\n",
    "    #                 y_var = f\"arrival_times_player_{player}_profile_{profile}\"\n",
    "    #                 x_var = \"r_1\"\n",
    "    #                 # Draw scatterplot using plotly\n",
    "    #                 fig.add_trace(go.Scatter(x=df_plot[x_var], y=df_plot[y_var],\n",
    "    #                                         mode='markers',\n",
    "    #                                         name=profile,\n",
    "    #                                         marker=dict(size=10),\n",
    "    #                                         ))\n",
    "    #             fig.update_layout(title=f\"Arrival Times for Player {player} across profiles\",\n",
    "    #                                     xaxis_title=\"Risk of Layer 1 Firms, r_1\",\n",
    "    #                                     yaxis_title=\"Arrival Time\",\n",
    "    #                                     )\n",
    "    #             figs.append(fig)\n",
    "    \n",
    "    # # Markov Chains\n",
    "    # for selected_p_1 in set_p_1:\n",
    "    #     for selected_s_1 in set_s_1:\n",
    "    #         for selected_p_2 in set_p_2:\n",
    "    #             for selected_s_2 in set_s_2:\n",
    "    #                 default_idx = df[(df.s_1==selected_s_1) & (df.p_1==selected_p_1)\n",
    "    #                 & (df.s_2==selected_s_2) & (df.p_2==selected_p_2)].index[0]\n",
    "    #                 strategy_labels = results[\"strategy_set\"]\n",
    "    #                 recurrent_states_labels = [\"-\".join([strategy_labels[int(s)-1] for s in state.split(\"-\")])\n",
    "    #                                         for state in results[\"recurrent_states\"]]\n",
    "    #                 node_facecolor = [map_state_color(state, recurrent_states_labels) for state in recurrent_states_labels]\n",
    "    #                 P = np.round(results['transition_matrix'][default_idx], decimals=4)\n",
    "    #                 mc = MarkovChain(P,\n",
    "    #                                 recurrent_states_labels,\n",
    "    #                                 percentages=True,\n",
    "    #                             #   self_arrows=False,\n",
    "    #                                 #  annotate=False,\n",
    "    #                                 scale_xlim=0.5,\n",
    "    #                                 scale_ylim=0.5,\n",
    "    #                                 n_columns=2,\n",
    "    #                                 node_facecolor= node_facecolor,\n",
    "    #                                 transparency_func=lambda p:max(min(1, 5 * p**0.5), 0.1) if p>1e-100 else 0)\n",
    "    #                 # figs.append(mc.draw())\n",
    "\n",
    "    # Dilemma zones and Arrival times\n",
    "    \n",
    "    for selected_p_2 in set_p_2:\n",
    "        for selected_s_2 in set_s_2:\n",
    "            df_plot = df[(df[\"s_2\"] == selected_s_2) & (df[\"p_2\"] == selected_p_2)]\n",
    "            # df_plot[\"AU_frequency\"] = df_plot[\"8-2_frequency\"]\n",
    "            # table = df_plot.pivot_table(index='r_1', columns='s_1', values=\"AU_frequency\")\n",
    "            # fig = plot_dilemma_zone_multi_race(table)\n",
    "            # figs.append(fig)\n",
    "            # more_figs = plot_mean_times(df_plot)\n",
    "            # figs.extend(more_figs)\n",
    "            \n",
    "            table = df_plot.pivot_table(index='r_1', columns='s_1', values=\"arrival_times_layer_1_mean\")\n",
    "            heatmap, ax, im = plot_heatmap(table,\n",
    "            ylabel='Risk of layer 1 firms, r_1',\n",
    "            xlabel='Speed of unsafe layer 1 firms, s_1',\n",
    "            zlabel=\"arrival_times_layer_1_mean\",\n",
    "            zmax=df_plot[\"arrival_times_layer_1_mean\"].max(),\n",
    "            cmap='inferno',\n",
    "            )\n",
    "            figs.append(heatmap)\n",
    "            table = df_plot.pivot_table(index='r_1', columns='s_1', values=\"arrival_times_layer_1_mean\")\n",
    "            heatmap, ax, im = plot_heatmap(table,\n",
    "            ylabel='Risk of layer 1 firms, r_1',\n",
    "            xlabel='Speed of unsafe layer 1 firms, s_1',\n",
    "            zlabel=\"arrival_times_layer_2_mean\",\n",
    "            zmax=df_plot[\"arrival_times_layer_2_mean\"].max(),\n",
    "            cmap='inferno',\n",
    "            )\n",
    "            figs.append(heatmap)\n",
    "            \n",
    "            df_reshaped = melt_df_into_states(df_plot)\n",
    "            \n",
    "            fig, axs = plot_generic_grid(df_reshaped, 's_1', 'r_1', 'arrival_time_layer_1')\n",
    "            axs = add_pdfs_to_grid(axs, df_reshaped, 's_1', 'r_1', 'arrival_time_layer_1', 'frequency')\n",
    "            fig.suptitle(\"PDFs of Arrival Times for Layer 1\",)\n",
    "            figs.append(fig)\n",
    "            \n",
    "            fig, axs = plot_generic_grid(df_reshaped, 's_1', 'r_1', 'arrival_time_layer_2')\n",
    "            axs = add_pdfs_to_grid(axs, df_reshaped, 's_1', 'r_1', 'arrival_time_layer_2', 'frequency')\n",
    "            fig.suptitle(\"PDFs of Arrival Times for Layer 2\",)\n",
    "            figs.append(fig)\n",
    "            \n",
    "            fig, axs = plot_generic_grid(df_reshaped, 's_1', 'r_1', 'safe_time_layer_1')\n",
    "            axs = add_pdfs_to_grid(axs, df_reshaped, 's_1', 'r_1', 'safe_time_layer_1', 'frequency')\n",
    "            fig.suptitle(\"PDFs of Time until Safe AI for Layer 1\",)\n",
    "            figs.append(fig)\n",
    "            \n",
    "            fig, axs = plot_generic_grid(df_reshaped, 's_1', 'r_1', 'safe_time_layer_2')\n",
    "            axs = add_pdfs_to_grid(axs, df_reshaped, 's_1', 'r_1', 'safe_time_layer_2', 'frequency')\n",
    "            fig.suptitle(\"PDFs of Time until Safe AI for Layer 2\",)\n",
    "            figs.append(fig)\n",
    "    \n",
    "    return figs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When planning how many models to run, calculate the memory (in GB) used to store the payoffs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.220703125, 0.6103515625, 0.30517578125)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_models_planned = 40000\n",
    "x = np.random.random((n_models_planned, 4))\n",
    "gb_size = lambda x: x.nbytes / (1024 ** 3) # Size in GB\n",
    "total_size = lambda x: gb_size(x) * 16**2 * 4\n",
    "total_size(x), total_size(x.astype(np.float32)), total_size(x.astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_multi_race' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m sector_strategies \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS2\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m      4\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m],}\n\u001b[1;32m      5\u001b[0m allowed_sectors \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP4\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP3\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP2\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS1\u001b[39m\u001b[38;5;124m\"\u001b[39m], }\n\u001b[0;32m----> 9\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_multi_race\u001b[49m(p_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     10\u001b[0m                           s_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     11\u001b[0m                           p_1\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.02\u001b[39m,\n\u001b[1;32m     12\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.02\u001b[39m,\n\u001b[1;32m     13\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.02\u001b[39m},\n\u001b[1;32m     14\u001b[0m                           s_1\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.50\u001b[39m,\n\u001b[1;32m     15\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5.02\u001b[39m,\n\u001b[1;32m     16\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.02\u001b[39m},\n\u001b[1;32m     17\u001b[0m                           gamma_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     18\u001b[0m                           gamma_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     19\u001b[0m                           delta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     20\u001b[0m                           delta_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     21\u001b[0m                           alpha_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     22\u001b[0m                           alpha_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     23\u001b[0m                           W_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     24\u001b[0m                           W_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     25\u001b[0m                           B_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     26\u001b[0m                           B_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     27\u001b[0m                           β\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,             \n\u001b[1;32m     28\u001b[0m                           strategy_set\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m                                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAU\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m                           )\n\u001b[1;32m     31\u001b[0m models \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodels,\n\u001b[1;32m     32\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdispatch-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiple-populations\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayoffs_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti-race-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msector_strategies\u001b[39m\u001b[38;5;124m\"\u001b[39m: sector_strategies,\n\u001b[1;32m     38\u001b[0m           }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_multi_race' is not defined"
     ]
    }
   ],
   "source": [
    "Z = {\"S2\": 100, \"S1\": 100}\n",
    "β = 0.1\n",
    "sector_strategies = {\"S2\": [5, 6, 7, 8],\n",
    "                     \"S1\": [1, 2, 3, 4],}\n",
    "allowed_sectors = {\"P4\": [\"S2\"],\n",
    "                   \"P3\": [\"S2\"],\n",
    "                   \"P2\": [\"S1\"],\n",
    "                   \"P1\": [\"S1\"], }\n",
    "models = build_multi_race(p_2=0.5,\n",
    "                          s_2=3,\n",
    "                          p_1={\"start\": 0.02,\n",
    "                               \"stop\": 1.02,\n",
    "                               \"step\": 0.02},\n",
    "                          s_1={\"start\": 1.50,\n",
    "                               \"stop\": 5.02,\n",
    "                               \"step\": 0.02},\n",
    "                          gamma_1=0,\n",
    "                          gamma_2=0,\n",
    "                          delta_1=0.9,\n",
    "                          delta_2=0.9,\n",
    "                          alpha_1=4,\n",
    "                          alpha_2=2,\n",
    "                          W_1=100,\n",
    "                          W_2=100,\n",
    "                          B_1=1000,\n",
    "                          B_2=1000,\n",
    "                          β=0.1,             \n",
    "                          strategy_set=[\"AS\", \"AU\", \"S1\", \"S2\",\n",
    "                                        \"AS\", \"S1\", \"S2\", \"AU\"],\n",
    "                          )\n",
    "models = {**models,\n",
    "          \"dispatch-type\": 'multiple-populations',\n",
    "          \"payoffs_key\": \"multi-race-v1\",\n",
    "          \"β\": β,\n",
    "          \"Z\": Z,\n",
    "          \"allowed_sectors\": allowed_sectors,\n",
    "          \"sector_strategies\": sector_strategies,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Phrase: formulated_indelicacy_concussions\n",
      "Sim ID: formulated_indelicacy_concussions_577419fd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ethos/git/gh-pages-example/gh_pages_example/model_utils.py:299: FutureWarning:\n",
      "\n",
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 132.39it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethos/git/gh-pages-example/gh_pages_example/model_utils.py:299: FutureWarning:\n",
      "\n",
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: data/formulated_indelicacy_concussions_577419fd/dataframe_formulated_indelicacy_concussions_577419fd_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 119.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: plots/formulated_indelicacy_concussions_577419fd/plot_formulated_indelicacy_concussions_577419fd_fig_0_0.png\n",
      "Saved file: plots/formulated_indelicacy_concussions_577419fd/plot_formulated_indelicacy_concussions_577419fd_fig_1_0.png\n",
      "Saved file: plots/formulated_indelicacy_concussions_577419fd/plot_formulated_indelicacy_concussions_577419fd_fig_2_0.png\n",
      "Saved file: plots/formulated_indelicacy_concussions_577419fd/plot_formulated_indelicacy_concussions_577419fd_fig_3_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: plots/formulated_indelicacy_concussions_577419fd/plot_formulated_indelicacy_concussions_577419fd_fig_4_0.png\n",
      "Saved file: plots/formulated_indelicacy_concussions_577419fd/plot_formulated_indelicacy_concussions_577419fd_fig_5_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "figs, dfs = run_all_simulations(model_list,\n",
    "                                simulation_fn=run_sim_helper,\n",
    "                                plotting_fn=plot_all_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethos/git/gh-pages-example/gh_pages_example/model_utils.py:299: FutureWarning:\n",
      "\n",
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:02<00:00, 107.02it/s]\n",
      "/home/ethos/git/gh-pages-example/gh_pages_example/methods.py:260: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = run_sim_helper(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a working model, it is time to prove we understand it.\n",
    "\n",
    "To do so, I will plot the dilemma zone's and arrival time distributions of\n",
    "several simple cases. In one set of cases, I limit the choice of strategy of the\n",
    "labs in Layer 1 to a single strategy, while restricting the choice of strategies\n",
    "in Layer 2 to just 2 strategies. I then, show how the dilemma zone varies with\n",
    "a change in the key new parameters introduced by the model, the spillover\n",
    "parameters, the contestability rates, and risk spillover.\n",
    "\n",
    "In a second set of cases, I allow Layer 2 to use 4 strategies at once, and\n",
    "consider cases where Layer 2 starts behind or ahead of Layer 1.\n",
    "\n",
    "Finally, in a third set of cases, I revisit the above cases about allow all\n",
    "4 strategies to be present. This time, we look not just at the frequency of\n",
    "unsafe behaviour, but more directly at the total harm caused.\n",
    "\n",
    "Aftewards, we move on to discuss how to use this model as a tool for thought\n",
    "about thinking through tech spillovers in real markets over the last few\n",
    "decades. Clearly, the field of AI has undergone several eras of development, but\n",
    "until rather recently the severity of risks appeared rather low. We instead\n",
    "speculate as to what it would mean for a company to make the trade offs we see\n",
    "in the model in the face of anticipated future spillovers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_hists_to_grid(axs, df, x_col, y_col, data_col):\n",
    "    \"\"\"\n",
    "    Add histogram plots to the grid of subplots in `axs`.\n",
    "    \n",
    "    Args:\n",
    "    axs (np.array): An array of subplots.\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    x_col (str): The column name for the x-axis.\n",
    "    y_col (str): The column name for the y-axis.\n",
    "    data_col (str): The column name for the data to plot.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: An array of subplots with the added histogram plots.\n",
    "    \"\"\"\n",
    "    categories_x = select_unique_values(df[x_col])\n",
    "    categories_y = select_unique_values(df[y_col])\n",
    "    \n",
    "    hist_range = [df[data_col].min(), df[data_col].max()]\n",
    "    for i, category_y in enumerate(categories_y):\n",
    "        for j, category_x in enumerate(categories_x):\n",
    "            data = df[(df[x_col] == category_x) & (df[y_col] == category_y)][data_col]\n",
    "            color = \"skyblue\"\n",
    "            axs[i, j].hist(data, bins=10, color = color, edgecolor='black', \n",
    "                           density=True, range=hist_range)\n",
    "            \n",
    "            \n",
    "    return axs\n",
    "\n",
    "def combine_duplicate_x_values(data, freq, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Combine duplicate x-values and sum their frequencies.\n",
    "    \n",
    "    Args:\n",
    "    data (np.array): The input x-values.\n",
    "    freq (np.array): The input frequencies.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: An array of combined x-values.\n",
    "    np.array: An array of combined frequencies.\n",
    "    \"\"\"\n",
    "    # First, we consider a tolerance for similar data points to be considered equal\n",
    "    data = np.round(data, int(-np.log10(tol)))\n",
    "    unique_data = np.unique(data)\n",
    "    unique_freq = np.array([np.sum(freq[data == d]) for d in unique_data])\n",
    "    return unique_data, unique_freq\n",
    "\n",
    "def add_pdfs_to_grid(axs, df, x_col, y_col, data_col, freq_col):\n",
    "    \"\"\"\n",
    "    Add pdf plots to the grid of subplots in `axs`.\n",
    "    \n",
    "    Args:\n",
    "    axs (np.array): An array of subplots.\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    x_col (str): The column name for the x-axis.\n",
    "    y_col (str): The column name for the y-axis.\n",
    "    data_col (str): The column name for the data to plot.\n",
    "    freq_col (str): The column name for the frequencies of the data values.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: An array of subplots with the added pdf plots.\n",
    "    \"\"\"\n",
    "    categories_x = select_unique_values(df[x_col])\n",
    "    categories_y = select_unique_values(df[y_col])\n",
    "    \n",
    "    hist_range = [df[data_col].min(), df[data_col].max()]\n",
    "    hist_height = df[freq_col].max()\n",
    "    for i, category_y in enumerate(categories_y):\n",
    "        for j, category_x in enumerate(categories_x):\n",
    "            data = df[(df[x_col] == category_x) & (df[y_col] == category_y)][data_col]\n",
    "            freq = df[(df[x_col] == category_x) & (df[y_col] == category_y)][freq_col]\n",
    "            # We need to combine any duplicate x-values and sum their frequencies\n",
    "            data, freq = combine_duplicate_x_values(data, freq, tol=1e-6)\n",
    "            \n",
    "            color = \"skyblue\"\n",
    "            axs[i, j].set_xlim(hist_range)  # Set the x-limits of the axes\n",
    "            axs[i, j].set_ylim([0, hist_height])  # Set the y-limits of the axes\n",
    "            \n",
    "            # If there is no data for the given category, skip plotting\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the width of the bars based on the smallest difference between consecutive x-values\n",
    "            sorted_data = np.sort(data)\n",
    "            if len(data) > 1:\n",
    "                min_diff = np.min(np.diff(sorted_data))\n",
    "                bar_width = 0.8 * min_diff  # 0.8 is a common choice to leave some space between bars\n",
    "                x_len = hist_range[1] - hist_range[0]\n",
    "                # Clip the bar width to be between 1 and 10% of the x-axis length\n",
    "                bar_width = np.clip(bar_width, 0.01 * x_len, 0.1 * x_len)\n",
    "            else:\n",
    "                bar_width = 0.5\n",
    "            \n",
    "            axs[i, j].hist(data, weights=freq, color = color, width=bar_width)\n",
    "            # Add back in x_axis spine which vanishes when plotting bar plots\n",
    "            # in some cases\n",
    "            axs[i, j].spines['bottom'].set_visible(True)\n",
    "            axs[i, j].spines['bottom'].set_linewidth(0.5)\n",
    "            \n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = {\"S2\": 100, \"S1\": 100}\n",
    "β = 0.1\n",
    "sector_strategies = {\"S2\": [8],\n",
    "                     \"S1\": [1, 2],}\n",
    "allowed_sectors = {\"P4\": [\"S2\"],\n",
    "                   \"P3\": [\"S2\"],\n",
    "                   \"P2\": [\"S1\"],\n",
    "                   \"P1\": [\"S1\"], }\n",
    "models = build_multi_race(p_2=0.5,\n",
    "                          s_2=3,\n",
    "                          p_1={\"start\": 0.02,\n",
    "                               \"stop\": 1.02,\n",
    "                               \"step\": 0.02},\n",
    "                          s_1={\"start\": 1.50,\n",
    "                               \"stop\": 5.02,\n",
    "                               \"step\": 0.02},\n",
    "                          gamma_1=0,\n",
    "                          gamma_2=0,\n",
    "                          delta_1=0.9,\n",
    "                          delta_2=0.9,\n",
    "                          alpha_1=2,\n",
    "                          alpha_2=2,\n",
    "                          β=1,             \n",
    "                          strategy_set=[\"AS\", \"AU\", \"S1\", \"S2\",\n",
    "                                        \"AS\", \"S1\", \"S2\", \"AU\"],\n",
    "                          )\n",
    "models = {**models,\n",
    "          \"dispatch-type\": 'multiple-populations',\n",
    "          \"payoffs_key\": \"multi-race-v1\",\n",
    "          \"β\": β,\n",
    "          \"Z\": Z,\n",
    "          \"allowed_sectors\": allowed_sectors,\n",
    "          \"sector_strategies\": sector_strategies,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethos/git/gh-pages-example/gh_pages_example/model_utils.py:299: FutureWarning:\n",
      "\n",
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 91.16it/s]\n"
     ]
    }
   ],
   "source": [
    "results = get_results(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s_1': (8800,),\n",
       " 'p_1': (8800,),\n",
       " 's_2': (8800,),\n",
       " 'p_2': (8800,),\n",
       " 'B_1': (8800,),\n",
       " 'B_2': (8800,),\n",
       " 'p_both': (8800,),\n",
       " 'W_1': (8800,),\n",
       " 'W_2': (8800,),\n",
       " 'gamma_1': (8800,),\n",
       " 'gamma_2': (8800,),\n",
       " 'delta_1': (8800,),\n",
       " 'delta_2': (8800,),\n",
       " 'alpha_1': (8800,),\n",
       " 'alpha_2': (8800,),\n",
       " 'transition_matrix': (8800, 2, 2),\n",
       " 'ergodic': (8800, 2),\n",
       " 'harm': (8800,),\n",
       " 'harm_l1': (8800,),\n",
       " 'harm_l2': (8800,),\n",
       " 'arrival_times_layer_1_mean': (8800,),\n",
       " 'arrival_times_layer_2_mean': (8800,),\n",
       " 'arrival_times_L1_profile_8-8-1-1': (8800,),\n",
       " 'arrival_times_L1_profile_8-8-1-2': (8800,),\n",
       " 'arrival_times_L1_profile_8-8-2-1': (8800,),\n",
       " 'arrival_times_L1_profile_8-8-2-2': (8800,),\n",
       " 'arrival_times_L2_profile_8-8-1-1': (8800,),\n",
       " 'arrival_times_L2_profile_8-8-1-2': (8800,),\n",
       " 'arrival_times_L2_profile_8-8-2-1': (8800,),\n",
       " 'arrival_times_L2_profile_8-8-2-2': (8800,),\n",
       " 'arrival_times_L1_laggards_profile_8-8-1-1': (8800,),\n",
       " 'arrival_times_L1_laggards_profile_8-8-1-2': (8800,),\n",
       " 'arrival_times_L1_laggards_profile_8-8-2-1': (8800,),\n",
       " 'arrival_times_L1_laggards_profile_8-8-2-2': (8800,),\n",
       " 'arrival_times_L2_laggards_profile_8-8-1-1': (8800,),\n",
       " 'arrival_times_L2_laggards_profile_8-8-1-2': (8800,),\n",
       " 'arrival_times_L2_laggards_profile_8-8-2-1': (8800,),\n",
       " 'arrival_times_L2_laggards_profile_8-8-2-2': (8800,),\n",
       " 'arrival_times_player_1_profile_8-8-1-1': (8800,),\n",
       " 'arrival_times_player_1_profile_8-8-1-2': (8800,),\n",
       " 'arrival_times_player_1_profile_8-8-2-1': (8800,),\n",
       " 'arrival_times_player_1_profile_8-8-2-2': (8800,),\n",
       " 'arrival_times_player_2_profile_8-8-1-1': (8800,),\n",
       " 'arrival_times_player_2_profile_8-8-1-2': (8800,),\n",
       " 'arrival_times_player_2_profile_8-8-2-1': (8800,),\n",
       " 'arrival_times_player_2_profile_8-8-2-2': (8800,),\n",
       " 'arrival_times_player_3_profile_8-8-1-1': (8800,),\n",
       " 'arrival_times_player_3_profile_8-8-1-2': (8800,),\n",
       " 'arrival_times_player_3_profile_8-8-2-1': (8800,),\n",
       " 'arrival_times_player_3_profile_8-8-2-2': (8800,),\n",
       " 'arrival_times_player_4_profile_8-8-1-1': (8800,),\n",
       " 'arrival_times_player_4_profile_8-8-1-2': (8800,),\n",
       " 'arrival_times_player_4_profile_8-8-2-1': (8800,),\n",
       " 'arrival_times_player_4_profile_8-8-2-2': (8800,),\n",
       " 'safe_times_layer_1_mean': (8800,),\n",
       " 'safe_times_layer_2_mean': (8800,),\n",
       " 'safe_times_L1_profile_8-8-1-1': (8800,),\n",
       " 'safe_times_L1_profile_8-8-1-2': (8800,),\n",
       " 'safe_times_L1_profile_8-8-2-1': (8800,),\n",
       " 'safe_times_L1_profile_8-8-2-2': (8800,),\n",
       " 'safe_times_L2_profile_8-8-1-1': (8800,),\n",
       " 'safe_times_L2_profile_8-8-1-2': (8800,),\n",
       " 'safe_times_L2_profile_8-8-2-1': (8800,),\n",
       " 'safe_times_L2_profile_8-8-2-2': (8800,),\n",
       " 'safe_times_L1_laggards_profile_8-8-1-1': (8800,),\n",
       " 'safe_times_L1_laggards_profile_8-8-1-2': (8800,),\n",
       " 'safe_times_L1_laggards_profile_8-8-2-1': (8800,),\n",
       " 'safe_times_L1_laggards_profile_8-8-2-2': (8800,),\n",
       " 'safe_times_L2_laggards_profile_8-8-1-1': (8800,),\n",
       " 'safe_times_L2_laggards_profile_8-8-1-2': (8800,),\n",
       " 'safe_times_L2_laggards_profile_8-8-2-1': (8800,),\n",
       " 'safe_times_L2_laggards_profile_8-8-2-2': (8800,),\n",
       " 'safe_times_player_1_profile_8-8-1-1': (8800,),\n",
       " 'safe_times_player_1_profile_8-8-1-2': (8800,),\n",
       " 'safe_times_player_1_profile_8-8-2-1': (8800,),\n",
       " 'safe_times_player_1_profile_8-8-2-2': (8800,),\n",
       " 'safe_times_player_2_profile_8-8-1-1': (8800,),\n",
       " 'safe_times_player_2_profile_8-8-1-2': (8800,),\n",
       " 'safe_times_player_2_profile_8-8-2-1': (8800,),\n",
       " 'safe_times_player_2_profile_8-8-2-2': (8800,),\n",
       " 'safe_times_player_3_profile_8-8-1-1': (8800,),\n",
       " 'safe_times_player_3_profile_8-8-1-2': (8800,),\n",
       " 'safe_times_player_3_profile_8-8-2-1': (8800,),\n",
       " 'safe_times_player_3_profile_8-8-2-2': (8800,),\n",
       " 'safe_times_player_4_profile_8-8-1-1': (8800,),\n",
       " 'safe_times_player_4_profile_8-8-1-2': (8800,),\n",
       " 'safe_times_player_4_profile_8-8-2-1': (8800,),\n",
       " 'safe_times_player_4_profile_8-8-2-2': (8800,)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: results[k].shape for k,v in results.items() if isinstance(v, np.ndarray)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sums = np.sum(results['ergodic'], axis=-1)\n",
    "fastcore.test.test_close(result_sums, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the payoffs are not 0 too often\n",
    "\n",
    "Test that the payoffs are not all identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that depending on paramter values, it may be possible that\n",
    "# all payoffs are identical or 0 within a profile and player combination.\n",
    "# So, all we do is store such cases for later inspection.\n",
    "\n",
    "n_unique_payoffs = []\n",
    "unique_payoffs_data = []\n",
    "zero_payoffs_data = []\n",
    "for profile, player_payoffs in results[\"payoffs\"].items():\n",
    "    for player, payoffs in player_payoffs.items():\n",
    "        unique_payoffs = np.unique(np.round(payoffs, decimals=2))\n",
    "        n_unique_payoffs.append(len(unique_payoffs))\n",
    "        unique_payoffs_data.append((profile, player, unique_payoffs))\n",
    "        zero_payoffs_data.append((profile, player, np.mean(payoffs == 0)))\n",
    "        # Print statements for debugging\n",
    "        # if len(unique_payoffs) > 1:\n",
    "        #     print(\"n unique payoffs: \", len(unique_payoffs))\n",
    "        # if len(unique_payoffs) == 1:\n",
    "        #     print(f\"profile, player, payoff: {profile}, {player}, {unique_payoffs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a sanity check that the result of keeping delta=1 and gamma=1 is that total harm is close to zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
