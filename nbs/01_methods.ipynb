{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275141f-78f8-43c7-8a4f-7517dee6f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6efd0-6adb-4de2-8383-2bdc5af68496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "from gh_pages_example.utils import *\n",
    "from gh_pages_example.types import *\n",
    "\n",
    "import functools\n",
    "import typing\n",
    "\n",
    "import fastcore.test\n",
    "from nbdev.showdoc import *\n",
    "import numpy as np\n",
    "import nptyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675457e-78d3-4dcc-8b4d-9dc966a3ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c06847-fe93-4fc5-9b91-30f92cc69f3c",
   "metadata": {},
   "source": [
    "# Methods in Evolutionary Game Theory\n",
    "\n",
    "> A set of methods for solving Evolutionary Games (see Nowak 2006 and the references section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e4126-c29d-41e8-bce9-54dcabbc63ba",
   "metadata": {},
   "source": [
    "## Evolutionary Dynamics in Finite Populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f91a3-f1d3-4ad5-a574-3ab583f1edf5",
   "metadata": {},
   "source": [
    "We examine a finite population of players using different strategies who engage in social learning.\n",
    "\n",
    "In the limit of small mutations, most of the time everyone plays the same strategy. States in which everyone plays the same strategy are known as **monomorphic states**. Occassionally, mutant strategies can fixate in the population, resulting in everyone adopting the same new strategy. We can use Markov Chains to analyse the relative frequencies with which each strategy is played by the population.\n",
    "\n",
    "The steps for computing the ergodic (i.e. long-run, stationary) strategy distribution is as follows:\n",
    "\n",
    "1. Build a transition matrix between all monomorphic states\n",
    "2. Find the ergodic distribution for the markov chain defined using this transition matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb44b7a3-d2ca-4492-b844-5bb9eec99c5c",
   "metadata": {},
   "source": [
    "### Fermi social learning\n",
    "\n",
    "> A Fermi social learning rule means that individuals make pairwise comparisons between their own strategy and and another strategy in the population that they may choose to copy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab4078-47e3-4468-af4a-742219f3b2c5",
   "metadata": {},
   "source": [
    "#### Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5245cb-1d83-4bbe-8683-96cf42af71a3",
   "metadata": {},
   "source": [
    "Each period of the evolutionary game involves individuals being randomly selected to play against one another individual.\n",
    "\n",
    "Letting $Z$ denote the size of the population, and $π$ denote the game's payoff matrix, we can compute the fitness of a strategy, $B$ for example, when $k$ individuals are of type $B$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "ΠB_k = πBA \\frac{k-1}{Z - 1} + πBB \\frac{Z-k}{Z- 1}\n",
    "\\end{equation}\n",
    "\n",
    "where $πBA$ and $πBB$ are the payoffs for playing $B$ against type $A$ or $B$ respectively.\n",
    "\n",
    "The **Fermi social learning rule** adopts strategy $B$ selected from the population over their current strategy $A$ with probability given by:\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(adopt \\, B | k) = \\frac{1}{(1 + \\exp^{-\\beta (ΠB_k - ΠA_k)})}\n",
    "\\end{equation}\n",
    "\n",
    "where $ΠB_k - ΠA_k$ is the relative fitness of strategy $B$ over $A$ in a population with $k$ individuals of type $B$, the rest of type $A$. Notice how the larger the relative fitness, the closer the denominator, and therefore the probability, is to $1$.\n",
    "\n",
    "Using the Fermi social learning rule above, we can write the probability of increasing the number of type $B$ individuals as\n",
    "\n",
    "\\begin{equation}\n",
    "T^+_B(k) = \\frac{Z-k}{Z} \\frac{k}{Z} Pr(adopt \\, B | k) \n",
    "\\end{equation}\n",
    "Z\n",
    "as an individual of type $A$ needs to randomly be chosen to compare their strategy against someone of type $B$.\n",
    "\n",
    "and the probability of decreasing the number of type $B$ individuals as\n",
    "\n",
    "\\begin{equation}\n",
    "T^-_B(k) = \\frac{k}{Z} \\frac{Z-k}{Z} Pr(adopt \\, A | k) \n",
    "\\end{equation}\n",
    "\n",
    "as an individual of type $B$ needs to randomly be chosen to compare their strategy against someone of type $A$.\n",
    "\n",
    "We will often employ their ratio, which is: \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{T^-_B(k)}{T^+_B(k)} = \\frac{Pr(adopt \\, A | k) }{Pr(adopt \\, B | k)} = \\frac{1 + \\exp^{-\\beta (ΠB_k - ΠA_k)}}{1 + \\exp^{-\\beta (ΠA_k - ΠB_k)}}\n",
    "\\end{equation}\n",
    "\n",
    "Notice that $\\frac{1 + \\exp^x}{1 + \\exp^{-x}} = \\exp^{x}$\n",
    "\n",
    "So, this ratio simplifies to $\\frac{T^-_B(k)}{T^+_B(k)} =  \\exp^{-\\beta (ΠB_k - ΠA_k)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86898d7e-42d1-4ef9-af35-0859f8097c22",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea9cbf-f84a-4131-893c-47099a4487a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def fermi_learning(fitnessA:nptyping.NDArray, # fitness of strategy A\n",
    "                   fitnessB:nptyping.NDArray, # fitness of strategy B\n",
    "                   β:nptyping.NDArray, # learning rate\n",
    "                  ) -> nptyping.NDArray:\n",
    "    \"\"\"Compute the likelihood that a player with strategy A adopts strategy B using the fermi function.\"\"\"\n",
    "    return (1 + np.exp(-β*(fitnessB - fitnessA)))**(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62f2bd-4534-4464-9147-de20bf2dc42b",
   "metadata": {},
   "source": [
    "#### Examples and Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca8e00-f849-490b-b29d-54119b064a2e",
   "metadata": {},
   "source": [
    "When each strategy has the same fitness, then the likelihood that a player adopts strategy $B$ is 50%, no matter the value of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678244e2-9597-4d85-96ae-a69211d64539",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fermi_learning(np.array([5]),\n",
    "                   np.array([5]),\n",
    "                   np.array([1]),)\n",
    "nptyping.assert_isinstance(x, nptyping.NDArray[nptyping.Shape[\"1\"], typing.Any])\n",
    "fastcore.test.test_eq(x, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881f80e-272a-4727-9a46-881e5b51e38f",
   "metadata": {},
   "source": [
    "### Fixation rate\n",
    "\n",
    "> The fixation rate for type B in a population of type A, $\\rho$, is defined as the probability that the appearance of a mutant of type B leads to the entire population adopting type B instead of A, i.e. what is the likelihood that a mutant of type B invades population A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a01463-b50c-4fd6-aade-8ec965813481",
   "metadata": {},
   "source": [
    "#### Derivation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffe1bf-842e-4890-b40f-493bd627e9ce",
   "metadata": {},
   "source": [
    "A derivation of the fixation rate defined below can be found in Nowak 2006 (reproduced below).\n",
    "\n",
    "> Consider a one-dimensional stochastic process on a discrete state space, $ i \\in \\{0, 1, \\cdots, N\\}$ that represents the number of individuals in a population of $N$ individuals who are of type $B$, the rest are type $A$.\n",
    ">\n",
    "> In each stochastic event, the number of individuals of type $B$ can at most increase or decrease by 1.\n",
    ">\n",
    "> For a given number of individuals, $i$, let $a_i$, $b_i$, and $1 - a_i - b_i$ represent the chance of an increase, decrease, or no change in $i$.\n",
    "> \n",
    "> This stochastic process follows the transition matrix ,$P$ (*not to be confused with the transition matrices we discuss elsewhere!*)\n",
    ">\n",
    ">\n",
    "> \\begin{equation}\n",
    "P \\, = \\, \\begin{pmatrix}\n",
    "1 & 0 & 0 & \\cdots & 0 & 0 & 0\\\\\n",
    "b_1 & (1 - a_1 - b_1) & a_1 & \\cdots & 0 & 0 & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\cdots & b_{n-1} & (1 - a_{n-1} - b_{n-1}) & a_{n-1}\\\\\n",
    "0 & 0 & 0 & \\cdots & 0 & 0 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    ">\n",
    "> Denote by $x_i$ the probability of reaching state $N$ when starting from $i$.\n",
    ">\n",
    "> From transition matrix $P$ above, we can see that $x_i$ must satisfy:\n",
    ">\n",
    "> $x_0 = 0$\n",
    ">\n",
    "> $x_i = b_i x_{i-1} + (1 - a_i - b_i) x_i + a_i x_{i+1}$\n",
    ">\n",
    "> $x_N = 1$\n",
    ">\n",
    "> The fixation rate for a mutant B in a population of type A is clearly $x_1$\n",
    ">\n",
    "> We can solve for $x_i$ by rewriting the above as $b_i x_i - b_i  x_{i-1} = a_i x_{i+1} - a_i x_i$.\n",
    "> \n",
    "> We can denote $y_i = x_i - x_{i-1}$ to simplify the above to $y_{i+1} = \\frac{b_i}{a_i} y_i$\n",
    ">\n",
    "> Notice that $\\sum_{i=1}^N{y_i} = x_N - x_0 = 1$ and that $y_1 = x_1$\n",
    ">\n",
    "> We can use the above to write\n",
    "\\begin{equation}\n",
    "x_1 + {\\sum_{i=2}^N{y_i}} = x_1 (1 + {\\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}}}) = 1\n",
    "\\end{equation}\n",
    ">\n",
    "> And so\n",
    "\\begin{equation}\n",
    "x_1 = \\frac{1}{(1 + \\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}})}\n",
    "\\end{equation}\n",
    ">\n",
    "> Note that $x_1$ is the fixation rate for a mutant $B$ in a population of type $A$, often denoted as $\\rho$.\n",
    ">\n",
    "> *Also note that $1 - x_{N-1}$ is the fixation rate for a mutant $A$ in a population of type $B$. We could find expressions for all $x_i$ if we note that $x_i = x_1 (1 + \\sum_{j=1}^{i-1}{\\prod_{k=1}^{j} \\frac{b_k}{a_k}})$ (see Nowak 2006 for further details).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b526b83-8fe7-4a4e-bf12-228f4879dceb",
   "metadata": {},
   "source": [
    "We can use our definitions above to determine when the fixation rate for a mutant $B$ in a population of type $A$ is greater than that for a mutant $A$ in a population of type $B$. \n",
    "\n",
    "This condition requires that $x_1 > 1 - x_{N-1}$, i.e. $\\frac{1}{(1 + \\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}})} > \\frac{\\prod_{j=1}^{N-1} \\frac{b_j}{a_j}}{(1 + \\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}})}$.\n",
    "\n",
    "Using the fermi social learning rule and the aforementioned simplifications, we can see that this condition holds true whenever $1 > \\exp^{-\\beta \\sum_{j=1}^{N-1}{\\Pi_B(j) - \\Pi_A(j)}}$ which implies $\\sum_{j=1}^{N-1}{\\Pi_B(j)} > \\sum_{j=1}^{N-1}{\\Pi_A(j)}$.\n",
    "\n",
    "Lastly, we can make use of the equation $\\sum_{j=1}^{N-1}{j}=\\frac{(N-1) N}{2}$ to simplify this condition to $\\pi_{BA} + \\pi_{BA} > \\pi_{AA} + \\pi_{AB}$\n",
    "\n",
    "This is exactly the risk dominance condition implied by 2 by 2 payoff matrices. The risk dominance condition has been used in the literature to offer a reason to motivate selecting one monomorphic equilibria over another in such games. In such games there is a precise connection between risk dominance and the monomorphic equilibria selected for by social learning. This connection disappears in games with larger payoff matrices (which is why theorists tends to consider the concept of stochastic stability instead, perhaps using Young's method (Young 2003)).\n",
    "\n",
    "Even in games with more than 2 players (or populations), we can make use of this condition to tell us in which direction the fixation rate is stronger between two strategies. At times, this is enough to gain an intuition for the gradient of selection present in polymorphic states where multiple strategies coexist in one or more populations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd408dd8-b57b-4fce-9100-758299ea3c18",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b2af0-7469-4788-bd6b-96d143e27ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "T_type = list[nptyping.NDArray[nptyping.Shape[\"N_models\"], typing.Any]]\n",
    "\n",
    "def fixation_rate(Tplus: T_type, # A list of NDarrays, one array (of size n_models) for each possible number of mutants in the population; the probability of gaining one mutant\n",
    "                  Tneg: T_type, # A list of NDarrays, one array (of size n_models) for each possible number of mutants in the population; the probability of losing one mutant\n",
    "                 ) -> nptyping.NDArray[nptyping.Shape[\"N_models\"], typing.Any]: # Fixation rates for the given strategy in each model\n",
    "    \"\"\"Calculate the likelihood that a mutant invades the population.\"\"\"\n",
    "    Z = len(Tplus) + 1\n",
    "    ρ = (np.sum([np.prod([Tneg[j-1]/Tplus[j-1]\n",
    "                         for j in range(1,i+1)],\n",
    "                        axis=0,\n",
    "                        keepdims=False)\n",
    "                 for i in range(1,Z)],\n",
    "                axis=0,\n",
    "                keepdims=False)\n",
    "        + 1)**-1\n",
    "    return ρ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236065c-1875-4503-b790-8b59188e13b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### fixation_rate\n",
       "\n",
       ">      fixation_rate (Tplus:list[nptyping.base_meta_classes.NDArray],\n",
       ">                     Tneg:list[nptyping.base_meta_classes.NDArray])\n",
       "\n",
       "Calculate the likelihood that a mutant invades the population.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Tplus | list | A list of NDarrays, one array (of size n_models) for each possible number of mutants in the population; the probability of gaining one mutant |\n",
       "| Tneg | list | A list of NDarrays, one array (of size n_models) for each possible number of mutants in the population; the probability of losing one mutant |\n",
       "| **Returns** | **NDArray** | **Fixation rates for the given strategy in each model** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### fixation_rate\n",
       "\n",
       ">      fixation_rate (Tplus:list[nptyping.base_meta_classes.NDArray],\n",
       ">                     Tneg:list[nptyping.base_meta_classes.NDArray])\n",
       "\n",
       "Calculate the likelihood that a mutant invades the population.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Tplus | list | A list of NDarrays, one array (of size n_models) for each possible number of mutants in the population; the probability of gaining one mutant |\n",
       "| Tneg | list | A list of NDarrays, one array (of size n_models) for each possible number of mutants in the population; the probability of losing one mutant |\n",
       "| **Returns** | **NDArray** | **Fixation rates for the given strategy in each model** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(fixation_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeab27b-5b89-424e-981d-0a6d1b161e7e",
   "metadata": {},
   "source": [
    "#### Examples and Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005877f9-bb91-4f68-a982-a5d50ca48f71",
   "metadata": {},
   "source": [
    "When the chance of gaining a mutant always equals the chance of losing a mutant, then the fixation rate will be $\\frac{1}{Z}$\n",
    "\n",
    "Note that because we have to sample the population for a mutant and the player of the type being invaded, the chance of gaining or losing a mutant can be no greater than $\\frac{k}{Z} \\frac{Z-k}{Z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4272f-1d85-416b-b424-53d4d3186b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 2 # With Z=2, we only need to evaluate Tplus and Tneg for when k=1\n",
    "Tplus_example = [np.array([1/8])]\n",
    "Tneg_example =  [np.array([1/8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d9199-f2e7-4dfd-927e-f6c21fc83454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|  hide\n",
    "# validate test inputs\n",
    "assert len(Tplus_example) == len(Tneg_example)\n",
    "for tplus, tneg in zip(Tplus_example, Tneg_example):\n",
    "    assert tplus.shape == tneg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f41700-0399-4be0-9992-5a111cc086c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixation_rate_result = fixation_rate(Tplus_example, Tneg_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97469d4-36bf-4d67-9e1f-d39651db1b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "nptyping.assert_isinstance(fixation_rate_result,\n",
    "                           nptyping.NDArray[nptyping.Shape[\"1\"], typing.Any])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36d254-7f82-4f86-bd1f-c5bcd6d679e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcore.test.test_eq(fixation_rate_result, np.array([0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9516d8-2e23-4468-8086-54e4dcd59c8a",
   "metadata": {},
   "source": [
    "When the chance of gaining a mutant is half the chance of losing a mutant, then the fixation rate will be\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\frac{1}{(1 + \\sum_{j=1}^{Z-1}{2^j})}\n",
    "\\end{equation}\n",
    "\n",
    "When $Z=2$, we have $\\rho = \\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c929e5-3ab3-48ae-9dba-002c0054f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 2 # With Z=2, we only need to evaluate Tplus and Tneg for when k=1\n",
    "Tplus_example = [np.array([0.1])]\n",
    "Tneg_example =  [np.array([0.2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ccc5de-08ce-4d23-bcc4-0f19eb93f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|  hide\n",
    "# validate test inputs\n",
    "assert len(Tplus_example) == len(Tneg_example)\n",
    "for i, tplus in enumerate(Tplus_example):\n",
    "    assert tplus.shape == Tneg_example[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ff240-e534-4220-bd37-fb42cb30caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixation_rate_result = fixation_rate(Tplus_example, Tneg_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca864e7-693c-4787-9eb7-c3416c5245fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcore.test.test_eq(fixation_rate_result, np.array([1/3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2cff06-3c08-458d-a02a-e68a1c518211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "nptyping.assert_isinstance(fixation_rate_result,\n",
    "                           nptyping.NDArray[nptyping.Shape[\"1\"], typing.Any])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f635d3-37a5-4969-a784-1875e9486665",
   "metadata": {},
   "source": [
    "We could instead consider an example where we have a mutant Defector (D) who appears in a population of Cooperators (C) playing a standard Prisoner's Dilemma.\n",
    "\n",
    "We will consider an example of such a scenario where chance of gaining/losing a D player be given by $\\frac{1}{1 + e^{\\pm \\beta \\frac{Z+1}{Z-1}}}$.\n",
    "\n",
    "The fixation rate will be given by the following expression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\frac{1}{1 + \\sum_{j=1}^{Z-1}{(\\frac{1 + e^{- \\beta \\frac{Z+1}{Z-1}}}{1 + e^{\\beta \\frac{Z+1}{Z-1}}})^j}}\n",
    "\\end{equation}\n",
    "\n",
    "For this example, we will let $\\beta=1$ and $Z=10$, so $\\beta \\frac{Z+1}{Z-1} = \\frac{11}{9}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3006c88-0cde-4e32-934e-59c97c91df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "β = 1\n",
    "Z = 10\n",
    "ρ_CD = 1 / (1 + sum((1 + np.exp(- β * (Z + 1) / (Z-1)))**j \n",
    "                    / (1 + np.exp(β * (Z + 1) / (Z-1)))**j\n",
    "                    for j in range(1, Z)))\n",
    "Tplus_example = [np.array([1 / (1 + np.exp(- β * (Z + 1) / (Z-1)))])\n",
    "                 for _ in range(Z-1)]\n",
    "Tneg_example =  [np.array([1 / (1 + np.exp(β * (Z + 1) / (Z-1)))])\n",
    "                 for _ in range(Z-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5b49e-be35-475b-80c1-249b248672dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|  hide\n",
    "# validate test inputs\n",
    "assert len(Tplus_example) == len(Tneg_example)\n",
    "for i, tplus in enumerate(Tplus_example):\n",
    "    assert tplus.shape == Tneg_example[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ec211-bf09-48b8-b14a-1f7d9f2a2e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "nptyping.assert_isinstance(fixation_rate(Tplus_example, Tneg_example),\n",
    "                           nptyping.NDArray[nptyping.Shape[\"1\"], typing.Any])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa15aa-41f1-478e-8d5f-edb6f7902064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastcore.test.is_close(fixation_rate(Tplus_example, Tneg_example), ρ_CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c7ecc-6786-44ea-b479-7f8eeec3cc0d",
   "metadata": {},
   "source": [
    "Finally, it is useful to know how the fixation rate behaves when any elements of Tplus are zero (as the fixation rate divides by those elements). Even though the Fermi learning rule we use theoretically gives a number between 0 and 1 exclusive, in practise the number may underflow to a 0 if low enough. This will cause unexpected behaviour if we allow it in our alogorithm for computing the transition matrix.\n",
    "\n",
    "We can avoid this issue by using a slightly altered method for calculating the fixation rate, taking advantage of our choice to use the `fermi_learning` rule.\n",
    "\n",
    "In the above fixation rate calculations we used the `fermi_learning` function to calculate the probability of a player with strategy $D$ adopting strategy $C$ (and likewise for the probability of a player with $C$ adopting $D$). Their ratio takes the form, $\\frac{1 + e^x}{1 + e^{-x}}$. It is not too hard to verify that $\\frac{1 + e^x}{1 + e^{-x}} = e^x$.\n",
    "\n",
    "Moreover, we can avoid taking the product of the ratios at all, since the product of exponentials (with the same base) is just the exponential of the sum of their exponents.\n",
    "\n",
    "By using the above substitution and algebraic manipulation, we can substantially mitigate the numerical stability issues. For this reason, we will not use `fermi_learning` nor `fixation_rate` in our algorithm at all (although in most cases we would expect these methods to yield the same answers). Instead, we will use `fixation_rate_stable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822d05b-aa2c-407a-babb-0fa6dc2b7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fixation_rate_stable(ΠA:list, # Average payoffs for the strategy A they consider adopting for each number of mutants following A\n",
    "                         ΠB:list, # Average payoffs for the strategy B that the player currently follows for each number of mutants following A\n",
    "                         β:Array1D, # learning rate \n",
    "                        ):\n",
    "    \"\"\"Calculate the likelihood that a mutant B invades population A\n",
    "    using a numerically stable method.\"\"\"\n",
    "    fastcore.test.test_eq(len(ΠA), len(ΠB))\n",
    "    Z = len(ΠA) + 1\n",
    "    ρ = (np.sum([np.exp(np.clip(np.sum([-β*(ΠB[j-1] - ΠA[j-1])\n",
    "                                        for j in range(1,i+1)],\n",
    "                                       axis=0,\n",
    "                                       keepdims=False),\n",
    "                                -500,\n",
    "                                500)) # avoid underflow/overflow warnings\n",
    "                 for i in range(1,Z)],\n",
    "                axis=0,\n",
    "                keepdims=False)\n",
    "        + 1)**-1\n",
    "    return ρ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f22eb-9d81-4cd6-86e1-c86cefbdf7b1",
   "metadata": {},
   "source": [
    "We can see in the examples which follow that both methods usually give the same answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97c8c5-cd72-48da-87ed-d17e2c47b950",
   "metadata": {},
   "source": [
    "To match an earlier example where `Tplus` and `Tneg` were both equal to $\\frac{1}{8}$ (as $Z=2$ we only need to consider one value for each when $k=1$), we let $\\beta=1$ and recall that $T^+_B(k) = \\frac{Z-k}{Z} \\frac{k}{Z} Pr(adopt \\, B | k) = \\frac{Z-k}{Z} \\frac{k}{Z} \\frac{1}{1 + \\exp^{-\\beta (ΠB(k) - ΠA(k))}} $\n",
    "\n",
    "We can then say that $ΠA - ΠB = \\log{(\\frac{1}{\\frac{4}{8}} - 1)} = \\log{\\frac{4}{4}} = \\log{4} - \\log{4}$\n",
    "\n",
    "Notice that to achieve netural drift, the payoffs have to be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6a3ee-2171-4ce2-a9f7-b9ad627f71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 2\n",
    "β = 1\n",
    "ΠA = [np.array([np.log(4)])]\n",
    "ΠB = [np.array([np.log(4)])]\n",
    "result = fixation_rate_stable(ΠA, ΠB, β)\n",
    "fastcore.test.test_close(result, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6a8b1-95c7-48fe-a235-b36a929e1dd1",
   "metadata": {},
   "source": [
    "We can also consider an example from a payoff matrix I've run into in practise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccbfcc-91c7-4849-96ed-d3e3b2c1dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payoffs = np.array([[51, 0.6, 51],\n",
    "                    [114.3, 57.75, 39.38],\n",
    "                    [51, 0.99798, 51]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526bcead-17a7-4305-9d09-ec9e8723e672",
   "metadata": {},
   "source": [
    "We are interested in the fixation rate of a mutant B in a population of A\n",
    "\n",
    "Strategy A is the strategy represented by row 3\n",
    "\n",
    "Strategy B is the strategy represented by row 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ae521-1122-484a-9b61-8f5405b479a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 100\n",
    "β = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f38ed-56ca-4ad5-b1f2-d2e5fe19c5dd",
   "metadata": {},
   "source": [
    "We need only the average payoffs for the stable calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd49060-30d6-4b2b-a395-cef069f36a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ΠA = [k/(Z-1) * payoffs[2,1] + (Z-k-1)/(Z-1) * payoffs[2,2]\n",
    "      for k in range(1, Z)]\n",
    "ΠB = [(k-1)/(Z-1) * payoffs[1,1] + (Z-k)/(Z-1) * payoffs[1,2]\n",
    "      for k in range(1, Z)]\n",
    "\n",
    "result_stable = fixation_rate_stable(ΠA, ΠB, β)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632080d7-60ac-4ca5-976c-e90a7087c5a5",
   "metadata": {},
   "source": [
    "We also need the adoption rates for the unstable calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007792b-11a9-4ff4-97fc-8f5efaa0823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tneg = [fermi_learning(ΠB[k-1], ΠA[k-1], β)\n",
    "                     for k in range(1, Z)]\n",
    "Tplus = [fermi_learning(ΠA[k-1], ΠB[k-1], β)\n",
    "         for k in range(1, Z)]\n",
    "\n",
    "# Naiive and unstable calculation\n",
    "result_unstable = fixation_rate(Tplus, Tneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007dbe6-4a3f-4f39-8ffe-88c4ba52df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcore.test.test_close(result_stable, 0)\n",
    "fastcore.test.test_close(result_unstable, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c827802-97f6-4bf4-a626-3e8db969fb97",
   "metadata": {},
   "source": [
    "### Build transition matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8d5bc-dfd0-4a4b-a750-9ecec7fdd117",
   "metadata": {},
   "source": [
    "Recall that step 1 of finding the solution to the Evolutionary Game dynamics is to build a transition matrix between all monomorphic states. \n",
    "\n",
    "The transition matrix captures the probability that if the population of the Evolutionary Game transitions to another state. We read an entry of the transition matrix as saying the probability of transitioning from the row state to column state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932859e9-a0bb-48c3-9853-7ff72c997165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ModelTypeEGT():\n",
    "    \"\"\"This is the schema for an Evolutionary Game Theory model.\n",
    "    \n",
    "    Note: This schema is not enforced and is here purely for documentation\n",
    "    purposes.\"\"\"\n",
    "    def __init__(self, \n",
    "                 Z: int, # the size of the population\n",
    "                 strategy_set: list[str], # the set of strategies in the model\n",
    "                 β: Array1D, # the learning rate\n",
    "                 payoffs: Array3D, # the payoffs of the game\n",
    "                 transition_matrix: Array3D=None, # the model's transition matrix\n",
    "                 ergodic: Array2D=None, # ergodic distribution of the model's markov chain\n",
    "                ):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aaa4a8-200c-4bbe-b6bb-34ed13151248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L69){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ModelTypeEGT\n",
       "\n",
       ">      ModelTypeEGT (Z:int, strategy_set:list[str],\n",
       ">                    β:gh_pages_example.types.Array1D,\n",
       ">                    payoffs:gh_pages_example.types.Array3D,\n",
       ">                    transition_matrix:gh_pages_example.types.Array3D=None,\n",
       ">                    ergodic:gh_pages_example.types.Array2D=None)\n",
       "\n",
       "This is the schema for an Evolutionary Game Theory model.\n",
       "\n",
       "Note: This schema is not enforced and is here purely for documentation\n",
       "purposes.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Z | int |  | the size of the population |\n",
       "| strategy_set | list |  | the set of strategies in the model |\n",
       "| β | Array1D |  | the learning rate |\n",
       "| payoffs | Array3D |  | the payoffs of the game |\n",
       "| transition_matrix | Array3D | None | the model's transition matrix |\n",
       "| ergodic | Array2D | None | ergodic distribution of the model's markov chain |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L69){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ModelTypeEGT\n",
       "\n",
       ">      ModelTypeEGT (Z:int, strategy_set:list[str],\n",
       ">                    β:gh_pages_example.types.Array1D,\n",
       ">                    payoffs:gh_pages_example.types.Array3D,\n",
       ">                    transition_matrix:gh_pages_example.types.Array3D=None,\n",
       ">                    ergodic:gh_pages_example.types.Array2D=None)\n",
       "\n",
       "This is the schema for an Evolutionary Game Theory model.\n",
       "\n",
       "Note: This schema is not enforced and is here purely for documentation\n",
       "purposes.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Z | int |  | the size of the population |\n",
       "| strategy_set | list |  | the set of strategies in the model |\n",
       "| β | Array1D |  | the learning rate |\n",
       "| payoffs | Array3D |  | the payoffs of the game |\n",
       "| transition_matrix | Array3D | None | the model's transition matrix |\n",
       "| ergodic | Array2D | None | ergodic distribution of the model's markov chain |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ModelTypeEGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525269fc-1afe-4e7b-9e72-c7f6f6596cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@multi\n",
    "def build_transition_matrix(models:dict # A dictionary that contains the parameters in `ModelTypeEGT`\n",
    "                           ):\n",
    "    \"\"\"Build a transition matrix between all monomorphic states using the\n",
    "    fermi social learning rule.\"\"\"\n",
    "    return models.get('dispatch-type')\n",
    "    \n",
    "\n",
    "@method(build_transition_matrix)\n",
    "def build_transition_matrix(models:dict # A dictionary that contains the parameters in `ModelTypeEGT`\n",
    "                           ):\n",
    "    \"\"\"Build a transition matrix between all monomorphic states\n",
    "    using the fermi social learning rule for each model.    \n",
    "    \"\"\"\n",
    "    \n",
    "    Z, S, β = [models[k] for k in ['Z','strategy_set', 'β']]\n",
    "    π = models['payoffs']\n",
    "    n_models = π.shape[0]\n",
    "    M = np.zeros(( n_models, len(S), len(S)))\n",
    "    for row_ind, s in enumerate(S):\n",
    "        for col_ind, sₒ in enumerate(S):\n",
    "            if row_ind == col_ind:\n",
    "                M[:, row_ind, row_ind] += 1\n",
    "                # We calibrate these entries later so rows add up to 1\n",
    "                continue\n",
    "            πAA = π[:, row_ind, row_ind]\n",
    "            πAB = π[:, row_ind, col_ind]\n",
    "            πBA = π[:, col_ind, row_ind]\n",
    "            πBB = π[:, col_ind, col_ind]\n",
    "            ΠA = [πAA*(Z-k-1)/(Z-1) + πAB*k/(Z-1)\n",
    "                  for k in range(1, Z)]\n",
    "            ΠB = [πBA*(Z-k)/(Z-1)  + πBB*(k-1)/(Z-1)\n",
    "                  for k in range(1, Z)]\n",
    "            # We use a numerically stable method to find the fixation rate, ρ.\n",
    "            # ρ is the probability that mutant B successfully invades A\n",
    "            ρ = fixation_rate_stable(ΠA, ΠB, β)\n",
    "            M[:, row_ind, col_ind] = ρ / max(1, len(S)-1)\n",
    "            M[:, row_ind, row_ind] -= ρ / max(1, len(S)-1)\n",
    "    return {**models, \"transition_matrix\": M}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e727d1b-c3a9-411c-89b4-e4483a4d32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@method(build_transition_matrix, 'unstable')\n",
    "def build_transition_matrix(models:dict # A dictionary that contains the parameters in `ModelTypeEGT`\n",
    "                           ):\n",
    "    \"\"\"Build a transition matrix using a numerically unstable method.\"\"\"\n",
    "    \n",
    "    Z, S, β = [models[k] for k in ['Z','strategy_set', 'β']]\n",
    "    π = models['payoffs']\n",
    "    n_models = π.shape[0]\n",
    "    M = np.zeros(( n_models, len(S), len(S)))\n",
    "    for row_ind, s in enumerate(S):\n",
    "        for col_ind, sₒ in enumerate(S):\n",
    "            if row_ind == col_ind:\n",
    "                M[:, row_ind, row_ind] += 1\n",
    "                # We calibrate these entries later so rows add up to 1\n",
    "                continue\n",
    "            πAA = π[:, row_ind, row_ind]\n",
    "            πAB = π[:, row_ind, col_ind]\n",
    "            πBA = π[:, col_ind, row_ind]\n",
    "            πBB = π[:, col_ind, col_ind]\n",
    "            ΠA = [πAA*(Z-k-1)/(Z-1) + πAB*k/(Z-1)\n",
    "                  for k in range(1, Z)]\n",
    "            ΠB = [πBA*(Z-k)/(Z-1)  + πBB*(k-1)/(Z-1)\n",
    "                  for k in range(1, Z)]\n",
    "            Tneg = [fermi_learning(ΠB[k-1], ΠA[k-1], β)\n",
    "                    for k in range(1, Z)]\n",
    "            Tplus = [fermi_learning(ΠA[k-1], ΠB[k-1], β)\n",
    "                     for k in range(1, Z)]\n",
    "            ρ = fixation_rate(Tplus, Tneg)\n",
    "            M[:, row_ind, col_ind] = ρ / max(1, len(S)-1)\n",
    "            M[:, row_ind, row_ind] -= ρ / max(1, len(S)-1)\n",
    "    return {**models, \"transition_matrix\": M}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfebf8-5411-4d33-89b6-cf010a272b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L188){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### build_transition_matrix\n",
       "\n",
       ">      build_transition_matrix (models:dict)\n",
       "\n",
       "Build a transition matrix between all monomorphic states using the\n",
       "fermi social learning rule.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| models | dict | A dictionary that contains the parameters in `ModelTypeEGT` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L188){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### build_transition_matrix\n",
       "\n",
       ">      build_transition_matrix (models:dict)\n",
       "\n",
       "Build a transition matrix between all monomorphic states using the\n",
       "fermi social learning rule.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| models | dict | A dictionary that contains the parameters in `ModelTypeEGT` |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(build_transition_matrix.__dispatch_fn__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a1361-4d6c-4eb5-8c6f-6df0a2ab8938",
   "metadata": {},
   "source": [
    "#### Examples and Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25222a93-d57e-47b9-abb6-d1280b5d99a3",
   "metadata": {},
   "source": [
    "Consider the following two examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182c962-85a7-4812-aa09-d5a9487909fd",
   "metadata": {},
   "source": [
    "**Example 1**\n",
    "\n",
    "Let all payoffs be equal in the game's payoff matrix. All expected payoffs will be equal too.\n",
    "\n",
    "So, Fermi learning will say that each individual has a 50% chance of adopting the behaviour of the one they observe.\n",
    "\n",
    "We therefore have an equal chance during each epoch of gaining or losing an individual of the given type, in this example we denote the type as $s \\in \\{A, B\\}$, although this probability depends on population size $Z$ and the current number of individuals of that type, $k$, $T^+_s(k) = T^-_s(k) = \\frac{Z-k}{Z} \\frac{k}{Z} \\frac{1}{2}$.\n",
    "\n",
    "Recall that we calculate the fixation rate, $\\rho$ as follows:\n",
    "\\begin{equation}\n",
    "\\rho = \\frac{1}{1 + \\sum_{j=1}^{N-1}{\\prod_{k=1}^{j} \\frac{b_k}{a_k}}}\n",
    "\\end{equation}\n",
    "where $N=Z$, $b_k = T^-_s(k)$ and $a_k = T^+_s(k)$\n",
    "\n",
    "In this example, for each strategy $s$, $T^-_s(k) = T^+_s(k), \\, \\forall k$, so $\\rho = \\frac{1}{Z}$.\n",
    "\n",
    "We only have $2$ strategies, and $Z=10$, so the final transition matrix will look like\n",
    "\n",
    "\\begin{equation}\n",
    "M \\, = \\, \\begin{pmatrix}\n",
    "1 - \\frac{\\rho}{2 - 1} & \\frac{\\rho}{2 - 1} &\\\\\n",
    "\\frac{\\rho}{2 - 1} & 1 - \\frac{\\rho}{2 - 1}\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "0.9 & 0.1 &\\\\\n",
    "0.1 & 0.9\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the above example describes neutral drift, the idea that even if there is no advantage to be gained from any particular strategy, social learning can still result in the spread of that behaviour. Neutral drift also occurs if we set the Fermi learning rate $\\beta = 0$, no matter what payoff matrix describes the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b51afb-7c71-4fed-a636-a67705304ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "payoffs = np.array([[[2, 2],\n",
    "                     [2, 2]]\n",
    "                   ])\n",
    "Z = 10\n",
    "β = 1\n",
    "models = {\"payoffs\": payoffs,\n",
    "          \"Z\": Z,\n",
    "          \"β\": β,\n",
    "          \"strategy_set\": [\"A\", \"B\"],\n",
    "         }\n",
    "result = build_transition_matrix(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d41c9a-9ad4-482f-a003-63eb6477a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcore.test.test_close(result['transition_matrix'],\n",
    "                         np.array([[0.9, 0.1],\n",
    "                                   [0.1, 0.9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a32d142-e5ae-455d-b782-117edb407a10",
   "metadata": {},
   "source": [
    "**Example 2**\n",
    "\n",
    "Let the payoff matrix be akin to a Prisoner's Dilemma with two strategies, $C$ or $D$ (Cooperate or Defect respectively):\n",
    "\n",
    "\\begin{pmatrix}\n",
    "2 & 0\\\\\n",
    "3 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "Again, for this simple example, the relative average success of strategy $C$ is independent of the number of $C$ players, $k$. This is rarely the case in practise but permits a more legible example.\n",
    "\n",
    "$C$'s relative success over $D$ will be $\\frac{2 (k-1)}{Z-1} - \\frac{3 k + (Z - k - 1)}{Z-1} = - \\frac{Z + 1}{Z-1}$.\n",
    "\n",
    "Fermi learning means the probability of a $D$ player adopting what they see $C$ do is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{1 + e^{- \\beta (\\Pi_C(k) - \\Pi_D(k))}} = \\frac{1}{1 + e^{\\beta \\frac{Z + 1}{Z-1}}}\n",
    "\\end{equation}\n",
    "\n",
    "The fixation rate for mutant $C$ in a population of $D$ players, $\\rho_{DC}$, can be computed as\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{DC} = \\frac{1}{1 + \\sum_{j=1}^{Z-1}{(\\frac{1 + e^{\\beta \\frac{Z + 1}{Z-1}}}{1 + e^{-\\beta \\frac{Z + 1}{Z-1}}})^j}}\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, the fixation rate for mutant $D$ in a population of $C$ players, $\\rho_{CD}$, can be computed as \n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{CD} = \\frac{1}{1 + \\sum_{j=1}^{Z-1}{(\\frac{1 + e^{-\\beta \\frac{Z + 1}{Z-1}}}{1 + e^{\\beta \\frac{Z + 1}{Z-1}}})^j}}\n",
    "\\end{equation}\n",
    "\n",
    "For $Z=10$ and $\\beta = 1$, the above yields the following transition matrix,\n",
    "\n",
    "\\begin{equation}\n",
    "M \\, = \\, \\begin{pmatrix}\n",
    "1 - \\frac{\\rho_{CD}}{2 - 1} & \\frac{\\rho_{CD}}{2 - 1} &\\\\\n",
    "\\frac{\\rho_{DC}}{2 - 1} & 1 - \\frac{\\rho_{DC}}{2 - 1}\\\\\n",
    "\\end{pmatrix}\n",
    "\\approx \\begin{pmatrix}\n",
    "0.295 & 0.705 &\\\\\n",
    "0.000 & 1.000\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de05f4-e0b9-4cbb-963c-18b83039e7d4",
   "metadata": {},
   "source": [
    "\n",
    "Note how in the above fixation rate calculations how we used the `fermi_learning` function to calculate the probability of a player with strategy $D$ adopting strategy $C$ (and likewise for the probability of a player with $C$ adopting $D$). This function has special properties which aid us in calculating the fixation rate.\n",
    "\n",
    "Notice how the ratio of the two adoption rates takes the form, $\\frac{1 + e^x}{1 + e^{-x}}$. It is not too hard to verify that $\\frac{1 + e^x}{1 + e^{-x}} = e^x$.\n",
    "\n",
    "We utilities this property to considerably improve the numerical stability of our algorithm for building a transition matrix. For this reason, we do not use `fermi_learning` in our algorithm at all.\n",
    "\n",
    "We can similarly note that $\\frac{1}{1 + e^{-x}} = 1 - \\frac{1}{1 + e^{x}}$, i.e. the two adoption rates are complementary probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e885818-dcfd-4443-9ae3-2c616b8e976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payoffs = np.array([[[2, 0],\n",
    "                     [3, 1]],\n",
    "                   ])\n",
    "Z = 10\n",
    "β = 1\n",
    "models = {\"payoffs\": payoffs,\n",
    "          \"Z\": Z,\n",
    "          \"β\": β,\n",
    "          \"strategy_set\": [\"C\", \"D\"],\n",
    "         }\n",
    "result = build_transition_matrix(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9b18d-a668-430f-8fcf-a9702fbfd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ρ_CD = 1 / (1 + sum((1 + np.exp(- β * (Z + 1) / (Z-1)))**j \n",
    "                    / (1 + np.exp(β * (Z + 1) / (Z-1)))**j\n",
    "                    for j in range(1, Z)))\n",
    "ρ_DC = 1 / (1 + sum((1 + np.exp(β * (Z + 1) / (Z-1)))**j\n",
    "                    / (1 + np.exp(- β * (Z + 1) / (Z-1)))**j \n",
    "                    for j in range(1, Z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de16cf-c528-4dfd-aeb2-e349a3811415",
   "metadata": {},
   "outputs": [],
   "source": [
    "ρ_CD_alt = 1 / (1 + sum(np.exp(- j * β * (Z + 1) / (Z-1))\n",
    "                        for j in range(1, Z)))\n",
    "ρ_DC_alt = 1 / (1 + sum(np.exp(j * β * (Z + 1) / (Z-1))\n",
    "                        for j in range(1, Z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ede78a-9d20-4bfa-92b3-61ae7a4096a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcore.test.test_close(ρ_CD, ρ_CD_alt)\n",
    "fastcore.test.test_close(ρ_DC, ρ_DC_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed5ac3-48a9-47ea-86b8-d3a1135077fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcore.test.test_close(result['transition_matrix'],\n",
    "                         np.array([[1- ρ_CD, ρ_CD],\n",
    "                                   [ρ_DC, 1 - ρ_DC]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e68e50-329b-4e8d-b3cb-9b81e051384c",
   "metadata": {},
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81be48-0a09-488f-94f0-dc60127c454e",
   "metadata": {},
   "source": [
    "Here is an additional example for the 3 by 3 matrix we discussed when testing other functions.\n",
    "\n",
    "This time, we make sure we get the correct probabilities for each transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94eb04-dcff-4d54-a218-d90c5137eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "payoffs = np.array([[[51, 0.6, 51],\n",
    "                     [114.3, 57.75, 39.38],\n",
    "                     [51, 0.99798, 51]],\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dd779-f4db-4347-a0a2-e78a9bd85738",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = np.array([[[0.495, 0.5, 0.005],\n",
    "                     [0, 1, 0],\n",
    "                     [0.005, 0, 0.995]],\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abd346-2a8a-4f0d-8136-7a56fa4f4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 100\n",
    "β = 1\n",
    "models = {\"payoffs\": payoffs,\n",
    "          \"Z\": Z,\n",
    "          \"β\": β,\n",
    "          \"strategy_set\": [\"AS\", \"AU\", \"PS\"],\n",
    "         }\n",
    "result = build_transition_matrix(models)\n",
    "fastcore.test.test_close(result['transition_matrix'], expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf604f74-2729-4ac9-bbfc-f1cf1fa4a9fc",
   "metadata": {},
   "source": [
    "### Find ergodic strategy distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aced671-f623-440b-ab31-c0d200ee0729",
   "metadata": {},
   "source": [
    "Step 2 is to find the ergodic distribution for the Evolutionary Game using the transition matrix we constructed in step 1.\n",
    "\n",
    "Let $M$ denote the transition matrix, and $\\omega_t$ be the column vector describing the proportions with which each strategy is played in the population.\n",
    "\n",
    "We can describe the evolution of this system with $\\omega_{t+1} = M^T \\omega_t$, i.e. the proportion of players that use a given strategy in the next round will be equal to the sum of the proportions of players for each strategy who adopted that strategy in the current round. Equivalently, we can also consider $\\omega_t$ as describing the probabilities that the system at time t is in each of the monomorphic states.\n",
    "\n",
    "As each of the monomporphic states described in the transition matrix is reachable from any other with some probability and since the transition probabilities only depend on the current state, what we have is a markov chain which is irreducible.\n",
    "\n",
    "The ergodicity theorem guarantees that such irreducible and aperiodic markov chains have an ergodic distribution that the system converges to, no matter where it starts. An ergodic distribution (also called a stationary distribution),  $\\omega^*$ satisfies  $\\omega^* = M^T \\omega^*$ [[1]](https://gregorygundersen.com/blog/2019/10/28/ergodic-markov-chains/) [[2]](http://www.stat.columbia.edu/~liam/teaching/neurostat-spr11/papers/mcmc/Ergodicity_Theorem.pdf) [[3]](https://textbooks.math.gatech.edu/ila/1553/stochastic-matrices.html).\n",
    "\n",
    "Our ergodic distribution, $\\omega^*$, is therefore defined as the normalised right-hand eigenvector with eigenvalue 1 of the transposed transition matrix, $M^T$ (or equivalently, if we defined $\\omega$ as a row vector instead, $\\omega^*$ would be the left-hand eigenvector with eigenvalue 1 of transition matrix, $M$; numerical computing packages usually return the right-hand eigenvectors more directly, which is why I used the other formalism).\n",
    "\n",
    "We use standard linear algebra methods from the [numpy](https://numpy.org/) package to find this eigenvector. These numerical methods will usually not return an eigenvector which is normalised to sum to 1, so we must normalise the eigenvector we are given. See their documentation to learn more about these numerical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c98786-9d06-4471-b322-a90e521e6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_ergodic_distribution(models:dict # A dictionary that contains the parameters in `ModelTypeEGT`\n",
    "                             ):\n",
    "    \"\"\"Find the ergodic distribution of a markov chain with the\n",
    "    given transition matrix.\"\"\"\n",
    "    M = models[\"transition_matrix\"]\n",
    "    # find unit eigenvector of markov chain\n",
    "    Λ,V = np.linalg.eig(M.transpose(0,2,1))\n",
    "    V = np.real_if_close(V)\n",
    "    x = np.isclose(Λ, 1)\n",
    "    # if multiple unit eigenvalues then choose the first\n",
    "    y = np.zeros_like(x, dtype=bool)\n",
    "    idx = np.arange(len(x)), x.argmax(axis=1)\n",
    "    y[idx] = x[idx]\n",
    "    ergodic = np.array(V.transpose(0,2,1)[y], dtype=float)\n",
    "    # ensure ergodic frequencies are positive and sum to 1\n",
    "    ergodic = np.abs(ergodic) / np.sum(np.abs(ergodic), axis=1)[:, None]\n",
    "    return {**models, 'ergodic':ergodic}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249dbbc-12f1-4cee-8488-c5f755216cf9",
   "metadata": {},
   "source": [
    "#### Examples and Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2ae7f-f94f-4b54-90eb-ec4af9077aba",
   "metadata": {},
   "source": [
    "Let our transition matrix, $M$ be\n",
    "\n",
    "\\begin{equation}\n",
    "M = \\begin{pmatrix}\n",
    "\\frac{3}{4} & \\frac{1}{4} \\\\\n",
    "\\frac{1}{4} & \\frac{3}{4} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Note that $M^T$ is a stochastic matrix because each column of the transposed matrix would sum to $1$ (in general the rows of the transposed matrix are unlikely to sum to 1, but choosing an example like the above makes it easy to compute the eigenvectors).\n",
    "\n",
    "It's not too hard to verify that the characteristic polynomial of $M^T$ can be factored into $(\\lambda - 1)(\\lambda - \\frac{1}{2})$, so we have two eigenvalues, $1$ and $\\frac{1}{2}$.\n",
    "\n",
    "It's not too hard to verify that column vector $[1, 1]$ is the eigenvector of $M^T$ with eigenvalue $1$\n",
    ".\n",
    "\n",
    "Now that we know the weights placed on each strategy, we can compute the strategy distribution by normalising our eigenvector.\n",
    "\n",
    "The ergodic distribution i $\\omega^* = [\\frac{1}{2}, \\frac{1}{2}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac241d-d8c9-40f6-ad80-0ddb81b00e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[[3/4, 1/4],\n",
    "               [1/4, 3/4]],\n",
    "             ])\n",
    "models = {\"transition_matrix\": M}\n",
    "result = find_ergodic_distribution(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0de01-3983-48c1-956f-3c686c4d27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcore.test.test_eq(result['ergodic'],\n",
    "                      np.array([[1/2, 1/2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a57ac-f485-46a0-b736-2a08cdf18a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# # Here is some code which illustrates how one could use sympy to find the relevant eigenvectors \n",
    "# # using symbolic methods (but please note that even sympy must resort to numerical methods if\n",
    "# # the matrices are bigger than 5 by 5 in size, due to the fundamental lack of exact solutions to \n",
    "# # polynomial equations with order greater than 5)\n",
    "# import sympy\n",
    "# for m in M:\n",
    "#     # Sympy needs integers or expressions to work\n",
    "#     # Integers is usually safer\n",
    "#     m = np.array(1000 * m, dtype=int)\n",
    "#     M_symbolic = sympy.Matrix(m)\n",
    "#     for result in M_symbolic.eigenvects():\n",
    "#         lamda, multiplicity, evs = result\n",
    "        \n",
    "#         # print(\"lambda: \" , lamda,\n",
    "#         #           \"multiplicity: \", multiplicity,\n",
    "#         #           \"eigenvectors: \", evs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94604c8-2f2b-46d5-8234-334635abd500",
   "metadata": {},
   "source": [
    "Here is another quick illustrative example.\n",
    "\n",
    "Let our transition matrix, $M$ be\n",
    "\n",
    "\\begin{equation}\n",
    "M = \\begin{pmatrix}\n",
    "\\frac{3}{4} & \\frac{1}{4} \\\\\n",
    "\\frac{3}{4} & \\frac{1}{4} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "$M^T$ is a stochastic matrix. It is easy to verify that $[\\frac{3}{4}, \\frac{1}{4}]$ is the normalised eigenvector with eigenvalue 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae7587-ea91-4a61-a316-35563ea0222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[[3/4, 1/4],\n",
    "               [3/4, 1/4]],\n",
    "             ])\n",
    "models = {\"transition_matrix\": M}\n",
    "result = find_ergodic_distribution(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8543490-6385-4b3b-9801-9229a54cb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcore.test.test_eq(result['ergodic'],\n",
    "                      np.array([[3/4, 1/4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27968b3f-8b47-4298-b6e6-1020c894e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L159){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### find_ergodic_distribution\n",
       "\n",
       ">      find_ergodic_distribution (models:dict)\n",
       "\n",
       "Find the ergodic distribution of a markov chain with the\n",
       "given transition matrix.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| models | dict | A dictionary that contains the parameters in `ModelTypeEGT` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L159){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### find_ergodic_distribution\n",
       "\n",
       ">      find_ergodic_distribution (models:dict)\n",
       "\n",
       "Find the ergodic distribution of a markov chain with the\n",
       "given transition matrix.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| models | dict | A dictionary that contains the parameters in `ModelTypeEGT` |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(find_ergodic_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870ac66-b641-434b-bb9a-27df3577eecf",
   "metadata": {},
   "source": [
    "### Run full markov chain algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd9f0e-f1e5-459a-b7a9-829c11bfc7e9",
   "metadata": {},
   "source": [
    "Finally, here is a helper function to both build the transition matrix for the model and find its ergodic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4a9b3-6248-4f70-9694-820307de7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def markov_chain(models:dict # A dictionary that contains the parameters in `ModelTypeEGT`\n",
    "                ):\n",
    "    \"\"\"Find the ergodic distribution of the evolutionary\n",
    "    game given by each model in models.\"\"\"\n",
    "    return thread_macro(models,\n",
    "                        build_transition_matrix,\n",
    "                        find_ergodic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0fb6f2-cb09-4a98-ac1c-f91aa1b292f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L178){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### markov_chain\n",
       "\n",
       ">      markov_chain (models:dict)\n",
       "\n",
       "Find the ergodic distribution of the evolutionary\n",
       "game given by each model in models.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| models | dict | A dictionary that contains the parameters in `ModelTypeEGT` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L178){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### markov_chain\n",
       "\n",
       ">      markov_chain (models:dict)\n",
       "\n",
       "Find the ergodic distribution of the evolutionary\n",
       "game given by each model in models.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| models | dict | A dictionary that contains the parameters in `ModelTypeEGT` |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(markov_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab3139-41ee-40d3-9a22-3506072288ae",
   "metadata": {},
   "source": [
    "## Multiple Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54550640-f67d-465d-a380-a94671f12b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#| hide\n",
    "@method(build_transition_matrix, 'multiple-populations')\n",
    "def build_transition_matrix(models:dict # A dictionary that contains the parameters in `ModelTypeEGT`\n",
    "                           ):\n",
    "    \"\"\"Build a transition matrix between all monomorphic states\n",
    "    when there are multiple populations.    \n",
    "    \"\"\"\n",
    "    Z, S, β = [models[k] for k in ['Z', 'recurrent_state_space', 'β']]\n",
    "    valid_transitions = models['valid_transitions']\n",
    "    π = models['payoffs']\n",
    "    M = np.zeros((payoffs.shape[0], len(S), len(S)))\n",
    "    for row_ind in range(M.shape[-1]):\n",
    "        M[:, row_ind, row_ind] += 1\n",
    "    for transition in valid_transitions.values():\n",
    "        strategy_profile_indices = transition['strategy_profile_indices']\n",
    "        player_index = transition['player_index']\n",
    "        row_ind = transition['row_ind']\n",
    "        col_ind = transition['col_ind']\n",
    "        πAA = π[:, strategy_profile_indices['AA'], player_index]\n",
    "        πAB = π[:, strategy_profile_indices['AB'], player_index]\n",
    "        πBA = π[:, strategy_profile_indices['BA'], player_index]\n",
    "        πBB = π[:, strategy_profile_indices['BB'], player_index]\n",
    "        ΠA = [πAA*(Z-k-1)/(Z-1) + πAB*k/(Z-1)\n",
    "              for k in range(1, Z)]\n",
    "        ΠB = [πBA*(Z-k)/(Z-1)  + πBB*(k-1)/(Z-1)\n",
    "              for k in range(1, Z)]\n",
    "        # We use a numerically stable method to find the fixation rate, ρ.\n",
    "        # ρ is the probability that mutant B successfully invades A\n",
    "        ρ = fixation_rate_stable(ΠA, ΠB, β)\n",
    "        # We have to divide this rate by the number of possible mutations\n",
    "        n_mutations = 0\n",
    "        for vt in valid_transitions.values():\n",
    "            if vt['row_ind'] == row_ind:\n",
    "                n_mutations += 1\n",
    "        M[:, row_ind, col_ind] = ρ / n_mutations\n",
    "        M[:, row_ind, row_ind] -= ρ / n_mutations\n",
    "    return {**models, 'transition_matrix':M}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f54813-0349-47f6-8c27-0ecb1a1f0b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L188){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### build_transition_matrix\n",
       "\n",
       ">      build_transition_matrix (models:dict)\n",
       "\n",
       "Build a transition matrix between all monomorphic states\n",
       "when there are multiple populations.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| models | dict | A dictionary that contains the parameters in `ModelTypeEGT` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/methods.py#L188){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### build_transition_matrix\n",
       "\n",
       ">      build_transition_matrix (models:dict)\n",
       "\n",
       "Build a transition matrix between all monomorphic states\n",
       "when there are multiple populations.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| models | dict | A dictionary that contains the parameters in `ModelTypeEGT` |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(build_transition_matrix.__multi__['multiple-populations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726f055-32f6-44c2-bd72-f86671e6f86e",
   "metadata": {},
   "source": [
    "Here is an example of how to build a transition matrix when we have 2 populations.\n",
    "\n",
    "In the limit of small mutation rates, the system spends almost all its time in states where each population plays one strategy. Moreover, only a mutant for one population has the opportunity to fixate in that population. This means we only need to consider transitions where the strategy played by one population has changed. Transitions where both populations would have to change strategy occur with probability 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c478f-62eb-404e-ad11-170f14f69a23",
   "metadata": {},
   "source": [
    "As we are working with multiple populations, our `models` variable needs to declare this with the `dispatch-type` key.\n",
    "\n",
    "This time, the `payoffs` key must have payoffs for each population. We also need a set of `strategy_contests`, not only a `strategy_set`. `strategy_contests` tells us which strategy profiles are being compared for the relevant transition, which population is affected, and where to find the relevant payoffs for the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c466975-079c-4db1-ae7f-2a7a77788be2",
   "metadata": {},
   "source": [
    "`payoffs` is a 3D Array containing payoffs for each model, each strategy profile, and each player.\n",
    "\n",
    "**Note:** the above representation can also capture games where multiple players from the same population interact with players from another population (e.g. 2 companies and 1 regulator play an R&D game). This is achieved by noticing that players from the same population receive the same payoffs in a game as their counterparts do if they were to switch strategies (if we had a model with subpopulations, a similar logic applies but this time to each subpopulation rather than the population).\n",
    "\n",
    "**TODO:** the above representation needs to capture games where it is uncertain from which populations players will be sampled from. For example, in some climate change negotiation games, there are rich and poor subpopulations but due to random selection, games may feature all rich, all poor, or some mix of players. An alternative representation which may scale better is to give different symbols to each strategy depending on the subpopulation. In this way, a strategy implicitly informs us from which population the player is from. However, with such a representation, it is essential for `valid_transitions` to describe which strategies must be compared.\n",
    "\n",
    "**Warning:** In games with many players (e.g. > 100), the space of possible strategy profiles can be too large to justify storing payoffs in an array. Instead, payoffs will be a function that is called when needed.\n",
    "\n",
    "`valid-transitions` is a dictionary of transitions to information about the relevant players in the contest and the indices of the relevant payoffs for computing the likelihood of the transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525de3b-ff8a-4e73-bdda-dbd4da36d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 10\n",
    "β = 1\n",
    "payoffs = np.array([[[2, 0, 2],\n",
    "                     [3, 1, 3],\n",
    "                     [3, 1, 3],\n",
    "                     [4, 2, 4],\n",
    "                     [2, 0, 2],\n",
    "                     [3, 1, 3],\n",
    "                     [3, 1, 3],\n",
    "                     [4, 2, 4]],\n",
    "                   ])\n",
    "recurrent_state_space = [\"AX\", \"AY\", \"BX\", \"BY\"]\n",
    "valid_transitions = {\"AX->AY\": {\"row_ind\": 0,\n",
    "                                \"col_ind\": 1,\n",
    "                                \"player_index\": 0,  # We need to know which player's payoff matters\n",
    "                                # We need to know where to look for the payoffs\n",
    "                                # of the 4 strategy profiles relevant to deriving\n",
    "                                # the fixation rate for the given transition.\n",
    "                                \"strategy_profile_indices\": {\"AA\": 0, \"AB\": 1,\n",
    "                                                             \"BA\": 2, \"BB\": 3}},\n",
    "                     \"AY->AX\": {\"row_ind\": 1,\n",
    "                                \"col_ind\": 0,\n",
    "                                \"player_index\": 0,\n",
    "                                \"strategy_profile_indices\": {\"AA\": 3, \"AB\": 2,\n",
    "                                                             \"BA\": 1, \"BB\": 0}},\n",
    "                     \"BX->BY\": {\"row_ind\": 2,\n",
    "                                \"col_ind\": 3,\n",
    "                                \"player_index\": 0,\n",
    "                                \"strategy_profile_indices\": {\"AA\": 4, \"AB\": 5,\n",
    "                                                             \"BA\": 6, \"BB\": 7}},\n",
    "                     \"BY->BX\": {\"row_ind\": 3,\n",
    "                                \"col_ind\": 2,\n",
    "                                \"player_index\": 0,\n",
    "                                \"strategy_profile_indices\": {\"AA\": 7, \"AB\": 6,\n",
    "                                                             \"BA\": 5, \"BB\": 4}},\n",
    "                     \n",
    "                     # Regulators do not face each other, so their payoffs do\n",
    "                     # not depend on the number of mutants they see.\n",
    "                     # There are only two strategy profiles to consider, so for\n",
    "                     # compatibility with `build_transition_matrix`, we repeat\n",
    "                     # indices where necessary.\n",
    "                     \n",
    "                     \"AX->BX\": {\"row_ind\": 0,\n",
    "                                \"col_ind\": 2,\n",
    "                                \"player_index\": 2,\n",
    "                                \"strategy_profile_indices\": {\"AA\": 0, \"AB\": 0,\n",
    "                                                             \"BA\": 4, \"BB\": 4,\n",
    "                                                            }},\n",
    "                     \"AY->BY\": {\"row_ind\": 1,\n",
    "                                \"col_ind\": 3,\n",
    "                                \"player_index\": 2,\n",
    "                                \"strategy_profile_indices\": {\"AA\": 3, \"AB\": 3,\n",
    "                                                             \"BA\": 7, \"BB\": 7,\n",
    "                                                            }},\n",
    "                     \"BX->AX\": {\"row_ind\": 2,\n",
    "                                \"col_ind\": 0,\n",
    "                                \"player_index\": 2,\n",
    "                                \"strategy_profile_indices\": {\"AA\": 4, \"AB\": 4,\n",
    "                                                             \"BA\": 0, \"BB\": 0,\n",
    "                                                            }},\n",
    "                     \"BY->AY\": {\"row_ind\": 3,\n",
    "                                \"col_ind\": 1,\n",
    "                                \"player_index\": 2,\n",
    "                                \"strategy_profile_indices\": {\"AA\": 7, \"AB\": 7,\n",
    "                                                             \"BA\": 3, \"BB\": 3,\n",
    "                                                            }},\n",
    "                    }\n",
    "\n",
    "# What if Regulators could transition as a mutant company fixates (independent of the company's chance of fixating)\n",
    "# How do two populations emerge in the first place from one population? \n",
    "# Market games. Number of players of each strategy determines the market price which informs a strategy's average payoff\n",
    "# instead of averaging over the number of players of each strategy.\n",
    "\n",
    "models = {\"dispatch-type\": \"multiple-populations\",\n",
    "          \"β\": β,\n",
    "          \"Z\": Z,\n",
    "          \"recurrent_state_space\": recurrent_state_space,\n",
    "          \"valid_transitions\": valid_transitions,\n",
    "          \"payoffs\": payoffs,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c08fa6-4519-49cb-80b6-7c1630b15cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dispatch-type': 'multiple-populations',\n",
       " 'β': 1,\n",
       " 'Z': 10,\n",
       " 'recurrent_state_space': ['AX', 'AY', 'BX', 'BY'],\n",
       " 'valid_transitions': {'AX->AY': {'row_ind': 0,\n",
       "   'col_ind': 1,\n",
       "   'player_index': 0,\n",
       "   'strategy_profile_indices': {'AA': 0, 'AB': 1, 'BA': 2, 'BB': 3}},\n",
       "  'AY->AX': {'row_ind': 1,\n",
       "   'col_ind': 0,\n",
       "   'player_index': 0,\n",
       "   'strategy_profile_indices': {'AA': 3, 'AB': 2, 'BA': 1, 'BB': 0}},\n",
       "  'BX->BY': {'row_ind': 2,\n",
       "   'col_ind': 3,\n",
       "   'player_index': 0,\n",
       "   'strategy_profile_indices': {'AA': 4, 'AB': 5, 'BA': 6, 'BB': 7}},\n",
       "  'BY->BX': {'row_ind': 3,\n",
       "   'col_ind': 2,\n",
       "   'player_index': 0,\n",
       "   'strategy_profile_indices': {'AA': 7, 'AB': 6, 'BA': 5, 'BB': 4}},\n",
       "  'AX->BX': {'row_ind': 0,\n",
       "   'col_ind': 2,\n",
       "   'player_index': 2,\n",
       "   'strategy_profile_indices': {'AA': 0, 'AB': 0, 'BA': 4, 'BB': 4}},\n",
       "  'AY->BY': {'row_ind': 1,\n",
       "   'col_ind': 3,\n",
       "   'player_index': 2,\n",
       "   'strategy_profile_indices': {'AA': 3, 'AB': 3, 'BA': 7, 'BB': 7}},\n",
       "  'BX->AX': {'row_ind': 2,\n",
       "   'col_ind': 0,\n",
       "   'player_index': 2,\n",
       "   'strategy_profile_indices': {'AA': 4, 'AB': 4, 'BA': 0, 'BB': 0}},\n",
       "  'BY->AY': {'row_ind': 3,\n",
       "   'col_ind': 1,\n",
       "   'player_index': 2,\n",
       "   'strategy_profile_indices': {'AA': 7, 'AB': 7, 'BA': 3, 'BB': 3}}},\n",
       " 'payoffs': array([[[2, 0, 2],\n",
       "         [3, 1, 3],\n",
       "         [3, 1, 3],\n",
       "         [4, 2, 4],\n",
       "         [2, 0, 2],\n",
       "         [3, 1, 3],\n",
       "         [3, 1, 3],\n",
       "         [4, 2, 4]]]),\n",
       " 'transition_matrix': array([[[0.65551553, 0.29448447, 0.05      , 0.        ],\n",
       "         [0.00009879, 0.94990121, 0.        , 0.05      ],\n",
       "         [0.05      , 0.        , 0.65551553, 0.29448447],\n",
       "         [0.        , 0.05      , 0.00009879, 0.94990121]]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_transition_matrix(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b58acf-5902-4d52-98d5-85b75a502868",
   "metadata": {},
   "source": [
    "8 recurrent states since we have 3 sectors who each can choose one of 2 strategies.\n",
    "This means we have 8 * 3cr1 = 24 possible transitions.\n",
    "\n",
    "More generally, the number of valid transitions = n_recurrent_states * SUM_j(n_strategies_j - 1)\n",
    "where n_recurrent_states = PROD_j(n_strategies_j) for each population j.\n",
    "\n",
    "For 4 populations and 3 strategies each, this is 648 valid transitions.\n",
    "This is clearly untenable! Granted, an 81 by 81 transition matrix may be\n",
    "a little bit unwieldly, but its eigenvectors are certainly computable.\n",
    "\n",
    "On the other hand, it is certainly plausible to calculate the fixation rates for\n",
    "the comparison of each state. If we do this programatically, perhaps there is\n",
    "some filter we can program which marks some of the transitions as 0.\n",
    "Consistency is key. If we index each state by i, we must be able to index where\n",
    "each transition is in the transition matrix. If we order the states by the numbers\n",
    "of each type, we might have a way to create our filter too.\n",
    "\n",
    "One complication is that once we know the two states to compare, we must allow\n",
    "a general function for computing the average payoffs (and have a way of resolving\n",
    "how that function uses the payoff matrix and given states to compute the average payoffs)\n",
    "\n",
    "We should use a binary index when each sector only has two strategies. We can\n",
    "generalise this to another base depending on the number of strategies.\n",
    "We can then convert this state index to base 10 for placement in our\n",
    "transition matrix. A transition is easily represented by a pair of these numbers in\n",
    "whichever base we prefer.\n",
    "\n",
    "As a filter we can notice that if we take the difference of any two state indices\n",
    "we should find that only one of the numbers is non-zero. This would be our filter\n",
    "for a model with multiple sectors.\n",
    "\n",
    "What remains is for us to have useful information at hand for computing the\n",
    "payoff matrices. Create a profile index and filter. Allow player order to matter if desired.\n",
    "To allow such general games with sampling between the populations and variable player order\n",
    "we have to specify additional profile filter rules to go along with our game type.\n",
    "\n",
    "Our average payoff function can then calculate the profile in the way relevant to the filter rule.\n",
    "An excellent way of representing the payoffs would be to have a nested map.\n",
    "Level one indexes the players of the game. Level two indexes the profile using\n",
    "our profile index. Level three indexes the models. Level two is the most important,\n",
    "since this is where we can apply our profile filter to retain only the payoffs relevant\n",
    "to computing the average payoff (we could switch level one and two if that is easier).\n",
    "\n",
    "There is a reason why I haven't suggested using a 3D array in this case. \n",
    "Very simple payoff models where all or the majority of sectors are fixed players\n",
    "have many invalid player orderings and compositions. We wouldn't want to force\n",
    "the user to create a larger array than they need.\n",
    "\n",
    "I also suspect that the nested map is far more legible. Note that our algorithm\n",
    "only uses 1D arrays from our payoff matrices anyways.\n",
    "\n",
    "For further legibility, it makes sense to generate a map from the state index\n",
    "to their human readable labels (e.g. 111 -> CCC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f73d3-8e65-4b33-9366-b55a6bc788fc",
   "metadata": {},
   "source": [
    "Consider DataArray, xarray, and Pandas instead of numpy structured arrays or dicts: https://numpy.org/doc/stable/user/basics.rec.html#structured-datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671d169-c677-4a13-a9e9-a67a5b12e325",
   "metadata": {},
   "source": [
    "#### Examples and tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6535bc-97f4-4a91-8aad-73d64059b5f8",
   "metadata": {},
   "source": [
    "**Example 1**\n",
    "\n",
    "This example comes from a paper by Encarnacao et al. 2016.\n",
    "\n",
    "They have a 3 sector model and report fixation probabilities for a particular scenario. Can we replicate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0882f0-de3a-4e57-8e91-da0fa7130f0a",
   "metadata": {},
   "source": [
    "**Step 1:** define the payoffs of the game for each strategy profile. While the payoffs are fairly simple to represent as a 2D array for each model, we use a data structure which has been specifically chosen for comptability with more nuanced models (below this is a nested python dict)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972ddb2-c953-4068-a7f2-0bdcc12469ef",
   "metadata": {},
   "source": [
    "TODO: at least use python default_dicts instead of core dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39ca01-2963-44ee-8089-72bb82005405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def payoffs_encanacao_2016(models):\n",
    "    names = ['b_r', 'b_s', 'c_s', 'c_t', 'σ']\n",
    "    b_r, b_s, c_s, c_t, σ = [models[k] for k in names]\n",
    "    payoffs = {}\n",
    "    n_players = 3\n",
    "    n_sectors = 3\n",
    "    n_strategies_per_sector = [2, 2, 2]\n",
    "    n_strategies_total = 6\n",
    "    index_min = \"0-0-0\" # All players are from the first sector, playing that sector's first strategy\n",
    "    index_max = \"5-5-5\" # All players are from the third sector, playing that sector's second strategy\n",
    "    # Note: The seperator makes it easy to represent games where n_strategies_total >= 10.\n",
    "    \n",
    "    # It is also trivial to define a vector which maps these indexes to strategy profiles\n",
    "    # As sector order is fixed we could neglect to mention suscripts for each sector\n",
    "    strategy_names = [\"D\", \"C\", \"D\", \"C\", \"D\", \"C\"]\n",
    "    \n",
    "    zero = np.zeros(b_r.shape[0])\n",
    "    # As in the main text\n",
    "    payoffs[\"C-C-C\"] = {\"P3\": b_r-2*c_s,\n",
    "                        \"P2\": σ+b_s-c_t,\n",
    "                        \"P1\": σ+b_s}\n",
    "    payoffs[\"C-C-D\"] = {\"P3\": -c_s,\n",
    "                        \"P2\": b_s-c_t,\n",
    "                        \"P1\": zero}\n",
    "    payoffs[\"C-D-C\"] = {\"P3\": b_r-c_s,\n",
    "                        \"P2\": zero,\n",
    "                        \"P1\": b_s}\n",
    "    payoffs[\"C-D-D\"] = {\"P3\": zero,\n",
    "                        \"P2\": σ,\n",
    "                        \"P1\": σ}\n",
    "    payoffs[\"D-C-C\"] = {\"P3\": zero,\n",
    "                        \"P2\": σ-c_t,\n",
    "                        \"P1\": σ}\n",
    "    payoffs[\"D-C-D\"] = {\"P3\": zero,\n",
    "                        \"P2\": -c_t,\n",
    "                        \"P1\": zero}\n",
    "    payoffs[\"D-D-C\"] = {\"P3\": zero,\n",
    "                        \"P2\": zero,\n",
    "                        \"P1\": zero}\n",
    "    payoffs[\"D-D-D\"] = {\"P3\": zero,\n",
    "                        \"P2\": σ,\n",
    "                        \"P1\": σ}\n",
    "    \n",
    "    # The following indexes capture all strategy profiles where each player is fixed to a unique sector\n",
    "    # (and player order does not matter, so we need only consider one ordering of sectors).\n",
    "    payoffs[\"4-2-0\"] = payoffs[\"D-D-D\"]\n",
    "    payoffs[\"4-2-1\"] = payoffs[\"D-D-C\"]\n",
    "    payoffs[\"4-3-0\"] = payoffs[\"D-C-D\"]\n",
    "    payoffs[\"4-3-1\"] = payoffs[\"D-C-C\"]\n",
    "    payoffs[\"5-2-0\"] = payoffs[\"C-D-D\"]\n",
    "    payoffs[\"5-2-1\"] = payoffs[\"C-D-C\"]\n",
    "    payoffs[\"5-3-0\"] = payoffs[\"C-C-D\"]\n",
    "    payoffs[\"5-3-1\"] = payoffs[\"C-C-C\"]\n",
    "    return {**models, \"payoffs\": payoffs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d9f75-807e-4471-9824-06566efe2f62",
   "metadata": {},
   "source": [
    "Next, we need to provide a filter rule so that we know which payoffs are relevant to our average payoff (or success) computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376633e-bf07-40a3-9c9d-b6f2c7b8cdc6",
   "metadata": {},
   "source": [
    "Given the number of sectors, and number of players, and a rule which in this case is the allowed allowed sectors per player.\n",
    "\n",
    "We can specify this as a dict from player to a vector or set of sectors. \\\n",
    "We can use the number of sectors to mention valid strategies for each player. \\\n",
    "Our sectors are indexed from 0 to n_sectors - 1 (from right to left)\\\n",
    "Our players are indexed from 0 to n_players - 1 (from right to left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c140dbc-8df2-48de-960c-86f4ef1b4592",
   "metadata": {},
   "source": [
    "We should have a method for creating all possible player profiles. Once we apply\n",
    "our filters, we will know exactly which profiles we need to write payoffs for.\n",
    "We could also use this method to validate that we have passed sufficient information\n",
    "to payoffs, and warn us about what is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c5cc9-28b2-4527-ad05-8a50499b6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_all_profiles(models):\n",
    "    \"\"\"Create all strategy profiles for the set of models.\"\"\"\n",
    "    n_players, n_strategies = [models[k] for k in ['n_players', 'n_strategies']]\n",
    "    n_strategies_total = np.sum(n_strategies)\n",
    "    n_profiles = n_strategies_total ** n_players\n",
    "    strategy_axis = np.arange(n_strategies_total)[:, None]\n",
    "    grid = build_grid_from_axes([strategy_axis for _ in range(n_players)])\n",
    "    profiles = []\n",
    "    for row in grid:\n",
    "        profile = \"-\".join(map(str, row))\n",
    "        profiles.append(profile)\n",
    "    fastcore.test.test_eq(len(profiles), n_profiles)\n",
    "    return {**models, \"profiles\": profiles}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f8393-7c24-4c27-a7a3-c3f977695fb1",
   "metadata": {},
   "source": [
    "**Tests for `create_all_profiles`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046cdc72-4529-4f46-be80-f516f5f075ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "result = create_all_profiles({\"n_players\": 2, \"n_strategies\": 2})\n",
    "fastcore.test.test_eq(result['profiles'], [\"0-0\", \"0-1\", \"1-0\", \"1-1\"])\n",
    "\n",
    "result = create_all_profiles({\"n_players\": 2, \"n_strategies\": [2, 2]})\n",
    "fastcore.test.test_eq(result['profiles'],\n",
    "                       [\"0-0\", \"0-1\", \"0-2\", \"0-3\",\n",
    "                        \"1-0\", \"1-1\", \"1-2\", \"1-3\",\n",
    "                        \"2-0\", \"2-1\", \"2-2\", \"2-3\",\n",
    "                        \"3-0\", \"3-1\", \"3-2\", \"3-3\",])\n",
    "\n",
    "fastcore.test.test_eq(create_all_profiles({\"n_players\": 2,\n",
    "                                           \"n_strategies\": [2, 2]})['profiles'],\n",
    "                     create_all_profiles({\"n_players\": 2,\n",
    "                                          \"n_strategies\": 4})['profiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1516c7-731c-4e1b-ba0d-64d50c571327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@multi\n",
    "def profile_filter(models):\n",
    "    \"Filter strategy profiles to those which satisfy the given rule.\"\n",
    "    return models.get('profile_filter_rule')\n",
    "\n",
    "@method(profile_filter, 'allowed_sectors')\n",
    "def profile_filter(models):\n",
    "    \"\"\"Filter strategy profiles to only those where players are from their\n",
    "    allowed sectors.\"\"\"\n",
    "    profiles = models.get('profiles_filtered',\n",
    "                          models.get('profiles',\n",
    "                                     create_all_profiles(models)['profiles']))\n",
    "    allowed_sectors = models['allowed_sectors']\n",
    "    sector_strategies = models['sector_strategies']\n",
    "    profiles_filtered = []\n",
    "    for k in profiles:\n",
    "        k_tuple = list(map(int, k.split(\"-\")))\n",
    "        valid = True\n",
    "        for i, ind in enumerate(k_tuple[::-1]):\n",
    "            allowed_inds = np.hstack([sector_strategies[j]\n",
    "                                      for j in allowed_sectors[f\"P{i+1}\"]])\n",
    "            if ind not in allowed_inds:\n",
    "                valid = False\n",
    "        if valid==True:\n",
    "            profiles_filtered.append(k)\n",
    "    return {**models, \"profiles_filtered\": profiles_filtered}\n",
    "\n",
    "@method(profile_filter)\n",
    "def profile_filter(models):\n",
    "    \"\"\"The default filter method leaves models unchanged.\"\"\"\n",
    "    print(\"\"\"`profile_filter` called but `models` did not specify a\n",
    "           `profile_filter_rule`. Try specifying one.\"\"\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeec315-c6ff-433a-8f02-f4e36e923bf2",
   "metadata": {},
   "source": [
    "We also need a filter which yields the relevant profiles for each transition considered.\n",
    "\n",
    "Given two states and their transition we first check that it is valid, and if we so we know\n",
    "the sector affected. Only that sector may choose different strategies, so the others are fixed.\n",
    "\n",
    "Note that the recurrent states of the game describes each strategy employed by each sector. This is written in the same form as we write the strategy profile, \"{strategy_code}-{strategy_code}-{strategy_code}\" but this time each strategy code refers to a strategy for each sector (right to left), and they are indexed from 0 again to mark a clear difference from the codes for the strategy profile. This also makes it straight forward to build up the the set of allowed strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c67cf-4703-4d33-9b3f-265ab7b85fe5",
   "metadata": {},
   "source": [
    "**Note:** Below, my filter only keeps strategies which are relevant to the two recurrent states relevant to the transition. If we need additional strategies, this filter will not be sufficient (this might be the case if we allow a social learning rule which explores more than one mutant strategy at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e833c29-95dd-432f-851f-f14bb11baf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@method(profile_filter, 'relevant_to_transition')\n",
    "def profile_filter(models):\n",
    "    \"\"\"Filter for strategy profiles relevant to the given transition.\"\"\"\n",
    "    ind1, ind2 = models['transition_indices']\n",
    "    sector_strategies = models['sector_strategies']\n",
    "    profiles = models.get('profiles_filtered',\n",
    "                          models.get('profiles',\n",
    "                                     create_all_profiles(models)['profiles']))\n",
    "    ind1_tuple = list(map(int, ind1.split(\"-\")))\n",
    "    ind2_tuple = list(map(int, ind2.split(\"-\")))\n",
    "    differ = [i1!=i2 for i1, i2 in zip(ind1_tuple, ind2_tuple)]\n",
    "    valid = sum(differ) == 1\n",
    "    if valid:\n",
    "        affected_sector = f\"S{np.argmax(differ[::-1]) + 1}\"\n",
    "        flexible_strategies = sector_strategies[affected_sector]\n",
    "        strategies1 = [sector_strategies[f\"S{i+1}\"][ind]\n",
    "                             for i, ind in enumerate(ind1_tuple[::-1])]\n",
    "        strategies2 = [sector_strategies[f\"S{i+1}\"][ind]\n",
    "                       for i, ind in enumerate(ind2_tuple[::-1])]\n",
    "        strategies_valid = np.unique(np.hstack([strategies1, strategies2]))\n",
    "        profiles_filtered = []\n",
    "        for profile in profiles:\n",
    "            relevant = True\n",
    "            for strategy in list(map(int, profile.split(\"-\"))):\n",
    "                if strategy not in strategies_valid:\n",
    "                    relevant = False\n",
    "            if relevant == True:\n",
    "                profiles_filtered.append(profile)\n",
    "        relevant = True\n",
    "        return {**models, \"profiles_filtered\": profiles_filtered}\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08572f94-c343-4f8c-9368-8a987c96505e",
   "metadata": {},
   "source": [
    "**Tests for `profile_filter`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acabca16-0784-44de-9bd8-f360c886bcf3",
   "metadata": {},
   "source": [
    "Let's test the `\"allowed_sectors\"` filter rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9783e6-f5cd-4e00-9172-c094438cc0bc",
   "metadata": {},
   "source": [
    "First I test a game with 3 players but only two sectors.\\\n",
    "Each player is fixed to a specific sector, but two players belong to the same sector.\\\n",
    "In this case the rule should filter to only those profiles where players use\\\n",
    "the strategies available to the sectors they can play as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb6373-0bce-49ce-9a46-91c38d184ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "allowed_sectors = {\"P3\": [\"S2\"],\n",
    "                   \"P2\": [\"S2\"],\n",
    "                   \"P1\": [\"S1\"]}\n",
    "sector_strategies = {\"S2\": [2, 3],\n",
    "                     \"S1\": [0, 1]}\n",
    "n_players = 3\n",
    "n_strategies = [2, 2] # this could be derived from sector_strategies or the other way round.\n",
    "models = {\"profile_filter_rule\": \"allowed_sectors\",\n",
    "          \"n_players\": n_players,\n",
    "          \"n_strategies\": n_strategies,\n",
    "          \"allowed_sectors\": allowed_sectors,\n",
    "          \"sector_strategies\": sector_strategies}\n",
    "result = profile_filter(models)['profiles_filtered']\n",
    "# Only strategy 2 is irrelevant\n",
    "expected = [\"2-2-0\", \"2-2-1\", \n",
    "            \"2-3-0\", \"2-3-1\",\n",
    "            \"3-2-0\", \"3-2-1\",\n",
    "            \"3-3-0\", \"3-3-1\",]\n",
    "fastcore.test.test_eq(result, expected)\n",
    "fastcore.test.test_eq(len(result), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f62cd-df5d-4261-b082-ad24d5569893",
   "metadata": {},
   "source": [
    "I then test a 3 player game with 3 sectors. Each player is fixed to a particular sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f824a8-dcf9-4673-91cb-f40a4d9a0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "allowed_sectors = {\"P3\": [\"S3\"],\n",
    "                   \"P2\": [\"S2\"],\n",
    "                   \"P1\": [\"S1\"]}\n",
    "sector_strategies = {\"S3\": [4, 5],\n",
    "                     \"S2\": [2, 3],\n",
    "                     \"S1\": [0, 1]}\n",
    "models = {\"profile_filter_rule\": \"allowed_sectors\",\n",
    "          \"n_players\": 3,\n",
    "          \"n_strategies\": [2, 2, 2],\n",
    "          \"allowed_sectors\": allowed_sectors,\n",
    "          \"sector_strategies\": sector_strategies}\n",
    "result = profile_filter(models)['profiles_filtered']\n",
    "expected = [\"4-2-0\",\n",
    "            \"4-2-1\",\n",
    "            \"4-3-0\",\n",
    "            \"4-3-1\",\n",
    "            \"5-2-0\",\n",
    "            \"5-2-1\",\n",
    "            \"5-3-0\",\n",
    "            \"5-3-1\",\n",
    "           ]\n",
    "fastcore.test.test_eq(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff4f4b-54d5-4bb0-a409-95055ddd436a",
   "metadata": {},
   "source": [
    "Now, let's test the `\"relevant_to_transition\"` filter rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf65975-c159-48cd-8a8c-9732f4b14bb5",
   "metadata": {},
   "source": [
    "First I test a game with 3 players but only two populations.\\\n",
    "In this case the rule should filter to only the relevant profiles where strategy 2 is missing.\\\n",
    "Recall that strategy 2 is the first strategy available to a player from sector 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d413677-9636-4369-9cb5-7208d2da7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "sector_strategies = {\"S2\": [2, 3],\n",
    "                     \"S1\": [0, 1]}\n",
    "transition_indices = [\"1-0\", \"1-1\"]\n",
    "n_players = 3\n",
    "n_strategies = [2, 2] # this could be derived from sector_strategies or the other way round.\n",
    "models = {\"profile_filter_rule\": \"relevant_to_transition\",\n",
    "          \"n_players\": n_players,\n",
    "          \"n_strategies\": n_strategies,\n",
    "          \"sector_strategies\": sector_strategies,\n",
    "          \"transition_indices\": transition_indices}\n",
    "result = profile_filter(models)['profiles_filtered']\n",
    "# Only strategy 2 is irrelevant\n",
    "expected = [\"0-0-0\", \"0-0-1\", \"0-0-3\", \n",
    "            \"0-1-0\", \"0-1-1\", \"0-1-3\",\n",
    "            \"0-3-0\", \"0-3-1\", \"0-3-3\",\n",
    "            \"1-0-0\", \"1-0-1\", \"1-0-3\",\n",
    "            \"1-1-0\", \"1-1-1\", \"1-1-3\",\n",
    "            \"1-3-0\", \"1-3-1\", \"1-3-3\",\n",
    "            \"3-0-0\", \"3-0-1\", \"3-0-3\",\n",
    "            \"3-1-0\", \"3-1-1\", \"3-1-3\",\n",
    "            \"3-3-0\", \"3-3-1\", \"3-3-3\"]\n",
    "fastcore.test.test_eq(result, expected)\n",
    "expected = (np.sum(n_strategies) - 1) ** n_players  # 1 of the 4 strategies won't be relevant here\n",
    "fastcore.test.test_eq(len(result), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d836d54-4208-4009-9c9d-574b32a0a098",
   "metadata": {},
   "source": [
    "I then test a larger game with 3 players and 3 sectors. The list is long, so\n",
    "I only check that the number of profiles kept is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57aa66-9097-47cd-a57d-609da6d82b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "sector_strategies = {\"S3\": [4, 5],\n",
    "                     \"S2\": [2, 3],\n",
    "                     \"S1\": [0, 1]}\n",
    "transition_indices = [\"1-1-1\", \"1-1-0\"]\n",
    "n_players = 3\n",
    "n_strategies = [2, 2, 2]\n",
    "models = {\"profile_filter_rule\": \"relevant_to_transition\",\n",
    "          \"n_players\": n_players,\n",
    "          \"n_strategies\": n_strategies,\n",
    "          \"sector_strategies\": sector_strategies,\n",
    "          \"transition_indices\": transition_indices}\n",
    "result = profile_filter(models)['profiles_filtered']\n",
    "expected = (np.sum(n_strategies) - 2) ** n_players  # 2 of the 6 strategies won't be relevant here\n",
    "fastcore.test.test_eq(len(result), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c167bc-93f2-4f20-8a7b-0884ad01605f",
   "metadata": {},
   "source": [
    "I now test the `\"allowed_sectors\"` and \"`relevant_to_transition`\" rules when used together.\\\n",
    "The game is as before with 3 players and 3 sectors. Here, we can check that the list of profiles is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a3786-8b29-4f21-97bc-79c0f83282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "allowed_sectors = {\"P3\": [\"S3\"],\n",
    "                   \"P2\": [\"S2\"],\n",
    "                   \"P1\": [\"S1\"]}\n",
    "sector_strategies = {\"S3\": [4, 5],\n",
    "                     \"S2\": [2, 3],\n",
    "                     \"S1\": [0, 1]}\n",
    "transition_indices = [\"1-1-1\", \"1-1-0\"]\n",
    "n_players = 3\n",
    "n_strategies = [2, 2, 2]\n",
    "models = {\"profile_filter_rule\": \"relevant_to_transition\",\n",
    "          \"n_players\": n_players,\n",
    "          \"n_strategies\": n_strategies,\n",
    "          \"allowed_sectors\": allowed_sectors,\n",
    "          \"sector_strategies\": sector_strategies,\n",
    "          \"transition_indices\": transition_indices}\n",
    "result = thread_macro(models,\n",
    "                      profile_filter,\n",
    "                      (assoc, \"profile_filter_rule\", \"allowed_sectors\"),\n",
    "                      profile_filter)\n",
    "expected = [\"5-3-0\", \"5-3-1\"]\n",
    "fastcore.test.test_eq(len(result['profiles_filtered']), 2)\n",
    "fastcore.test.test_eq(result['profiles_filtered'], expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a40e764-66ff-44a8-bb65-e54914e2bb03",
   "metadata": {},
   "source": [
    "The order we apply these two filter rules should not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2dd37a-1b04-42f7-9cb2-f3c1fca4a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "allowed_sectors = {\"P3\": [\"S3\"],\n",
    "                   \"P2\": [\"S2\"],\n",
    "                   \"P1\": [\"S1\"]}\n",
    "sector_strategies = {\"S3\": [4, 5],\n",
    "                     \"S2\": [2, 3],\n",
    "                     \"S1\": [0, 1]}\n",
    "transition_indices = [\"1-1-1\", \"1-1-0\"]\n",
    "n_players = 3\n",
    "n_strategies = [2, 2, 2]\n",
    "models = {\"profile_filter_rule\": \"relevant_to_transition\",\n",
    "          \"n_players\": n_players,\n",
    "          \"n_strategies\": n_strategies,\n",
    "          \"allowed_sectors\": allowed_sectors,\n",
    "          \"sector_strategies\": sector_strategies,\n",
    "          \"transition_indices\": transition_indices}\n",
    "result1 = thread_macro(models,\n",
    "                       profile_filter,\n",
    "                       (assoc, \"profile_filter_rule\", \"allowed_sectors\"),\n",
    "                       profile_filter)\n",
    "result2 = thread_macro(models,\n",
    "                       (assoc, \"profile_filter_rule\", \"allowed_sectors\"),\n",
    "                       profile_filter,\n",
    "                       (assoc, \"profile_filter_rule\", \"relevant_to_transition\"),\n",
    "                       profile_filter)\n",
    "expected = [\"5-3-0\", \"5-3-1\"]\n",
    "fastcore.test.test_eq(result1['profiles_filtered'], expected)\n",
    "fastcore.test.test_eq(result2['profiles_filtered'], expected)\n",
    "fastcore.test.test_eq(result1['profiles_filtered'],\n",
    "                      result2['profiles_filtered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d347f2-50ca-4406-baae-6cdea6829e46",
   "metadata": {},
   "source": [
    "Let's also check the eariler game with 2 sectors and 3 players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c957f-c525-4464-a5e7-9b007956374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "allowed_sectors = {\"P3\": [\"S2\"],\n",
    "                   \"P2\": [\"S2\"],\n",
    "                   \"P1\": [\"S1\"]}\n",
    "sector_strategies = {\"S2\": [2, 3],\n",
    "                     \"S1\": [0, 1]}\n",
    "transition_indices = [\"1-0\", \"1-1\"]\n",
    "n_players = 3\n",
    "n_strategies = [2, 2] # this could be derived from sector_strategies or the other way round.\n",
    "models = {\"profile_filter_rule\": \"relevant_to_transition\",\n",
    "          \"n_players\": n_players,\n",
    "          \"n_strategies\": n_strategies,\n",
    "          \"allowed_sectors\": allowed_sectors,\n",
    "          \"sector_strategies\": sector_strategies,\n",
    "          \"transition_indices\": transition_indices}\n",
    "result1 = thread_macro(models,\n",
    "                       profile_filter,\n",
    "                       (assoc, \"profile_filter_rule\", \"allowed_sectors\"),\n",
    "                       profile_filter)\n",
    "result2 = thread_macro(models,\n",
    "                       (assoc, \"profile_filter_rule\", \"allowed_sectors\"),\n",
    "                       profile_filter,\n",
    "                       (assoc, \"profile_filter_rule\", \"relevant_to_transition\"),\n",
    "                       profile_filter)\n",
    "expected = [\"3-3-0\", \"3-3-1\"]\n",
    "fastcore.test.test_eq(result1['profiles_filtered'], expected)\n",
    "fastcore.test.test_eq(result2['profiles_filtered'], expected)\n",
    "fastcore.test.test_eq(result1['profiles_filtered'],\n",
    "                      result2['profiles_filtered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64f4dd-0bc0-486b-9486-d6069219a388",
   "metadata": {},
   "source": [
    "Notice that in this game, transitions which affect the sector with multiple players require us to look at more profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75b0ea-6202-4f98-a13c-fc0df11e3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "allowed_sectors = {\"P3\": [\"S2\"],\n",
    "                   \"P2\": [\"S2\"],\n",
    "                   \"P1\": [\"S1\"]}\n",
    "sector_strategies = {\"S2\": [2, 3],\n",
    "                     \"S1\": [0, 1]}\n",
    "transition_indices = [\"0-1\", \"1-1\"]\n",
    "n_players = 3\n",
    "n_strategies = [2, 2] # this could be derived from sector_strategies or the other way round.\n",
    "models = {\"profile_filter_rule\": \"relevant_to_transition\",\n",
    "          \"n_players\": n_players,\n",
    "          \"n_strategies\": n_strategies,\n",
    "          \"allowed_sectors\": allowed_sectors,\n",
    "          \"sector_strategies\": sector_strategies,\n",
    "          \"transition_indices\": transition_indices}\n",
    "result1 = thread_macro(models,\n",
    "                       profile_filter,\n",
    "                       (assoc, \"profile_filter_rule\", \"allowed_sectors\"),\n",
    "                       profile_filter)\n",
    "result2 = thread_macro(models,\n",
    "                       (assoc, \"profile_filter_rule\", \"allowed_sectors\"),\n",
    "                       profile_filter,\n",
    "                       (assoc, \"profile_filter_rule\", \"relevant_to_transition\"),\n",
    "                       profile_filter)\n",
    "expected = [\"2-2-1\", \"2-3-1\", \"3-2-1\", \"3-3-1\"]\n",
    "fastcore.test.test_eq(result1['profiles_filtered'], expected)\n",
    "fastcore.test.test_eq(result2['profiles_filtered'], expected)\n",
    "fastcore.test.test_eq(result1['profiles_filtered'],\n",
    "                      result2['profiles_filtered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796c239-4732-4bb4-be37-88f9ce003879",
   "metadata": {},
   "source": [
    "Here is a method for checking that a given transition is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1aba5b-8631-47ca-9b89-6827685ff5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def valid_transition(ind1:str, # The index of the current state, expressed in the form \"{strategy_code}-{strategy_code}-{strategy_code}\"\n",
    "                     ind2:str,  # The index of the next state, expressed in the same form as `ind1`\n",
    "                    ) -> bool: # True if the transition is valid, false otherwise\n",
    "    \"\"\"Check if the transition from ind1->ind2 is valid\n",
    "    i.e. that only one population undergoes a change in strategy.\"\"\"\n",
    "    ind1_tuple = list(map(int, ind1.split(\"-\")))\n",
    "    ind2_tuple = list(map(int, ind2.split(\"-\")))\n",
    "    differ = [i1!=i2 for i1, i2 in zip(ind1_tuple, ind2_tuple)]\n",
    "    valid = sum(differ) == 1\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2be49-d383-4aa8-82f4-c87268446d0a",
   "metadata": {},
   "source": [
    "**Tests for `valid_transition`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02347d26-0167-4784-8c93-c64e00d004a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "fastcore.test.test_eq(valid_transition(\"1-1-1\", \"2-1-1\"), True)\n",
    "fastcore.test.test_eq(valid_transition(\"1-1-1\", \"2-1-2\"), False)\n",
    "fastcore.test.test_eq(valid_transition(\"1-1-1\", \"0-0-0\"), False)\n",
    "fastcore.test.test_eq(valid_transition(\"1-1-1\", \"22-1-3\"), False)\n",
    "fastcore.test.test_eq(valid_transition(\"1-1-1\", \"1-1-1\"), False) # Even though possible, self transitions are marked as false since we never compute them directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c79050-23b5-4662-b0d5-2b099d1d0e04",
   "metadata": {},
   "source": [
    "We now have the methods we need for building the transition matrix for a game with an arbitrary number of sectors and various interactions between those sectors.\n",
    "\n",
    "Such a method takes these steps:\n",
    "1. For each possible transition\n",
    "  - Check if the transition is valid\n",
    "    - If self-transition, assign the value 1\n",
    "    - If not, skip\n",
    "  - Filter profiles down to only those which are relevant\n",
    "  - Compute average payoffs using the payoffs and those profiles\n",
    "  - Compute the fixation rate\n",
    "  - Compute transition probabilities as before\n",
    "\n",
    "**Note:** we do not yet support an arbitrary number of games with cross-learning between them. Those types of games are better suited when we explain tools from multilayer network science. In most simple cases, these games can be reframed as a single game and then our code applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62785d-9dd4-4e21-b871-5c749a8529ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get(m:dict, k:str):\n",
    "    \"Get attribute k from dictionary m.\"\n",
    "    return m.get(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10148a32-441a-49b8-bf2b-4a8ee58fa9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def apply_profile_filters(models):\n",
    "    \"Apply all profile filters listed in `profile_filters` in `models`.\"\n",
    "    for rule in models.get('profile_filters', \n",
    "                           [\"allowed_sectors\",\n",
    "                            \"relevant_to_transition\"]):\n",
    "        models = profile_filter({**models, \"profile_filter_rule\": rule})\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1e9e4-0214-4aa5-976b-8a40b89e995b",
   "metadata": {},
   "source": [
    "@multi\n",
    "def compute_success(models):\n",
    "    \"Compute the success of the two strategies under consideration.\"\n",
    "    return models.get('compute_success_rule')\n",
    "\n",
    "@method(compute_success)\n",
    "def compute_success(models):\n",
    "    \"\"\"Compute the success of the two strategies under consideration for each\n",
    "    number of k mutants implied by the transition.\"\"\"\n",
    "    models = apply_profile_filters(models)\n",
    "    ind1, ind2 = models['transition_indices']\n",
    "    payoffs = models['payoffs']\n",
    "    profiles_filtered = models['profiles_filtered']\n",
    "    payoffs_filtered = {k:v for k,v in payoffs.items() if k in profiles_filtered}\n",
    "    sector_strategies = models['sector_strategies']\n",
    "    allowed_sectors = models['allowed_sectors']\n",
    "    ind1_tuple = list(map(int, ind1.split(\"-\")))\n",
    "    ind2_tuple = list(map(int, ind2.split(\"-\")))\n",
    "    differ = [i1!=i2 for i1, i2 in zip(ind1_tuple, ind2_tuple)]\n",
    "    valid = sum(differ) == 1\n",
    "    affected_sector = f\"S{np.argmax(differ[::-1]) + 1}\"\n",
    "    current_strategy = sector_strategies[affected_sector][ind1[np.argmax(differ)]]\n",
    "    mutant_strategy = sector_strategies[affected_sector][ind2[np.argmax(differ)]]\n",
    "    relevant_players = [player\n",
    "                        for player, sectors in allowed_sectors.items()\n",
    "                        if affected_sector in sectors]\n",
    "    # As all other sectors are fixed, we care only about the number of\n",
    "    # players who can be from the affected sector, and whether that sector\n",
    "    # participating as a player prevents another sector from participating as\n",
    "    # that player.\n",
    "    # What we do depends on the number of players.\n",
    "    # TODO: Generalise code so that it can handle an arbitrary number of players.\n",
    "    # Does it not also depend on how freely players can switch among the other sectors\n",
    "    # as this would lead to more profiles which we could feasibly be in. In these\n",
    "    # cases it matters a lot how the players are sampled. We need a sampling\n",
    "    # function which samples each player from the available sectors.\n",
    "    # With some probability our agent is selected to play as a suitable player\n",
    "    # in a game with the other sampled players.\n",
    "    # We need a sampler which places an agent as a specific player in each\n",
    "    # relevant profile with some probability. The sampler function should accept\n",
    "    # the relevant profiles, the affected sector, and the agent's chosen strategy.\n",
    "    # It may need additional information too. Once we know how likely the player\n",
    "    # is to be a given player using the given strategy in each of the relevant\n",
    "    # profiles, we can easily compute the sucess by taking the dot product\n",
    "    # of the associated payoffs and probabilities.\n",
    "    # TODO: Define a sampler for 0 players, 1 players, 2 players, and N players\n",
    "    if len(relevant_players) == 0:\n",
    "        raise ValueError(\"models['allowed_sectors'] never allows affected sector. Either this transition is invalid or allowed_sectors is mistaken.\")\n",
    "    if len(relevant_players) == 1:\n",
    "        only_player = relevant_players[0]\n",
    "        current_profile = \"_\".join(map(str, [sector_strategies[sector][]]))\n",
    "        new_profile = \n",
    "        πAA = πAB = payoffs_filtered[current_profile][only_player]\n",
    "        πBA = πBB = payoffs_filtered[new_profile][only_player]\n",
    "    πAA = \n",
    "    πAB = \n",
    "    πBA = \n",
    "    πBB =\n",
    "    Z = models['Z'][affected_sector]\n",
    "    ΠA = [πAA*(Z-k-1)/(Z-1) + πAB*k/(Z-1)\n",
    "          for k in range(1, Z)]\n",
    "    ΠB = [πBA*(Z-k)/(Z-1)  + πBB*(k-1)/(Z-1)\n",
    "          for k in range(1, Z)]\n",
    "    return ΠA, ΠB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ec39a-c883-40d6-8efe-033d26623ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#| hide\n",
    "@method(build_transition_matrix, 'multiple-populations-v2')\n",
    "def build_transition_matrix(models:dict # A dictionary that contains the parameters in `ModelTypeEGT`\n",
    "                           ):\n",
    "    \"\"\"Build a transition matrix between all monomorphic states\n",
    "    when there are multiple populations.    \n",
    "    \"\"\"\n",
    "    Z, S, β = [models[k] for k in ['Z', 'recurrent_state_space', 'β']]\n",
    "    π = models['payoffs']\n",
    "    M = np.zeros((payoffs.shape[0], len(S), len(S)))\n",
    "    for row_ind in range(M.shape[-1]):\n",
    "        M[:, row_ind, row_ind] += 1\n",
    "    transition_inds = [(i, j) for i in range(len(S)) for j in range(len(S))]\n",
    "    for row_ind, col_ind in transition_inds:\n",
    "        current_state, new_state = S[row_ind], S[col_ind]\n",
    "        if current_state == new_state:\n",
    "            continue\n",
    "        if not valid_transition(current_state, new_state):\n",
    "            continue\n",
    "        ΠA, ΠB = compute_success(assoc(models,\n",
    "                                       \"transition_indices\",\n",
    "                                       [current_state, new_state]))\n",
    "        ρ = fixation_rate_stable(ΠA, ΠB, β)\n",
    "        n_mutations = sum(valid_transition(current_state, s_alt) for s_alt in S)\n",
    "        M[:, row_ind, col_ind] = ρ / n_mutations\n",
    "        M[:, row_ind, row_ind] -= ρ / n_mutations\n",
    "    return {**models, 'transition_matrix':M}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e45d2e-83b0-4197-9147-3368910118dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         strategy_profile_indices = transition['strategy_profile_indices']\n",
    "#         player_index = transition['player_index']\n",
    "#         row_ind = transition['row_ind']\n",
    "#         col_ind = transition['col_ind']\n",
    "#         πAA = π[:, strategy_profile_indices['AA'], player_index]\n",
    "#         πAB = π[:, strategy_profile_indices['AB'], player_index]\n",
    "#         πBA = π[:, strategy_profile_indices['BA'], player_index]\n",
    "#         πBB = π[:, strategy_profile_indices['BB'], player_index]\n",
    "#         ΠA = [πAA*(Z-k-1)/(Z-1) + πAB*k/(Z-1)\n",
    "#               for k in range(1, Z)]\n",
    "#         ΠB = [πBA*(Z-k)/(Z-1)  + πBB*(k-1)/(Z-1)\n",
    "#               for k in range(1, Z)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22e0c1-bb42-49fd-aed4-877a877e7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create arbitrary payoffs for all possible player combinations\n",
    "# payoffs = {}\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         for k in range(6):\n",
    "#             payoffs[f\"{i}-{j}-{k}\"] = None\n",
    "# payoffs_filtered = {k:v for k,v in payoffs.items() if k in profiles_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd255fe-94e4-4a15-b2a3-cae2f1fefd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 10\n",
    "β = 1\n",
    "recurrent_state_space = [\"0-0-0\", \"0-0-1\", \"0-1-0\", \"0-1-1\",\n",
    "                         \"0-1-1\", \"1-0-0\", \"1-0-1\", \"1-1-1\"]\n",
    "payoffs = {}\n",
    "models = {\"dispatch-type\": \"multiple-populations-v2\",\n",
    "          \"β\": β,\n",
    "          \"Z\": Z,\n",
    "          \"recurrent_state_space\": recurrent_state_space,\n",
    "          \"payoffs\": payoffs,\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7387c-e3a6-4aff-bfc4-6ce526a77853",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2068db8-0c99-4932-bdf8-cfe23f6b0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
