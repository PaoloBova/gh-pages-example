<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.78">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="A number of replications and extensions of the models provided in Han et al.&nbsp;(2020 , 2021, 2022 )">

<title>gh-pages-example - Analysis of the DSAIR model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="gh-pages-example - Analysis of the DSAIR model">
<meta property="og:description" content="A number of replications and extensions of the models provided in Han et al. ([2020](https://doi.org/10.1613/jair.1.12225) , [2021](https://doi.org/10.1371/journal.pone.0244592), [2022](https://doi.">
<meta property="og:site-name" content="gh-pages-example">
<meta name="twitter:title" content="gh-pages-example - Analysis of the DSAIR model">
<meta name="twitter:description" content="A number of replications and extensions of the models provided in Han et al. ([2020](https://doi.org/10.1613/jair.1.12225) , [2021](https://doi.org/10.1371/journal.pone.0244592), [2022](https://doi.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">gh-pages-example</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../blog/index.html" rel="" target="">
 <span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/paolobova/gh-pages-example" rel="" target=""><i class="bi bi-github" role="img" aria-label="Github respository">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Analysis of the DSAIR model</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analytical_conditions.html" class="sidebar-item-text sidebar-link">Analytical conditions</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data_utils.html" class="sidebar-item-text sidebar-link">Data utilities</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../methods.html" class="sidebar-item-text sidebar-link">Methods in Evolutionary Game Theory</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../model_utils.html" class="sidebar-item-text sidebar-link">Model Utilities</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../plots_utils.html" class="sidebar-item-text sidebar-link">Plot utilities</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../types.html" class="sidebar-item-text sidebar-link">Types</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../utils.html" class="sidebar-item-text sidebar-link">Utilities</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Methods</span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../Payoffs/index.html" class="sidebar-item-text sidebar-link">Payoffs</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Payoffs/payoffs1.html" class="sidebar-item-text sidebar-link">Payoff Matrices (part 1)</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Payoffs/payoffs2.html" class="sidebar-item-text sidebar-link">Payoff Matrices (part 2)</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../Analysis/index.html" class="sidebar-item-text sidebar-link">Analysis</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Analysis/analysis_dsair.html" class="sidebar-item-text sidebar-link active">Analysis of the DSAIR model</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#baseline-dsair-model" id="toc-baseline-dsair-model" class="nav-link active" data-scroll-target="#baseline-dsair-model">Baseline DSAIR model</a>
  <ul>
  <li><a href="#create-parameter-space" id="toc-create-parameter-space" class="nav-link" data-scroll-target="#create-parameter-space">Create parameter space</a></li>
  <li><a href="#build_dsair" id="toc-build_dsair" class="nav-link" data-scroll-target="#build_dsair">build_DSAIR</a></li>
  <li><a href="#run-the-model" id="toc-run-the-model" class="nav-link" data-scroll-target="#run-the-model">Run the model</a></li>
  <li><a href="#process-the-results" id="toc-process-the-results" class="nav-link" data-scroll-target="#process-the-results">Process the results</a></li>
  <li><a href="#visualise-results-and-explain-what-we-observe" id="toc-visualise-results-and-explain-what-we-observe" class="nav-link" data-scroll-target="#visualise-results-and-explain-what-we-observe">Visualise results and explain what we observe</a></li>
  </ul></li>
  <li><a href="#dsair-with-punishment" id="toc-dsair-with-punishment" class="nav-link" data-scroll-target="#dsair-with-punishment">DSAIR with punishment</a>
  <ul>
  <li><a href="#punishments-on-longer-timelines" id="toc-punishments-on-longer-timelines" class="nav-link" data-scroll-target="#punishments-on-longer-timelines">Punishments on longer timelines</a></li>
  </ul></li>
  <li><a href="#rewards" id="toc-rewards" class="nav-link" data-scroll-target="#rewards">Rewards</a></li>
  <li><a href="#voluntary-commitments" id="toc-voluntary-commitments" class="nav-link" data-scroll-target="#voluntary-commitments">Voluntary Commitments</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/PaoloBova/gh-pages-example/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Analysis of the DSAIR model</h1>
</div>

<div>
  <div class="description">
    A number of replications and extensions of the models provided in Han et al.&nbsp;(<a href="https://doi.org/10.1613/jair.1.12225">2020</a> , <a href="https://doi.org/10.1371/journal.pone.0244592">2021</a>, <a href="https://doi.org/10.1016/j.techsoc.2021.101843">2022</a> )
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>This notebook contains a number of analyses of different versions of the DSAIR model.</p>
<p>Each analysis involves a number of steps:</p>
<ol type="1">
<li>Create parameter space</li>
<li>Run the model</li>
<li>Process the results</li>
<li>Visualise results and explain what we observe</li>
</ol>
<section id="baseline-dsair-model" class="level2">
<h2 class="anchored" data-anchor-id="baseline-dsair-model">Baseline DSAIR model</h2>
<section id="create-parameter-space" class="level3">
<h3 class="anchored" data-anchor-id="create-parameter-space">Create parameter space</h3>
<hr>
<p><a href="https://github.com/PaoloBova/gh-pages-example/blob/main/gh_pages_example/payoffs.py#L19" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="build_dsair" class="level3">
<h3 class="anchored" data-anchor-id="build_dsair">build_DSAIR</h3>
<blockquote class="blockquote">
<pre><code> build_DSAIR (b:Union[float,list[float],numpy.ndarray,dict]=4,
              c:Union[float,list[float],numpy.ndarray,dict]=1,
              s:Union[float,list[float],numpy.ndarray,dict]={'start': 1,
              'stop': 5.1, 'step': 0.1},
              p:Union[float,list[float],numpy.ndarray,dict]={'start': 0,
              'stop': 1.02, 'step': 0.02},
              B:Union[float,list[float],numpy.ndarray,dict]=10000,
              W:Union[float,list[float],numpy.ndarray,dict]=100,
              pfo:Union[float,list[float],numpy.ndarray,dict]=0,
              α:Union[float,list[float],numpy.ndarray,dict]=0,
              γ:Union[float,list[float],numpy.ndarray,dict]=0,
              epsilon:Union[float,list[float],numpy.ndarray,dict]=0,
              ω:Union[float,list[float],numpy.ndarray,dict]=0, collective_
              risk:Union[float,list[float],numpy.ndarray,dict]=0,
              β:Union[float,list[float],numpy.ndarray,dict]=0.01,
              Z:int=100, strategy_set:list[str]=['AS', 'AU'],
              exclude_args:list[str]=['Z', 'strategy_set'],
              override:bool=False, drop_args:list[str]=['override',
              'exclude_args', 'drop_args'])</code></pre>
</blockquote>
<p>Initialise baseline DSAIR models for all combinations of the provided parameter valules. By default, we create models for replicating Figure 1 of Han et al.&nbsp;2021.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>b</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>4</td>
<td>benefit: The size of the per round benefit of leading the AI development race, b&gt;0</td>
</tr>
<tr class="even">
<td>c</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>1</td>
<td>cost: The cost of implementing safety recommendations per round, c&gt;0</td>
</tr>
<tr class="odd">
<td>s</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>{‘start’: 1, ‘stop’: 5.1, ‘step’: 0.1}</td>
<td>speed: The speed advantage from choosing to ignore safety recommendations, s&gt;1</td>
</tr>
<tr class="even">
<td>p</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>{‘start’: 0, ‘stop’: 1.02, ‘step’: 0.02}</td>
<td>avoid_risk: The probability that unsafe firms avoid an AI disaster, p ∈ [0, 1]</td>
</tr>
<tr class="odd">
<td>B</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>10000</td>
<td>prize: The size of the prize from winning the AI development race, B&gt;&gt;b</td>
</tr>
<tr class="even">
<td>W</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>100</td>
<td>timeline: The anticipated timeline until the development race has a winner if everyone behaves safely, W ∈ [10, 10**6]</td>
</tr>
<tr class="odd">
<td>pfo</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>0</td>
<td>detection risk: The probability that firms who ignore safety precautions are found out, pfo ∈ [0, 1]</td>
</tr>
<tr class="even">
<td>α</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>0</td>
<td>the cost of rewarding/punishing a peer</td>
</tr>
<tr class="odd">
<td>γ</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>0</td>
<td>the effect of a reward/punishment on a developer’s speed</td>
</tr>
<tr class="even">
<td>epsilon</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>0</td>
<td>commitment_cost: The cost of setting up and maintaining a voluntary commitment, ϵ &gt; 0</td>
</tr>
<tr class="odd">
<td>ω</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>0</td>
<td>noise: Noise in arranging an agreement, with some probability they fail to succeed in making an agreement, ω ∈ [0, 1]</td>
</tr>
<tr class="even">
<td>collective_risk</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>0</td>
<td>The likelihood that a disaster affects all actors</td>
</tr>
<tr class="odd">
<td>β</td>
<td>typing.Union[float, list[float], numpy.ndarray, dict]</td>
<td>0.01</td>
<td>learning_rate: the rate at which players imitate each other</td>
</tr>
<tr class="even">
<td>Z</td>
<td>int</td>
<td>100</td>
<td>population_size: the number of players in the evolutionary game</td>
</tr>
<tr class="odd">
<td>strategy_set</td>
<td>list</td>
<td>[‘AS’, ‘AU’]</td>
<td>the set of available strategies</td>
</tr>
<tr class="even">
<td>exclude_args</td>
<td>list</td>
<td>[‘Z’, ‘strategy_set’]</td>
<td>a list of arguments that should be returned as they are</td>
</tr>
<tr class="odd">
<td>override</td>
<td>bool</td>
<td>False</td>
<td>whether to build the grid if it is very large</td>
</tr>
<tr class="even">
<td>drop_args</td>
<td>list</td>
<td>[‘override’, ‘exclude_args’, ‘drop_args’]</td>
<td>a list of arguments to drop from the final result</td>
</tr>
<tr class="odd">
<td><strong>Returns</strong></td>
<td><strong>dict</strong></td>
<td></td>
<td><strong>A dictionary containing items from <a href="https://PaoloBova.github.io/gh-pages-example/types.html#modeltypedsair"><code>ModelTypeDSAIR</code></a> and <a href="https://PaoloBova.github.io/gh-pages-example/methods.html#modeltypeegt"><code>ModelTypeEGT</code></a></strong></td>
</tr>
</tbody>
</table>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> build_DSAIR()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="run-the-model" class="level3">
<h3 class="anchored" data-anchor-id="run-the-model">Run the model</h3>
<p>I use <a href="https://PaoloBova.github.io/gh-pages-example/utils.html#thread_macro"><code>thread_macro</code></a> to pipe the <code>models</code> I created earlier through each function that follows.</p>
<p>I first build the payoff matrices for each model, and compute some analytical thresholds that will be useful in our discussion. Finally, I pass the result to <a href="https://PaoloBova.github.io/gh-pages-example/methods.html#markov_chain"><code>markov_chain</code></a>, a method for computing the distribution of strategies that will be present in the population in the long run.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> thread_macro(models,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                       payoffs_sr,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                       payoffs_lr,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                       threshold_society_prefers_safety_dsair,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                       threshold_risk_dominant_safety_dsair,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>                       markov_chain,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>                      )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="process-the-results" class="level3">
<h3 class="anchored" data-anchor-id="process-the-results">Process the results</h3>
<p>Now that we have collected some results, we need to process them so that we can display what we want to.</p>
<p>The general approach I follow is to flatten the <code>results</code> dictionary and convert it into a <a href="https://pandas.pydata.org/pandas-docs/stable/index.html">pandas</a> dataframe. I have a convenience function called <a href="https://PaoloBova.github.io/gh-pages-example/data_utils.html#results_to_dataframe_egt"><code>results_to_dataframe_egt</code></a> for this purpose.</p>
<p>In <a href="https://PaoloBova.github.io/gh-pages-example/data_utils.html#process_dsair_data"><code>process_dsair_data</code></a>, we also compute the risk of an AI related disaster, <span class="math inline">\(p_{risk} = 1 - p\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> thread_macro(results,</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>                  results_to_dataframe_egt,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                  process_dsair_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>s</th>
      <th>p</th>
      <th>b</th>
      <th>c</th>
      <th>B</th>
      <th>W</th>
      <th>pfo</th>
      <th>α</th>
      <th>γ</th>
      <th>epsilon</th>
      <th>ω</th>
      <th>collective_risk</th>
      <th>β</th>
      <th>threshold_society_prefers_safety</th>
      <th>threshold_risk_dominant_safety</th>
      <th>AS_frequency</th>
      <th>AU_frequency</th>
      <th>pr</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.00</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.019231</td>
      <td>0.662338</td>
      <td>1.000000e+00</td>
      <td>8.508121e-12</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>0.02</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.019231</td>
      <td>0.662338</td>
      <td>1.000000e+00</td>
      <td>3.927629e-11</td>
      <td>0.98</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>0.04</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.019231</td>
      <td>0.662338</td>
      <td>1.000000e+00</td>
      <td>1.813123e-10</td>
      <td>0.96</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>0.06</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.019231</td>
      <td>0.662338</td>
      <td>1.000000e+00</td>
      <td>8.369975e-10</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>0.08</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.019231</td>
      <td>0.662338</td>
      <td>1.000000e+00</td>
      <td>3.863857e-09</td>
      <td>0.92</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2086</th>
      <td>5.0</td>
      <td>0.92</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.797619</td>
      <td>0.932921</td>
      <td>0.000000e+00</td>
      <td>1.000000e+00</td>
      <td>0.08</td>
    </tr>
    <tr>
      <th>2087</th>
      <td>5.0</td>
      <td>0.94</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.797619</td>
      <td>0.932921</td>
      <td>0.000000e+00</td>
      <td>1.000000e+00</td>
      <td>0.06</td>
    </tr>
    <tr>
      <th>2088</th>
      <td>5.0</td>
      <td>0.96</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.797619</td>
      <td>0.932921</td>
      <td>0.000000e+00</td>
      <td>1.000000e+00</td>
      <td>0.04</td>
    </tr>
    <tr>
      <th>2089</th>
      <td>5.0</td>
      <td>0.98</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.797619</td>
      <td>0.932921</td>
      <td>0.000000e+00</td>
      <td>1.000000e+00</td>
      <td>0.02</td>
    </tr>
    <tr>
      <th>2090</th>
      <td>5.0</td>
      <td>1.00</td>
      <td>4</td>
      <td>1</td>
      <td>10000</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.01</td>
      <td>0.797619</td>
      <td>0.932921</td>
      <td>2.710175e-20</td>
      <td>1.000000e+00</td>
      <td>0.00</td>
    </tr>
  </tbody>
</table>
<p>2091 rows × 18 columns</p>
</div>
</div>
</div>
</section>
<section id="visualise-results-and-explain-what-we-observe" class="level3">
<h3 class="anchored" data-anchor-id="visualise-results-and-explain-what-we-observe">Visualise results and explain what we observe</h3>
<p>I am using the <a href="https://matplotlib.org/stable/index.html">Matplotlib</a> library to visualize our data.</p>
<p>I first replicate Figure 1 from The Anh Han et al.&nbsp;2021</p>
<p>The figure describes how the the frequency of Always Unsafe (<strong>AU</strong>) varies with both the speed advantage given to those who play <strong>AU</strong>, <span class="math inline">\(s\)</span>, and the risk that such firms cause an AI disaster, <span class="math inline">\(p_{risk}\)</span>. We have also plotted two lines, the lower line shows the boundary where society is indifferent between the two strategies. A greater risk or a slower speed advantage from this boundary implies society prefers players to play Always Safe (<strong>AS</strong>). The higher line shows the threshold for which <strong>AU</strong> is risk dominant over <strong>AS</strong>. For this baseline model, risk dominance implies that the strategy will be selected for by evolution (which is why the line follows the boundary where players switch from <strong>AU</strong> to <strong>AS</strong>). As with the lower line, any higher risk or lower speed implies that <strong>AS</strong> will instead by risk dominant over <strong>AU</strong>.</p>
<p>These lines therefore split the heatmap into 3 regions. (i) Society prefers <strong>AS</strong> and <strong>AS</strong> is selected by social learning. (ii) Society prefers <strong>AS</strong> but <strong>AU</strong> is selected by social learning (iii) Society prefers <strong>AU</strong> and <strong>AU</strong> is selected by social learning</p>
<p>In region (i) companies will be alligned with Society’s preference for safety. In region (iii), society is willing to accept the risks as they anticipate greater benefits from innovation. In region (ii), we see a dilemma where all players are choosing to play <strong>AU</strong>, even though society prefers them to play <strong>AS</strong>. We can refer to this region as the <em>Dilemma zone</em>.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that this model is illustrative only: at best here, society refers to the collection of all firms.</p>
<p>We could instead explicitly model society’s preferences over safety and innovation, distinct from the companies. Such a model will still have the 3 regions we are currently discussing, though the negative externalities of an AI disaster will likely lead to a greater <em>dillemma zone</em>.</p>
<p>Another insight that such an extension would communicate is that companies may have incentive to work together to make sure their preferences are weighted more highly than the rest of society. It would be interesting to see whether we can observe this in pracitse, for examle in the European AI Act. The main challenge this task presents us is how to determine whether companies are working together to have their voices heard or whether they each already have strong enough incentives to uniltaterally influence policy.</p>
</div>
</div>
<p>I have also plotted a cross-section of the above heatmap for speed advantage, <span class="math inline">\(s=1.5\)</span>. This plot shows how players are distributed between the 2 available strategies, <span class="math inline">\(AU\)</span> and <span class="math inline">\(AS\)</span>. Here, the blue area represents the proportion of players who follow <span class="math inline">\(AU\)</span> for each level of risk, whereas the red area tells us the proportion who play <span class="math inline">\(AS\)</span>. I also mark the 3 regions discussed above.</p>
<p>We will often make use of this cross-section plot in more complex models when we want to show the relative frequencies of more than 2 strategies.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="dsair-with-punishment" class="level2">
<h2 class="anchored" data-anchor-id="dsair-with-punishment">DSAIR with punishment</h2>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The above plots show what the addition of peer punishment can do for the dynamics of an AI Development race. In the scenario presented above, a sufficiently large punishment can completely eliminate unsafe behaviour, even when the cost is somewhat large for the sanctioner (plot 3).</p>
<p>We compare this to a scenario where the sanctioner is all talk (i.e.&nbsp;punishment is costless to both parties). Here, we see no difference in the distribution of strategies compared to the baseline model (plot 1)</p>
<p>When the punishment hurts both parties equally, we see that it can curtail unsafe behavior, but to a far lesser extent. In such a scenario, it is more difficult for <strong>PS</strong> players to survive, so they provide less protection to <strong>AS</strong> players from <strong>AS</strong> players (plot 2).</p>
<p>One important takeaway which is apparent above is that punishments can be too punishing. Even when society collectively prefers that players are <strong>AU</strong>, punishment can prevent players from doing so.</p>
<p>This isn’t an obvious problem to overcome. Due to the Colignridge Dilemma, it will be very difficult for companies to determine what the risk of an AI disaster will be.</p>
<p>We have so far only looked at short timelines. On longer timelines, we would expect punishments to be less effective, but we also expect the dilemma region to be smaller, so an interesting question is whether we should expect punishments to be as overzealous in restricting behaviour. Could it even be that punishments are insufficient to remove the dilemma zone on longer timelines?</p>
<p>The answer is yes to both questions. See the plots below.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Below, I replicate 3 of the subfigures from Figure 5 of Han et al 2021, Mediating AI development races with incentives.</p>
<p>All these figures show th effects of punishment in a typical AI development race on a short timescale.</p>
<p>These figures show the effects of punishment as it varies in efficiency for a range of AI risk levels. With larger risk, players are more likely to play safe anyways, so punishments are rarely used. When risk is much lower, players prefer to be unsafe, although efficient punishments deter risk taking behaviour. Efficient punishments can mitigate unsafe behaviour when AI risk is in an intermediate stage, where it is not high enough to motivate firms to be safe, but is still high enough in principle to warrant their caution on society’s behalf.</p>
<p>Note again that these numbers are illustrative. Depending on the size of a disaster, much lower levels of risk may demand caution from the international community.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-14-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-14-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Below, I replicate another figure from the paper. We display the frequency of <strong>Always Unsafe</strong> for a range of speed advantages and risk levels. The presence of a Peer Punishment strategy appears to mitigate the dilemma zone we noted earlier. Here, punishments are 75% effective in slowing down unsafe firms and apply the same slowdown to the sanctioner who levied the punishment, <span class="math inline">\(s_γ=s_α=3s/4\)</span>.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The above figure looks fairly similar to the paper. However, I intuitively expected the reduction in AU to be more significant. Try making this graph specifically in mathematica and see what comes up.</p>
<section id="punishments-on-longer-timelines" class="level3">
<h3 class="anchored" data-anchor-id="punishments-on-longer-timelines">Punishments on longer timelines</h3>
<p>I now extend the analysis of punishments to longer timelines. A longer timeline implies that any large benefits from winning the development race are likely to occur farther into the future, which means that they may be weighted less than before relative to the benefits of selling AI services in the current period (we capture this by looking at the average payoffs to firms over the duration of the race).</p>
<p>It is likely that increasing timelines has the same effect as reducing the prize from winning the race. This is in fact the case for the DSAIR model without punishments. Either change in the simpler model would ubiquitously reduce incentives to race unsafely.</p>
<p>In the case of punishments, the story is somewhat more complicated. On the one hand, it is now easier for punishments of different levels of impact to not only disincentivise but also stop the R&amp;D activities of unsafe firms. On the other hand, it also means that firms who levy the punishments could more easily fail to survive the experience. If such firms are less able to survive, they are less likely to offer protection to safe firms from being exploited by unsafe firms.</p>
<p>Nevertheless, we should longer timescales to lead to a net increase in punishment efficiency, as safer firms who levy punishments are less motivated by the long-run prize in the first place.</p>
<p>Yet, a more nuanced question is whether this is what society wants? On longer timelines, society is likely to want to encourage more risk taking. We should anticipate that the improved efficacy of punishments should in some scenarios conflict with society’s preferences. This would be particularly troubling if this were to lead to backlash against the punishers, especially if society’s preferences failed to capture important negative externalities that could arise from risky technology races (e.g.&nbsp;the tail risk of a catastrophe caused by the misuse of autonomous weapons).</p>
<p>How can we represent a longer timeline?</p>
<p>We represent short timelines as taking <span class="math inline">\(W=100\)</span> time periods. If we assume firms can act nearly every month, then this corresponds to a timeline of around 10 years.</p>
<p>Surveys of AI experts suggest that those with longer timelines assume the development of AI systems which surprass humans in all domains to at least take a century, so a <span class="math inline">\(W=1000\)</span> seems appropriate (see Katja et al.&nbsp;2016, Carlier et al, Brundage et al.). However, note that there are timelines where the development of transformative AI systems place longer timelines at around 60 years instead, see Cotra 2020 (transformative AI systems are sufficient to cause radical economic change to a similar degree than the industrial revolution).</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-16-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-16-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Are the results above for a longer timeline what we anticipated?</p>
<p>Taking each graph in order, starting with the figure for high AI risk, we see that unsafe behaviour is barely present (though it is perhaps notable that on a longer timeline, the slightly different shades indicate that firms are unsafe for a very small percentage of time, which vanishes with more efficient punishments).</p>
<p>For the figure with intermediate risk, the differences with shorter timelines are more apparent. Unsafe frequencies are much smaller and there is a larger region where no unsafe behaviour is present at all.</p>
<p>The final figure with low risk tells a similar story. The region where no unsafe behaviour is present is very large and the region where all companies are unsafe is smaller than on a shorter timeline.</p>
<p>How do we find the interaction effect of longer timelines with punishments. To isolate this interaction effect, we could take the difference of the two plots. If longer timelines had a uniform effect on unsafe behaviour, then the differences would be constant. Our results above already indicate that there is no such uniform effect.</p>
<p>However, it may be of interest to know whether the effect of longer timelines is weaker for more efficient punishmens and stronger for less efficient ones. This effect is more difficult to determine, as the most efficient punishments already quell all unsafe behaviour. Though, on close inspection of the figures above, one can see that clearly the difference is greatest where punishments would otherwise have just failed to reduce unsafe behaviour, near the boundary where on shorter timelines for the last two plots we see a quick switch from punishments completely stopping unsafe behavior to failing to stop any unsafe behaviour at all. This is strong evidence that longer timelines intensify the effect of punishments on unsafe behaviour.</p>
<p>To answer whether this is what society wants (ignoring externalities), I now plot the unsafe frequencies for a range of AI risk levels and speed advantages, as well as the thresholds for when society prefers safety and when unsafe behaviour would usually dominate safe behaviour in the absense of punishment strategies. If you recall our earlier discussion, these thresholds split the figure into 3 regions, the middle region representing a <em>dilemma zone</em> where safety is collectively preffered but unsafe behaviour is what everyone learns to do.</p>
<p>Before we consider this figure, you may wish to remind yourself of the analogous figure for shorter timelines. We saw that punishments which were equivalent to a reduction of 75% the speed that unsafe firms could obtain were only somewhat effective in shrinking the dilemma zone. This was consistent with our earlier figures since equal and relatively large speed reductions for the sanctioner and punished would place us closer to the upper right corner of those figures which plotted the unsafe frequencies as we varied the sanctioner and punished costs.</p>
<p>However, we know that longer timelines allow punishments to be considerably more effective, even in those areas. We should anticipate a greater shrinkage of the dilemma zone. Whether this is what society wants is less clear from the outset. It is plausible that unsafe behaviour diminishes so much that it eats into the region where risk taking is prefferred by society over safety.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The figure above is surprising. First, notice that the threshold AI risk level for society to prefer safety is higher than before as expected (the threshold for unsafe behaviour risk dominating safe behaviour should appear to be somewhat lower, but the difference in this case is barely apparent).</p>
<p>Most dramatic is the reduciton in unsafe behaviour in the dilemma zone. Although the frequency is still above 50% for most of the dilemma zone, the overall effect is much higher than on shorter timelines. Note that we have also plotted the threshold for the Sanctioners to risk dominate the unsafe firms they punish. Even though this threshold is very close to the threshold before safe behaviour is risk dominant, we still see a dramatic reduction in unsafe behaviour before this. Even if sanctioners are not selected for by evolution, they can still catalyse safe behaviour.</p>
<p>My warnings of overegulation in this scenario are unwarranted. While there is a slight reduction in risk taking behaviour in the lower left corner where AI risk and the speed advantage are small, the effect is minimal.</p>
<p>What is more interesting is the effect when risk is higher. It is notable that even when safe behaviour is risk dominant, that a small percentage of firms still choose to be unsafe. This happens because the average payoffs for safe firms when against other safe firms also falls with the longer timeline. Tied with our relatively weak learning rate, <span class="math inline">\(beta=0.01\)</span>, this leads to a less dramatic swtich from unsafe to safe behaviour. Importantly, this is not a consequence of the punishments themselves.</p>
<p>One other check I would like to do is to see if the figure would be similar if we had instead shrunk the prize directly by a factor of <span class="math inline">\(10\)</span>. In the simpler DSAIR model, this change would be equivalent a <span class="math inline">\(10\)</span> times longer timeline, since players only considered the average prize over the duration of the timeline.</p>
<p>However, our punishments ensure that players who are unsafe, players who are unsafe but who are punished, players who are safe but sanction unsafe players, and safe players would all finish the race at different times. This implies that the effect of punishments could be starkly different than if we had shrunk the prize directly. Where it not so, if extending the timeline had the same effect as a direct prize reduction, then we might have anticipated that punishments had a qualitatively similar effect on unsafe behaviour as on shorter timelines. This was not the case.</p>
<p>Let’s plot the above figures again with smaller prize, <span class="math inline">\(B\)</span>, for comparison.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-18-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-18-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Again, I am surprised. The smaller prize did in fact have the same effects as a longer timeline. It seems that the asymmetric effects on when sanctioners and the punished finish their races is just not large enough to influence the race independently of the effects of longer timelines. This is likely because it has (i) a negligible effect on who wins the race, and (ii) a negligible effect on the product of the prize and the time taken to win the race.</p>
<p>I should also double check whether the effect is largely driven by timelines rather than the punishments by setting the punishment effects to be <span class="math inline">\(0\)</span>.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The above figure confirms that most of the effect is from longer timelines independent of its interaction with punishments. However, it is noticeable that longer timelines dramatically boost safety when the speed advantage is low. It also boosts safety for higher speed values, though this may be less aparrent (try spotting the larger purple area at <span class="math inline">\(s=5\)</span>).</p>
<p>I should also see whether the above effect on punishments (so punishment costs are no longer <span class="math inline">\(0\)</span>) is mainly driven by lower payoffs on average as a whole (by shrinking the learning rate, <span class="math inline">\(\beta\)</span> by a factor of <span class="math inline">\(10\)</span>). A glance below suggests that this is the case (there are slight but negligible differences)</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>All in all, it appears that the effects of changing <span class="math inline">\(B\)</span>, <span class="math inline">\(W\)</span>, and <span class="math inline">\(\beta\)</span> are largely interchangeable, even in the presence of punishments.</p>
<p>One takeaway is that in this simple model, the impact of punishments (as with the rest of the DSAIR models we have considered so far) only considers the average prize over the duration of the timeline. This is a strong assumption which in principle we could devise a test for: do companies really act to increase the average prize they expect over the long run? Do they instead discount future possible gains and act to maximise the net present value of their activities.</p>
<p>Admittedly, the mitigation effects of punishments still appear less than ideal. The <em>dilemma zones</em> remains large.</p>
<p>We could consider a different punishment scheme than we considered in both the short and long timeline scenarios above, one where perhaps due to well-designed incentives, we permit more efficient punishments which are less costly to the sanctioner. In these cases, we are much more likely to reduce unsafe behaviour (perhaps too when risk taking would be collectively desired).</p>
<p>Let’s consider naively that this time the speed reduction for sanctioners is only 25% of the speed that unsafe firms can obtain.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The analysis above is fairly comprehensive for our punishments model. A consideration of short term costs and benefits is unlikely to yield any further insights.</p>
<p>A further examination of punishments might consider more complex industry interactions. It seems unlikely that there will only be two firms in the race. Could punishments be more effective if there are multiple firms, and one firm can punish an unsafe one to encourage a remaining safe firm to win? This seems like it has the potential to be a useful extension.</p>
</section>
</section>
<section id="rewards" class="level2">
<h2 class="anchored" data-anchor-id="rewards">Rewards</h2>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="voluntary-commitments" class="level2">
<h2 class="anchored" data-anchor-id="voluntary-commitments">Voluntary Commitments</h2>
<p>We replicate a figure from the Voluntary Commitment paper below.</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="10_analysis_dsair_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>