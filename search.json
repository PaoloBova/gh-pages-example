[
  {
    "objectID": "blog/Posts/2022-10-27-blog1/index.html",
    "href": "blog/Posts/2022-10-27-blog1/index.html",
    "title": "Thoughts on: Recommendations for the AI RMF",
    "section": "",
    "text": "Regulatory roadmap image"
  },
  {
    "objectID": "blog/Posts/2022-10-27-blog1/index.html#what-is-nists-ai-rmf",
    "href": "blog/Posts/2022-10-27-blog1/index.html#what-is-nists-ai-rmf",
    "title": "Thoughts on: Recommendations for the AI RMF",
    "section": "What is NIST’s AI RMF?",
    "text": "What is NIST’s AI RMF?\nThe US National Institute for Standards and Technology (NIST) is preparing an initial draft of their Risk Managament Framework for AI (the AI RMF). This is a non-binding set of standards that developers of AI are encouraged to follow. These standards aim to provided recommendations that companies can follow to minimise regulatory uncertainty, even if they operate across nations.\nSome questions which arise:\n\nDo organisations follow NIST recommendations? Which ones?\nIf organisations do not follow NIST recommendations, are there any soft pressures to encourage them to reconsider?\nIs there precedence for NIST to accept recommendations from academic institutions into their guidance for industry, as the current paper hopes to achieve?\n\nIn addition, the topic of regulating AI systems raises a number of questions which are important to address:\n\nIs it reasonable to expect that AI systems have a sufficiently high probability of being powerful enough to risk catastrophic outcomes on a relevant timeline?\nAre these risks more likely if there is competitive and geopolitical incentives to develop AI capabilities faster?\nWhat do other academics think?\nWhat do other policy makers think?\nIs there any reason to expect the EU AI Act and US regulation to coordinate?"
  },
  {
    "objectID": "blog/Posts/2022-10-27-blog1/index.html#nbdev-in-industry",
    "href": "blog/Posts/2022-10-27-blog1/index.html#nbdev-in-industry",
    "title": "Thoughts on: Recommendations for the AI RMF",
    "section": "nbdev in industry",
    "text": "nbdev in industry\nWe have piloted nbdev at several companies. We were delighted to receive the following feedback, which fits our own experience using and developing nbdev:\n\n\n\nDavid Berg, on using nbdev for internal documentation at Netflix: “Prior to using nbdev, documentation was the most cumbersome aspect of our software development process… Using nbdev allows us to spend more time creating rich prose around the many code snippets guaranteeing the whole experience is robust. nbdev has turned what was once a chore into a natural extension of the notebook-based testing we were already doing.”\n\n\n\n\n\n\nErik Gaasedelen, on using nbdev in production at Lyft: “I use this in production at my company. It’s an awesome tool… nbdev streamlines everything so I can write docs, tests, and code all in one place… The packaging is also really well thought out. From my point of view it is close to a Pareto improvement over traditional Python library development.”\n\n\n\n\n\n\nHugo Bowne-Anderson, on using nbdev for Outerbounds: “nbdev has transformed the way we write documentation. Gone are the days of worrying about broken code examples when our API changes or [due to] human errors associated with copying & pasting code into markdown files. The authoring experience of nbdev… [allows] us to write prose and live code in a unified interface, which allows more experimentation… On top of this, nbdev allows us to include unit tests in our documentation which mitigates the burden of maintaining the docs over time.”\n\n\n\n\n\n\nRoxanna Pourzand, on using nbdev for Transform: “We’re so excited about using nbdev. Our product is technical so our resulting documentation includes a lot of code-based examples. Before nbdev, we had no way of maintaining our code examples and ensuring that it was up-to-date for both command inputs and outputs. It was all manual. With nbdev, we now have this under control in a sustainable way. Since we’ve deployed these docs, we also had a situation where we were able to identify a bug in one of our interfaces, which we found by seeing the error that was output in the documentation.”"
  },
  {
    "objectID": "blog/Posts/2022-10-27-blog1/index.html#whats-nbdev",
    "href": "blog/Posts/2022-10-27-blog1/index.html#whats-nbdev",
    "title": "Thoughts on: Recommendations for the AI RMF",
    "section": "What’s nbdev?",
    "text": "What’s nbdev?\nNbdev embraces the dynamic nature of python and REPL-driven development in ways that traditional IDEs and software development workflows cannot. We thoroughly discussed the motivation, history, and goals of nbdev in this initial launch post three years ago. The creator of Jupyter, Fernando Pérez, told us:\n\n[Nbdev] should be celebrated and used a lot more - I have kept a tab with your original nbdev blog post open for months in Chrome because of how often I refer to it and point others to this work\n\nIn short, nbdev embraces ideas from literate programming and exploratory programming. These paradigms have been revisited in platforms like XCode Playgrounds and languages like Smalltalk, LISP, and Mathematica. With nbdev, we sought to push these paradigms even further by enabling it for one of the most popular dynamic programming languages in the world: Python.\nEven though nbdev is most widely used in scientific computing communities due to its integration with Jupyter Notebooks, we’ve found that nbdev is well suited for a much wider range of software. We have used nbdev to write deep learning libraries, API clients, python language extensions,terminal user interfaces, and more!\nHamel: When I use nbdev, my colleagues are often astounded by how quickly I can create and distribute high-quality python packages. I consider nbdev to be a superpower that allows me to create tests and documentation without any additional friction, which makes all of my projects more maintainable. I also find writing software with nbdev to be more fun and productive as I can iterate very fast on ideas relative to more traditional software engineering workflows. Lastly, with nbdev I can also use traditional text-based IDEs if I want to, so I get the best of both worlds."
  },
  {
    "objectID": "blog/Posts/2022-10-27-blog1/index.html#what-we-learned-after-three-years-of-using-nbdev",
    "href": "blog/Posts/2022-10-27-blog1/index.html#what-we-learned-after-three-years-of-using-nbdev",
    "title": "Thoughts on: Recommendations for the AI RMF",
    "section": "What we learned after three years of using nbdev",
    "text": "What we learned after three years of using nbdev\nWhile nbdev was originally developed to simplify the software development workflow for various fast.ai projects, we found that users wanted to extend nbdev to:\n\nWrite and publish blog posts, books, papers, and other types of documents with Jupyter Notebooks\nDocument existing codebases not written in nbdev\nAccommodate traditional Python conventions–for those constrained in how their code is organized and formatted\nPublish content using any static site generator\n\nWhile we created projects such as fastpages and fastdoc to accomplish some of these tasks, we realized that it would be better to have a single set of flexible tools to accomplish all of them. To this end, we were extremely excited to discover Quarto, an open-source technical publishing system built on pandoc.\nHamel: The more I used nbdev for creating Python modules, the more I wanted to use it for writing blogs and documenting existing codebases. The ability to customize the way notebooks are rendered (hiding vs. showing cells, stripping output, etc.), along with the facilities for including unit tests, made it my go-to authoring tool for all technical content. I’m excited that nbdev2 unlocks all of these possibilities for everyone!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Phd Blog",
    "section": "",
    "text": "Thoughts on: Recommendations for the AI RMF\n\n\n\n\n\nA review of the recommendations provided by the team at the Berkley Existential Risk Initiative.\n\n\n\n\n\n\nNov 1, 2022\n\n\nPaolo Bova\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Analysis/analysis_dsair.html",
    "href": "Analysis/analysis_dsair.html",
    "title": "Analysis of the DSAIR model",
    "section": "",
    "text": "This notebook contains a number of analyses of different versions of the DSAIR model.\nEach analysis involves a number of steps:"
  },
  {
    "objectID": "Analysis/analysis_dsair.html#baseline-dsair-model",
    "href": "Analysis/analysis_dsair.html#baseline-dsair-model",
    "title": "Analysis of the DSAIR model",
    "section": "Baseline DSAIR model",
    "text": "Baseline DSAIR model\n\nCreate parameter space\n\nsource\n\n\nbuild_DSAIR\n\n build_DSAIR (b:Union[float,list[float],numpy.ndarray,dict]=4,\n              c:Union[float,list[float],numpy.ndarray,dict]=1,\n              s:Union[float,list[float],numpy.ndarray,dict]={'start': 1,\n              'stop': 5.1, 'step': 0.1},\n              p:Union[float,list[float],numpy.ndarray,dict]={'start': 0,\n              'stop': 1.02, 'step': 0.02},\n              B:Union[float,list[float],numpy.ndarray,dict]=10000,\n              W:Union[float,list[float],numpy.ndarray,dict]=100,\n              pfo:Union[float,list[float],numpy.ndarray,dict]=0,\n              α:Union[float,list[float],numpy.ndarray,dict]=0,\n              γ:Union[float,list[float],numpy.ndarray,dict]=0,\n              epsilon:Union[float,list[float],numpy.ndarray,dict]=0,\n              ω:Union[float,list[float],numpy.ndarray,dict]=0, collective_\n              risk:Union[float,list[float],numpy.ndarray,dict]=0,\n              β:Union[float,list[float],numpy.ndarray,dict]=0.01,\n              Z:int=100, strategy_set:list[str]=['AS', 'AU'],\n              exclude_args:list[str]=['Z', 'strategy_set'],\n              override:bool=False, drop_args:list[str]=['override',\n              'exclude_args', 'drop_args'])\n\nInitialise baseline DSAIR models for all combinations of the provided parameter valules. By default, we create models for replicating Figure 1 of Han et al. 2021.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\ntyping.Union[float, list[float], numpy.ndarray, dict]\n4\nbenefit: The size of the per round benefit of leading the AI development race, b>0\n\n\nc\ntyping.Union[float, list[float], numpy.ndarray, dict]\n1\ncost: The cost of implementing safety recommendations per round, c>0\n\n\ns\ntyping.Union[float, list[float], numpy.ndarray, dict]\n{‘start’: 1, ‘stop’: 5.1, ‘step’: 0.1}\nspeed: The speed advantage from choosing to ignore safety recommendations, s>1\n\n\np\ntyping.Union[float, list[float], numpy.ndarray, dict]\n{‘start’: 0, ‘stop’: 1.02, ‘step’: 0.02}\navoid_risk: The probability that unsafe firms avoid an AI disaster, p ∈ [0, 1]\n\n\nB\ntyping.Union[float, list[float], numpy.ndarray, dict]\n10000\nprize: The size of the prize from winning the AI development race, B>>b\n\n\nW\ntyping.Union[float, list[float], numpy.ndarray, dict]\n100\ntimeline: The anticipated timeline until the development race has a winner if everyone behaves safely, W ∈ [10, 10**6]\n\n\npfo\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\ndetection risk: The probability that firms who ignore safety precautions are found out, pfo ∈ [0, 1]\n\n\nα\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\nthe cost of rewarding/punishing a peer\n\n\nγ\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\nthe effect of a reward/punishment on a developer’s speed\n\n\nepsilon\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\ncommitment_cost: The cost of setting up and maintaining a voluntary commitment, ϵ > 0\n\n\nω\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\nnoise: Noise in arranging an agreement, with some probability they fail to succeed in making an agreement, ω ∈ [0, 1]\n\n\ncollective_risk\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\nThe likelihood that a disaster affects all actors\n\n\nβ\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0.01\nlearning_rate: the rate at which players imitate each other\n\n\nZ\nint\n100\npopulation_size: the number of players in the evolutionary game\n\n\nstrategy_set\nlist\n[‘AS’, ‘AU’]\nthe set of available strategies\n\n\nexclude_args\nlist\n[‘Z’, ‘strategy_set’]\na list of arguments that should be returned as they are\n\n\noverride\nbool\nFalse\nwhether to build the grid if it is very large\n\n\ndrop_args\nlist\n[‘override’, ‘exclude_args’, ‘drop_args’]\na list of arguments to drop from the final result\n\n\nReturns\ndict\n\nA dictionary containing items from ModelTypeDSAIR and ModelTypeEGT\n\n\n\n\nmodels = build_DSAIR()\n\n\n\nRun the model\nI use thread_macro to pipe the models I created earlier through each function that follows.\nI first build the payoff matrices for each model, and compute some analytical thresholds that will be useful in our discussion. Finally, I pass the result to markov_chain, a method for computing the distribution of strategies that will be present in the population in the long run.\n\nresults = thread_macro(models,\n                       payoffs_sr,\n                       payoffs_lr,\n                       threshold_society_prefers_safety_dsair,\n                       threshold_risk_dominant_safety_dsair,\n                       markov_chain,\n                      )\n\n\n\nProcess the results\nNow that we have collected some results, we need to process them so that we can display what we want to.\nThe general approach I follow is to flatten the results dictionary and convert it into a pandas dataframe. I have a convenience function called results_to_dataframe_egt for this purpose.\nIn process_dsair_data, we also compute the risk of an AI related disaster, \\(p_{risk} = 1 - p\\)\n\ndf = thread_macro(results,\n                  results_to_dataframe_egt,\n                  process_dsair_data)\n\n\n\n\n\n\n\n  \n    \n      \n      s\n      p\n      b\n      c\n      B\n      W\n      pfo\n      α\n      γ\n      epsilon\n      ω\n      collective_risk\n      β\n      threshold_society_prefers_safety\n      threshold_risk_dominant_safety\n      AS_frequency\n      AU_frequency\n      pr\n    \n  \n  \n    \n      0\n      1.0\n      0.00\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.019231\n      0.662338\n      1.000000e+00\n      8.508121e-12\n      1.00\n    \n    \n      1\n      1.0\n      0.02\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.019231\n      0.662338\n      1.000000e+00\n      3.927629e-11\n      0.98\n    \n    \n      2\n      1.0\n      0.04\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.019231\n      0.662338\n      1.000000e+00\n      1.813123e-10\n      0.96\n    \n    \n      3\n      1.0\n      0.06\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.019231\n      0.662338\n      1.000000e+00\n      8.369975e-10\n      0.94\n    \n    \n      4\n      1.0\n      0.08\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.019231\n      0.662338\n      1.000000e+00\n      3.863857e-09\n      0.92\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2086\n      5.0\n      0.92\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.797619\n      0.932921\n      0.000000e+00\n      1.000000e+00\n      0.08\n    \n    \n      2087\n      5.0\n      0.94\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.797619\n      0.932921\n      0.000000e+00\n      1.000000e+00\n      0.06\n    \n    \n      2088\n      5.0\n      0.96\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.797619\n      0.932921\n      0.000000e+00\n      1.000000e+00\n      0.04\n    \n    \n      2089\n      5.0\n      0.98\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.797619\n      0.932921\n      0.000000e+00\n      1.000000e+00\n      0.02\n    \n    \n      2090\n      5.0\n      1.00\n      4\n      1\n      10000\n      100\n      0\n      0\n      0\n      0\n      0\n      0\n      0.01\n      0.797619\n      0.932921\n      2.710175e-20\n      1.000000e+00\n      0.00\n    \n  \n\n2091 rows × 18 columns\n\n\n\n\n\nVisualise results and explain what we observe\nI am using the Matplotlib library to visualize our data.\nI first replicate Figure 1 from The Anh Han et al. 2021\nThe figure describes how the the frequency of Always Unsafe (AU) varies with both the speed advantage given to those who play AU, \\(s\\), and the risk that such firms cause an AI disaster, \\(p_{risk}\\). We have also plotted two lines, the lower line shows the boundary where society is indifferent between the two strategies. A greater risk or a slower speed advantage from this boundary implies society prefers players to play Always Safe (AS). The higher line shows the threshold for which AU is risk dominant over AS. For this baseline model, risk dominance implies that the strategy will be selected for by evolution (which is why the line follows the boundary where players switch from AU to AS). As with the lower line, any higher risk or lower speed implies that AS will instead by risk dominant over AU.\nThese lines therefore split the heatmap into 3 regions. (i) Society prefers AS and AS is selected by social learning. (ii) Society prefers AS but AU is selected by social learning (iii) Society prefers AU and AU is selected by social learning\nIn region (i) companies will be alligned with Society’s preference for safety. In region (iii), society is willing to accept the risks as they anticipate greater benefits from innovation. In region (ii), we see a dilemma where all players are choosing to play AU, even though society prefers them to play AS. We can refer to this region as the Dilemma zone.\n\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\nNote that this model is illustrative only: at best here, society refers to the collection of all firms.\nWe could instead explicitly model society’s preferences over safety and innovation, distinct from the companies. Such a model will still have the 3 regions we are currently discussing, though the negative externalities of an AI disaster will likely lead to a greater dillemma zone.\nAnother insight that such an extension would communicate is that companies may have incentive to work together to make sure their preferences are weighted more highly than the rest of society. It would be interesting to see whether we can observe this in pracitse, for examle in the European AI Act. The main challenge this task presents us is how to determine whether companies are working together to have their voices heard or whether they each already have strong enough incentives to uniltaterally influence policy.\n\n\nI have also plotted a cross-section of the above heatmap for speed advantage, \\(s=1.5\\). This plot shows how players are distributed between the 2 available strategies, \\(AU\\) and \\(AS\\). Here, the blue area represents the proportion of players who follow \\(AU\\) for each level of risk, whereas the red area tells us the proportion who play \\(AS\\). I also mark the 3 regions discussed above.\nWe will often make use of this cross-section plot in more complex models when we want to show the relative frequencies of more than 2 strategies."
  },
  {
    "objectID": "Analysis/analysis_dsair.html#dsair-with-punishment",
    "href": "Analysis/analysis_dsair.html#dsair-with-punishment",
    "title": "Analysis of the DSAIR model",
    "section": "DSAIR with punishment",
    "text": "DSAIR with punishment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above plots show what the addition of peer punishment can do for the dynamics of an AI Development race. In the scenario presented above, a sufficiently large punishment can completely eliminate unsafe behaviour, even when the cost is somewhat large for the sanctioner (plot 3).\nWe compare this to a scenario where the sanctioner is all talk (i.e. punishment is costless to both parties). Here, we see no difference in the distribution of strategies compared to the baseline model (plot 1)\nWhen the punishment hurts both parties equally, we see that it can curtail unsafe behavior, but to a far lesser extent. In such a scenario, it is more difficult for PS players to survive, so they provide less protection to AS players from AS players (plot 2).\nOne important takeaway which is apparent above is that punishments can be too punishing. Even when society collectively prefers that players are AU, punishment can prevent players from doing so.\nThis isn’t an obvious problem to overcome. Due to the Colignridge Dilemma, it will be very difficult for companies to determine what the risk of an AI disaster will be.\nWe have so far only looked at short timelines. On longer timelines, we would expect punishments to be less effective, but we also expect the dilemma region to be smaller, so an interesting question is whether we should expect punishments to be as overzealous in restricting behaviour. Could it even be that punishments are insufficient to remove the dilemma zone on longer timelines?\nThe answer is yes to both questions. See the plots below.\n\n\n\n\n\n\n\n\n\n\nBelow, I replicate 3 of the subfigures from Figure 5 of Han et al 2021, Mediating AI development races with incentives.\nAll these figures show th effects of punishment in a typical AI development race on a short timescale.\nThese figures show the effects of punishment as it varies in efficiency for a range of AI risk levels. With larger risk, players are more likely to play safe anyways, so punishments are rarely used. When risk is much lower, players prefer to be unsafe, although efficient punishments deter risk taking behaviour. Efficient punishments can mitigate unsafe behaviour when AI risk is in an intermediate stage, where it is not high enough to motivate firms to be safe, but is still high enough in principle to warrant their caution on society’s behalf.\nNote again that these numbers are illustrative. Depending on the size of a disaster, much lower levels of risk may demand caution from the international community.\n\n\n\n\n\n\n\n\n\n\n\nBelow, I replicate another figure from the paper. We display the frequency of Always Unsafe for a range of speed advantages and risk levels. The presence of a Peer Punishment strategy appears to mitigate the dilemma zone we noted earlier. Here, punishments are 75% effective in slowing down unsafe firms and apply the same slowdown to the sanctioner who levied the punishment, \\(s_γ=s_α=3s/4\\).\n\n\n\n\n\nThe above figure looks fairly similar to the paper. However, I intuitively expected the reduction in AU to be more significant. Try making this graph specifically in mathematica and see what comes up.\n\nPunishments on longer timelines\nI now extend the analysis of punishments to longer timelines. A longer timeline implies that any large benefits from winning the development race are likely to occur farther into the future, which means that they may be weighted less than before relative to the benefits of selling AI services in the current period (we capture this by looking at the average payoffs to firms over the duration of the race).\nIt is likely that increasing timelines has the same effect as reducing the prize from winning the race. This is in fact the case for the DSAIR model without punishments. Either change in the simpler model would ubiquitously reduce incentives to race unsafely.\nIn the case of punishments, the story is somewhat more complicated. On the one hand, it is now easier for punishments of different levels of impact to not only disincentivise but also stop the R&D activities of unsafe firms. On the other hand, it also means that firms who levy the punishments could more easily fail to survive the experience. If such firms are less able to survive, they are less likely to offer protection to safe firms from being exploited by unsafe firms.\nNevertheless, we should longer timescales to lead to a net increase in punishment efficiency, as safer firms who levy punishments are less motivated by the long-run prize in the first place.\nYet, a more nuanced question is whether this is what society wants? On longer timelines, society is likely to want to encourage more risk taking. We should anticipate that the improved efficacy of punishments should in some scenarios conflict with society’s preferences. This would be particularly troubling if this were to lead to backlash against the punishers, especially if society’s preferences failed to capture important negative externalities that could arise from risky technology races (e.g. the tail risk of a catastrophe caused by the misuse of autonomous weapons).\nHow can we represent a longer timeline?\nWe represent short timelines as taking \\(W=100\\) time periods. If we assume firms can act nearly every month, then this corresponds to a timeline of around 10 years.\nSurveys of AI experts suggest that those with longer timelines assume the development of AI systems which surprass humans in all domains to at least take a century, so a \\(W=1000\\) seems appropriate (see Katja et al. 2016, Carlier et al, Brundage et al.). However, note that there are timelines where the development of transformative AI systems place longer timelines at around 60 years instead, see Cotra 2020 (transformative AI systems are sufficient to cause radical economic change to a similar degree than the industrial revolution).\n\n\n\n\n\n\n\n\n\n\n\nAre the results above for a longer timeline what we anticipated?\nTaking each graph in order, starting with the figure for high AI risk, we see that unsafe behaviour is barely present (though it is perhaps notable that on a longer timeline, the slightly different shades indicate that firms are unsafe for a very small percentage of time, which vanishes with more efficient punishments).\nFor the figure with intermediate risk, the differences with shorter timelines are more apparent. Unsafe frequencies are much smaller and there is a larger region where no unsafe behaviour is present at all.\nThe final figure with low risk tells a similar story. The region where no unsafe behaviour is present is very large and the region where all companies are unsafe is smaller than on a shorter timeline.\nHow do we find the interaction effect of longer timelines with punishments. To isolate this interaction effect, we could take the difference of the two plots. If longer timelines had a uniform effect on unsafe behaviour, then the differences would be constant. Our results above already indicate that there is no such uniform effect.\nHowever, it may be of interest to know whether the effect of longer timelines is weaker for more efficient punishmens and stronger for less efficient ones. This effect is more difficult to determine, as the most efficient punishments already quell all unsafe behaviour. Though, on close inspection of the figures above, one can see that clearly the difference is greatest where punishments would otherwise have just failed to reduce unsafe behaviour, near the boundary where on shorter timelines for the last two plots we see a quick switch from punishments completely stopping unsafe behavior to failing to stop any unsafe behaviour at all. This is strong evidence that longer timelines intensify the effect of punishments on unsafe behaviour.\nTo answer whether this is what society wants (ignoring externalities), I now plot the unsafe frequencies for a range of AI risk levels and speed advantages, as well as the thresholds for when society prefers safety and when unsafe behaviour would usually dominate safe behaviour in the absense of punishment strategies. If you recall our earlier discussion, these thresholds split the figure into 3 regions, the middle region representing a dilemma zone where safety is collectively preffered but unsafe behaviour is what everyone learns to do.\nBefore we consider this figure, you may wish to remind yourself of the analogous figure for shorter timelines. We saw that punishments which were equivalent to a reduction of 75% the speed that unsafe firms could obtain were only somewhat effective in shrinking the dilemma zone. This was consistent with our earlier figures since equal and relatively large speed reductions for the sanctioner and punished would place us closer to the upper right corner of those figures which plotted the unsafe frequencies as we varied the sanctioner and punished costs.\nHowever, we know that longer timelines allow punishments to be considerably more effective, even in those areas. We should anticipate a greater shrinkage of the dilemma zone. Whether this is what society wants is less clear from the outset. It is plausible that unsafe behaviour diminishes so much that it eats into the region where risk taking is prefferred by society over safety.\n\n\n\n\n\nThe figure above is surprising. First, notice that the threshold AI risk level for society to prefer safety is higher than before as expected (the threshold for unsafe behaviour risk dominating safe behaviour should appear to be somewhat lower, but the difference in this case is barely apparent).\nMost dramatic is the reduciton in unsafe behaviour in the dilemma zone. Although the frequency is still above 50% for most of the dilemma zone, the overall effect is much higher than on shorter timelines. Note that we have also plotted the threshold for the Sanctioners to risk dominate the unsafe firms they punish. Even though this threshold is very close to the threshold before safe behaviour is risk dominant, we still see a dramatic reduction in unsafe behaviour before this. Even if sanctioners are not selected for by evolution, they can still catalyse safe behaviour.\nMy warnings of overegulation in this scenario are unwarranted. While there is a slight reduction in risk taking behaviour in the lower left corner where AI risk and the speed advantage are small, the effect is minimal.\nWhat is more interesting is the effect when risk is higher. It is notable that even when safe behaviour is risk dominant, that a small percentage of firms still choose to be unsafe. This happens because the average payoffs for safe firms when against other safe firms also falls with the longer timeline. Tied with our relatively weak learning rate, \\(beta=0.01\\), this leads to a less dramatic swtich from unsafe to safe behaviour. Importantly, this is not a consequence of the punishments themselves.\nOne other check I would like to do is to see if the figure would be similar if we had instead shrunk the prize directly by a factor of \\(10\\). In the simpler DSAIR model, this change would be equivalent a \\(10\\) times longer timeline, since players only considered the average prize over the duration of the timeline.\nHowever, our punishments ensure that players who are unsafe, players who are unsafe but who are punished, players who are safe but sanction unsafe players, and safe players would all finish the race at different times. This implies that the effect of punishments could be starkly different than if we had shrunk the prize directly. Where it not so, if extending the timeline had the same effect as a direct prize reduction, then we might have anticipated that punishments had a qualitatively similar effect on unsafe behaviour as on shorter timelines. This was not the case.\nLet’s plot the above figures again with smaller prize, \\(B\\), for comparison.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, I am surprised. The smaller prize did in fact have the same effects as a longer timeline. It seems that the asymmetric effects on when sanctioners and the punished finish their races is just not large enough to influence the race independently of the effects of longer timelines. This is likely because it has (i) a negligible effect on who wins the race, and (ii) a negligible effect on the product of the prize and the time taken to win the race.\nI should also double check whether the effect is largely driven by timelines rather than the punishments by setting the punishment effects to be \\(0\\).\n\n\n\n\n\nThe above figure confirms that most of the effect is from longer timelines independent of its interaction with punishments. However, it is noticeable that longer timelines dramatically boost safety when the speed advantage is low. It also boosts safety for higher speed values, though this may be less aparrent (try spotting the larger purple area at \\(s=5\\)).\nI should also see whether the above effect on punishments (so punishment costs are no longer \\(0\\)) is mainly driven by lower payoffs on average as a whole (by shrinking the learning rate, \\(\\beta\\) by a factor of \\(10\\)). A glance below suggests that this is the case (there are slight but negligible differences)\n\n\n\n\n\nAll in all, it appears that the effects of changing \\(B\\), \\(W\\), and \\(\\beta\\) are largely interchangeable, even in the presence of punishments.\nOne takeaway is that in this simple model, the impact of punishments (as with the rest of the DSAIR models we have considered so far) only considers the average prize over the duration of the timeline. This is a strong assumption which in principle we could devise a test for: do companies really act to increase the average prize they expect over the long run? Do they instead discount future possible gains and act to maximise the net present value of their activities.\nAdmittedly, the mitigation effects of punishments still appear less than ideal. The dilemma zones remains large.\nWe could consider a different punishment scheme than we considered in both the short and long timeline scenarios above, one where perhaps due to well-designed incentives, we permit more efficient punishments which are less costly to the sanctioner. In these cases, we are much more likely to reduce unsafe behaviour (perhaps too when risk taking would be collectively desired).\nLet’s consider naively that this time the speed reduction for sanctioners is only 25% of the speed that unsafe firms can obtain.\n\n\n\n\n\nThe analysis above is fairly comprehensive for our punishments model. A consideration of short term costs and benefits is unlikely to yield any further insights.\nA further examination of punishments might consider more complex industry interactions. It seems unlikely that there will only be two firms in the race. Could punishments be more effective if there are multiple firms, and one firm can punish an unsafe one to encourage a remaining safe firm to win? This seems like it has the potential to be a useful extension."
  },
  {
    "objectID": "Analysis/analysis_dsair.html#rewards",
    "href": "Analysis/analysis_dsair.html#rewards",
    "title": "Analysis of the DSAIR model",
    "section": "Rewards",
    "text": "Rewards"
  },
  {
    "objectID": "Analysis/analysis_dsair.html#voluntary-commitments",
    "href": "Analysis/analysis_dsair.html#voluntary-commitments",
    "title": "Analysis of the DSAIR model",
    "section": "Voluntary Commitments",
    "text": "Voluntary Commitments\nWe replicate a figure from the Voluntary Commitment paper below."
  },
  {
    "objectID": "Analysis/index.html",
    "href": "Analysis/index.html",
    "title": "Analyses",
    "section": "",
    "text": "Click through to any of these notebooks for analyses of EGT models.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nAnalysis of the DSAIR model\n\n\nA number of replications and extensions of the models provided in Han et al. (2020 , 2021, 2022 )\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods in Evolutionary Game Theory",
    "section": "",
    "text": "np.set_printoptions(suppress = True)"
  },
  {
    "objectID": "methods.html#evolutionary-dynamics-in-finite-populations",
    "href": "methods.html#evolutionary-dynamics-in-finite-populations",
    "title": "Methods in Evolutionary Game Theory",
    "section": "Evolutionary Dynamics in Finite Populations",
    "text": "Evolutionary Dynamics in Finite Populations\nWe examine a finite population of players using different strategies who engage in social learning.\nIn the limit of small mutations, most of the time everyone plays the same strategy. States in which everyone plays the same strategy are known as monomorphic states. Occassionally, mutant strategies can fixate in the population, resulting in everyone adopting the same new strategy. We can use Markov Chains to analyse the relative frequencies with which each strategy is played by the population.\nThe steps for computing the ergodic (i.e. long-run, stationary) strategy distribution is as follows:\n\nBuild a transition matrix between all monomorphic states\nFind the ergodic distribution for the markov chain defined using this transition matrix\n\n\nFermi social learning\n\nA Fermi social learning rule means that individuals make pairwise comparisons between their own strategy and and another strategy in the population that they may choose to copy.\n\n\nDerivation\nEach period of the evolutionary game involves individuals being randomly selected to play against one another individual.\nLetting \\(Z\\) denote the size of the population, and \\(π\\) denote the game’s payoff matrix, we can compute the fitness of a strategy, \\(B\\) for example, when \\(k\\) individuals are of type \\(B\\) as follows:\n\\[\\begin{equation}\nΠB_k = πBA \\frac{k-1}{Z - 1} + πBB \\frac{Z-k}{Z- 1}\n\\end{equation}\\]\nwhere \\(πBA\\) and \\(πBB\\) are the payoffs for playing \\(B\\) against type \\(A\\) or \\(B\\) respectively.\nThe Fermi social learning rule adopts strategy \\(B\\) selected from the population over their current strategy \\(A\\) with probability given by:\n\\[\\begin{equation}\nPr(adopt \\, B | k) = \\frac{1}{(1 + \\exp^{-\\beta (ΠB_k - ΠA_k)})}\n\\end{equation}\\]\nwhere \\(ΠB_k - ΠA_k\\) is the relative fitness of strategy \\(B\\) over \\(A\\) in a population with \\(k\\) individuals of type \\(B\\), the rest of type \\(A\\). Notice how the larger the relative fitness, the closer the denominator, and therefore the probability, is to \\(1\\).\nUsing the Fermi social learning rule above, we can write the probability of increasing the number of type \\(B\\) individuals as\n\\[\\begin{equation}\nT^+_B(k) = \\frac{Z-k}{Z} \\frac{k}{Z} Pr(adopt \\, B | k)\n\\end{equation}\\] Z as an individual of type \\(A\\) needs to randomly be chosen to compare their strategy against someone of type \\(B\\).\nand the probability of decreasing the number of type \\(B\\) individuals as\n\\[\\begin{equation}\nT^-_B(k) = \\frac{k}{Z} \\frac{Z-k}{Z} Pr(adopt \\, A | k)\n\\end{equation}\\]\nas an individual of type \\(B\\) needs to randomly be chosen to compare their strategy against someone of type \\(A\\).\nWe will often employ their ratio, which is:\n\\[\\begin{equation}\n\\frac{T^-_B(k)}{T^+_B(k)} = \\frac{Pr(adopt \\, A | k) }{Pr(adopt \\, B | k)} = \\frac{1 + \\exp^{-\\beta (ΠB_k - ΠA_k)}}{1 + \\exp^{-\\beta (ΠA_k - ΠB_k)}}\n\\end{equation}\\]\nNotice that \\(\\frac{1 + \\exp^x}{1 + \\exp^{-x}} = \\exp^{x}\\)\nSo, this ratio simplifies to \\(\\frac{T^-_B(k)}{T^+_B(k)} = \\exp^{-\\beta (ΠB_k - ΠA_k)}\\)\n\n\nDefinition\n\nsource\n\n\n\nfermi_learning\n\n fermi_learning (fitnessA:nptyping.ndarray.NDArray,\n                 fitnessB:nptyping.ndarray.NDArray,\n                 β:nptyping.ndarray.NDArray)\n\nCompute the likelihood that a player with strategy A adopts strategy B using the fermi function.\n\n\n\n\nType\nDetails\n\n\n\n\nfitnessA\nNDArray\nfitness of strategy A\n\n\nfitnessB\nNDArray\nfitness of strategy B\n\n\nβ\nNDArray\nlearning rate\n\n\nReturns\nNDArray\n\n\n\n\n\nExamples and Tests\nWhen each strategy has the same fitness, then the likelihood that a player adopts strategy \\(B\\) is 50%, no matter the value of \\(\\beta\\).\n\nx = fermi_learning(np.array([5]),\n                   np.array([5]),\n                   np.array([1]),)\nnptyping.assert_isinstance(x, nptyping.NDArray[nptyping.Shape[\"1\"], typing.Any])\nfastcore.test.test_eq(x, 0.5)\n\n\n\n\nFixation rate\n\nThe fixation rate for type B in a population of type A, \\(\\rho\\), is defined as the probability that the appearance of a mutant of type B leads to the entire population adopting type B instead of A, i.e. what is the likelihood that a mutant of type B invades population A.\n\n\nDerivation\nA derivation of the fixation rate defined below can be found in Nowak 2006 (reproduced below).\n\nConsider a one-dimensional stochastic process on a discrete state space, $ i {0, 1, , N}$ that represents the number of individuals in a population of \\(N\\) individuals who are of type \\(B\\), the rest are type \\(A\\).\nIn each stochastic event, the number of individuals of type \\(B\\) can at most increase or decrease by 1.\nFor a given number of individuals, \\(i\\), let \\(a_i\\), \\(b_i\\), and \\(1 - a_i - b_i\\) represent the chance of an increase, decrease, or no change in \\(i\\).\nThis stochastic process follows the transition matrix ,\\(P\\) (not to be confused with the transition matrices we discuss elsewhere!)\n\\[\\begin{equation}\nP \\, = \\, \\begin{pmatrix}\n1 & 0 & 0 & \\cdots & 0 & 0 & 0\\\\\nb_1 & (1 - a_1 - b_1) & a_1 & \\cdots & 0 & 0 & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & b_{n-1} & (1 - a_{n-1} - b_{n-1}) & a_{n-1}\\\\\n0 & 0 & 0 & \\cdots & 0 & 0 & 1\\\\\n\\end{pmatrix}\n\\end{equation}\\]\nDenote by \\(x_i\\) the probability of reaching state \\(N\\) when starting from \\(i\\).\nFrom transition matrix \\(P\\) above, we can see that \\(x_i\\) must satisfy:\n\\(x_0 = 0\\)\n\\(x_i = b_i x_{i-1} + (1 - a_i - b_i) x_i + a_i x_{i+1}\\)\n\\(x_N = 1\\)\nThe fixation rate for a mutant B in a population of type A is clearly \\(x_1\\)\nWe can solve for \\(x_i\\) by rewriting the above as \\(b_i x_i - b_i x_{i-1} = a_i x_{i+1} - a_i x_i\\).\nWe can denote \\(y_i = x_i - x_{i-1}\\) to simplify the above to \\(y_{i+1} = \\frac{b_i}{a_i} y_i\\)\nNotice that \\(\\sum_{i=1}^N{y_i} = x_N - x_0 = 1\\) and that \\(y_1 = x_1\\)\nWe can use the above to write \\[\\begin{equation}\nx_1 + {\\sum_{i=2}^N{y_i}} = x_1 (1 + {\\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}}}) = 1\n\\end{equation}\\]\nAnd so \\[\\begin{equation}\nx_1 = \\frac{1}{(1 + \\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}})}\n\\end{equation}\\]\nNote that \\(x_1\\) is the fixation rate for a mutant \\(B\\) in a population of type \\(A\\), often denoted as \\(\\rho\\).\nAlso note that \\(1 - x_{N-1}\\) is the fixation rate for a mutant \\(A\\) in a population of type \\(B\\). We could find expressions for all \\(x_i\\) if we note that \\(x_i = x_1 (1 + \\sum_{j=1}^{i-1}{\\prod_{k=1}^{j} \\frac{b_k}{a_k}})\\) (see Nowak 2006 for further details).\n\nWe can use our definitions above to determine when the fixation rate for a mutant \\(B\\) in a population of type \\(A\\) is greater than that for a mutant \\(A\\) in a population of type \\(B\\).\nThis condition requires that \\(x_1 > 1 - x_{N-1}\\), i.e. \\(\\frac{1}{(1 + \\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}})} > \\frac{\\prod_{j=1}^{N-1} \\frac{b_j}{a_j}}{(1 + \\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}})}\\).\nUsing the fermi social learning rule and the aforementioned simplifications, we can see that this condition holds true whenever \\(1 > \\exp^{-\\beta \\sum_{j=1}^{N-1}{\\Pi_B(j) - \\Pi_A(j)}}\\) which implies \\(\\sum_{j=1}^{N-1}{\\Pi_B(j)} > \\sum_{j=1}^{N-1}{\\Pi_A(j)}\\).\nLastly, we can make use of the equation \\(\\sum_{j=1}^{N-1}{j}=\\frac{(N-1) N}{2}\\) to simplify this condition to \\(\\pi_{BA} + \\pi_{BA} > \\pi_{AA} + \\pi_{AB}\\)\nThis is exactly the risk dominance condition implied by 2 by 2 payoff matrices. The risk dominance condition has been used in the literature to offer a reason to motivate selecting one monomorphic equilibria over another in such games. In such games there is a precise connection between risk dominance and the monomorphic equilibria selected for by social learning. This connection disappears in games with larger payoff matrices (which is why theorists tends to consider the concept of stochastic stability instead, perhaps using Young’s method (Young 2003)).\nEven in games with more than 2 players (or populations), we can make use of this condition to tell us in which direction the fixation rate is stronger between two strategies. At times, this is enough to gain an intuition for the gradient of selection present in polymorphic states where multiple strategies coexist in one or more populations.\n\n\nDefinition\n\nsource\n\n\n\nfixation_rate\n\n fixation_rate (Tplus:list[nptyping.base_meta_classes.NDArray],\n                Tneg:list[nptyping.base_meta_classes.NDArray])\n\nCalculate the likelihood that a mutant invades the population.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nTplus\nlist\nA list of NDarrays, one array (of size n_models) for each possible number of mutants in the population; the probability of gaining one mutant\n\n\nTneg\nlist\nA list of NDarrays, one array (of size n_models) for each possible number of mutants in the population; the probability of losing one mutant\n\n\nReturns\nNDArray\nFixation rates for the given strategy in each model\n\n\n\n\nExamples and Tests\nWhen the chance of gaining a mutant always equals the chance of losing a mutant, then the fixation rate will be \\(\\frac{1}{Z}\\)\nNote that because we have to sample the population for a mutant and the player of the type being invaded, the chance of gaining or losing a mutant can be no greater than \\(\\frac{k}{Z} \\frac{Z-k}{Z}\\)\n\nZ = 2 # With Z=2, we only need to evaluate Tplus and Tneg for when k=1\nTplus_example = [np.array([1/8])]\nTneg_example =  [np.array([1/8])]\n\n\nfixation_rate_result = fixation_rate(Tplus_example, Tneg_example)\n\n\nfastcore.test.test_eq(fixation_rate_result, np.array([0.5]))\n\nWhen the chance of gaining a mutant is half the chance of losing a mutant, then the fixation rate will be\n\\[\\begin{equation}\n\\rho = \\frac{1}{(1 + \\sum_{j=1}^{Z-1}{2^j})}\n\\end{equation}\\]\nWhen \\(Z=2\\), we have \\(\\rho = \\frac{1}{3}\\)\n\nZ = 2 # With Z=2, we only need to evaluate Tplus and Tneg for when k=1\nTplus_example = [np.array([0.1])]\nTneg_example =  [np.array([0.2])]\n\n\nfixation_rate_result = fixation_rate(Tplus_example, Tneg_example)\n\n\nfastcore.test.test_eq(fixation_rate_result, np.array([1/3]))\n\nWe could instead consider an example where we have a mutant Defector (D) who appears in a population of Cooperators (C) playing a standard Prisoner’s Dilemma.\nWe will consider an example of such a scenario where chance of gaining/losing a D player be given by \\(\\frac{1}{1 + e^{\\pm \\beta \\frac{Z+1}{Z-1}}}\\).\nThe fixation rate will be given by the following expression:\n\\[\\begin{equation}\n\\rho = \\frac{1}{1 + \\sum_{j=1}^{Z-1}{(\\frac{1 + e^{- \\beta \\frac{Z+1}{Z-1}}}{1 + e^{\\beta \\frac{Z+1}{Z-1}}})^j}}\n\\end{equation}\\]\nFor this example, we will let \\(\\beta=1\\) and \\(Z=10\\), so \\(\\beta \\frac{Z+1}{Z-1} = \\frac{11}{9}\\).\n\nβ = 1\nZ = 10\nρ_CD = 1 / (1 + sum((1 + np.exp(- β * (Z + 1) / (Z-1)))**j \n                    / (1 + np.exp(β * (Z + 1) / (Z-1)))**j\n                    for j in range(1, Z)))\nTplus_example = [np.array([1 / (1 + np.exp(- β * (Z + 1) / (Z-1)))])\n                 for _ in range(Z-1)]\nTneg_example =  [np.array([1 / (1 + np.exp(β * (Z + 1) / (Z-1)))])\n                 for _ in range(Z-1)]\n\n\nfastcore.test.is_close(fixation_rate(Tplus_example, Tneg_example), ρ_CD)\n\nTrue\n\n\nFinally, it is useful to know how the fixation rate behaves when any elements of Tplus are zero (as the fixation rate divides by those elements). Even though the Fermi learning rule we use theoretically gives a number between 0 and 1 exclusive, in practise the number may underflow to a 0 if low enough. This will cause unexpected behaviour if we allow it in our alogorithm for computing the transition matrix.\nWe can avoid this issue by using a slightly altered method for calculating the fixation rate, taking advantage of our choice to use the fermi_learning rule.\nIn the above fixation rate calculations we used the fermi_learning function to calculate the probability of a player with strategy \\(D\\) adopting strategy \\(C\\) (and likewise for the probability of a player with \\(C\\) adopting \\(D\\)). Their ratio takes the form, \\(\\frac{1 + e^x}{1 + e^{-x}}\\). It is not too hard to verify that \\(\\frac{1 + e^x}{1 + e^{-x}} = e^x\\).\nMoreover, we can avoid taking the product of the ratios at all, since the product of exponentials (with the same base) is just the exponential of the sum of their exponents.\nBy using the above substitution and algebraic manipulation, we can substantially mitigate the numerical stability issues. For this reason, we will not use fermi_learning nor fixation_rate in our algorithm at all (although in most cases we would expect these methods to yield the same answers). Instead, we will use fixation_rate_stable.\n\nsource\n\n\n\nfixation_rate_stable\n\n fixation_rate_stable (ΠA:list, ΠB:list, β:gh_pages_example.types.Array1D)\n\nCalculate the likelihood that a mutant B invades population A using a numerically stable method.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nΠA\nlist\nAverage payoffs for the strategy A they consider adopting for each number of mutants following A\n\n\nΠB\nlist\nAverage payoffs for the strategy B that the player currently follows for each number of mutants following A\n\n\nβ\nArray1D\nlearning rate\n\n\n\nWe can see in the examples which follow that both methods usually give the same answers.\nTo match an earlier example where Tplus and Tneg were both equal to \\(\\frac{1}{8}\\) (as \\(Z=2\\) we only need to consider one value for each when \\(k=1\\)), we let \\(\\beta=1\\) and recall that $T^+_B(k) = Pr(adopt , B | k) = $\nWe can then say that \\(ΠA - ΠB = \\log{(\\frac{1}{\\frac{4}{8}} - 1)} = \\log{\\frac{4}{4}} = \\log{4} - \\log{4}\\)\nNotice that to achieve netural drift, the payoffs have to be equal.\n\nZ = 2\nβ = 1\nΠA = [np.array([np.log(4)])]\nΠB = [np.array([np.log(4)])]\nresult = fixation_rate_stable(ΠA, ΠB, β)\nfastcore.test.test_close(result, 0.5)\n\nWe can also consider an example from a payoff matrix I’ve run into in practise.\n\npayoffs = np.array([[51, 0.6, 51],\n                    [114.3, 57.75, 39.38],\n                    [51, 0.99798, 51]])\n\nWe are interested in the fixation rate of a mutant B in a population of A\nStrategy A is the strategy represented by row 3\nStrategy B is the strategy represented by row 2\n\nZ = 100\nβ = 1\n\nWe need only the average payoffs for the stable calculation.\n\nΠA = [k/(Z-1) * payoffs[2,1] + (Z-k-1)/(Z-1) * payoffs[2,2]\n      for k in range(1, Z)]\nΠB = [(k-1)/(Z-1) * payoffs[1,1] + (Z-k)/(Z-1) * payoffs[1,2]\n      for k in range(1, Z)]\n\nresult_stable = fixation_rate_stable(ΠA, ΠB, β)\n\nWe also need the adoption rates for the unstable calculation\n\nTneg = [fermi_learning(ΠB[k-1], ΠA[k-1], β)\n                     for k in range(1, Z)]\nTplus = [fermi_learning(ΠA[k-1], ΠB[k-1], β)\n         for k in range(1, Z)]\n\n# Naiive and unstable calculation\nresult_unstable = fixation_rate(Tplus, Tneg)\n\n\nfastcore.test.test_close(result_stable, 0)\nfastcore.test.test_close(result_unstable, 0)\n\n\n\nBuild transition matrix\nRecall that step 1 of finding the solution to the Evolutionary Game dynamics is to build a transition matrix between all monomorphic states.\nThe transition matrix captures the probability that if the population of the Evolutionary Game transitions to another state. We read an entry of the transition matrix as saying the probability of transitioning from the row state to column state.\n\nsource\n\n\nModelTypeEGT\n\n ModelTypeEGT (Z:int, strategy_set:list[str],\n               β:gh_pages_example.types.Array1D,\n               payoffs:gh_pages_example.types.Array3D,\n               transition_matrix:gh_pages_example.types.Array3D=None,\n               ergodic:gh_pages_example.types.Array2D=None)\n\nThis is the schema for an Evolutionary Game Theory model.\nNote: This schema is not enforced and is here purely for documentation purposes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nZ\nint\n\nthe size of the population\n\n\nstrategy_set\nlist\n\nthe set of strategies in the model\n\n\nβ\nArray1D\n\nthe learning rate\n\n\npayoffs\nArray3D\n\nthe payoffs of the game\n\n\ntransition_matrix\nArray3D\nNone\nthe model’s transition matrix\n\n\nergodic\nArray2D\nNone\nergodic distribution of the model’s markov chain\n\n\n\n\nsource\n\n\nbuild_transition_matrix\n\n build_transition_matrix (models:dict)\n\nBuild a transition matrix between all monomorphic states using the fermi social learning rule.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary that contains the parameters in ModelTypeEGT\n\n\n\n\nExamples and Tests\nConsider the following two examples.\nExample 1\nLet all payoffs be equal in the game’s payoff matrix. All expected payoffs will be equal too.\nSo, Fermi learning will say that each individual has a 50% chance of adopting the behaviour of the one they observe.\nWe therefore have an equal chance during each epoch of gaining or losing an individual of the given type, in this example we denote the type as \\(s \\in \\{A, B\\}\\), although this probability depends on population size \\(Z\\) and the current number of individuals of that type, \\(k\\), \\(T^+_s(k) = T^-_s(k) = \\frac{Z-k}{Z} \\frac{k}{Z} \\frac{1}{2}\\).\nRecall that we calculate the fixation rate, \\(\\rho\\) as follows: \\[\\begin{equation}\n\\rho = \\frac{1}{1 + \\sum_{j=1}^{N-1}{\\prod_{k=1}^{j} \\frac{b_k}{a_k}}}\n\\end{equation}\\] where \\(N=Z\\), \\(b_k = T^-_s(k)\\) and \\(a_k = T^+_s(k)\\)\nIn this example, for each strategy \\(s\\), \\(T^-_s(k) = T^+_s(k), \\, \\forall k\\), so \\(\\rho = \\frac{1}{Z}\\).\nWe only have \\(2\\) strategies, and \\(Z=10\\), so the final transition matrix will look like\n\\[\\begin{equation}\nM \\, = \\, \\begin{pmatrix}\n1 - \\frac{\\rho}{2 - 1} & \\frac{\\rho}{2 - 1} &\\\\\n\\frac{\\rho}{2 - 1} & 1 - \\frac{\\rho}{2 - 1}\\\\\n\\end{pmatrix}\n= \\begin{pmatrix}\n0.9 & 0.1 &\\\\\n0.1 & 0.9\\\\\n\\end{pmatrix}\n\\end{equation}\\]\nNote that the above example describes neutral drift, the idea that even if there is no advantage to be gained from any particular strategy, social learning can still result in the spread of that behaviour. Neutral drift also occurs if we set the Fermi learning rate \\(\\beta = 0\\), no matter what payoff matrix describes the game.\n\npayoffs = np.array([[[2, 2],\n                     [2, 2]]\n                   ])\nZ = 10\nβ = 1\nmodels = {\"payoffs\": payoffs,\n          \"Z\": Z,\n          \"β\": β,\n          \"strategy_set\": [\"A\", \"B\"],\n         }\nresult = build_transition_matrix(models)\n\n\nfastcore.test.test_close(result['transition_matrix'],\n                         np.array([[0.9, 0.1],\n                                   [0.1, 0.9]]))\n\nExample 2\nLet the payoff matrix be akin to a Prisoner’s Dilemma with two strategies, \\(C\\) or \\(D\\) (Cooperate or Defect respectively):\n\\[\\begin{pmatrix}\n2 & 0\\\\\n3 & 1\\\\\n\\end{pmatrix}\\]\nAgain, for this simple example, the relative average success of strategy \\(C\\) is independent of the number of \\(C\\) players, \\(k\\). This is rarely the case in practise but permits a more legible example.\n\\(C\\)’s relative success over \\(D\\) will be \\(\\frac{2 (k-1)}{Z-1} - \\frac{3 k + (Z - k - 1)}{Z-1} = - \\frac{Z + 1}{Z-1}\\).\nFermi learning means the probability of a \\(D\\) player adopting what they see \\(C\\) do is:\n\\[\\begin{equation}\n\\frac{1}{1 + e^{- \\beta (\\Pi_C(k) - \\Pi_D(k))}} = \\frac{1}{1 + e^{\\beta \\frac{Z + 1}{Z-1}}}\n\\end{equation}\\]\nThe fixation rate for mutant \\(C\\) in a population of \\(D\\) players, \\(\\rho_{DC}\\), can be computed as\n\\[\\begin{equation}\n\\rho_{DC} = \\frac{1}{1 + \\sum_{j=1}^{Z-1}{(\\frac{1 + e^{\\beta \\frac{Z + 1}{Z-1}}}{1 + e^{-\\beta \\frac{Z + 1}{Z-1}}})^j}}\n\\end{equation}\\]\nSimilarly, the fixation rate for mutant \\(D\\) in a population of \\(C\\) players, \\(\\rho_{CD}\\), can be computed as\n\\[\\begin{equation}\n\\rho_{CD} = \\frac{1}{1 + \\sum_{j=1}^{Z-1}{(\\frac{1 + e^{-\\beta \\frac{Z + 1}{Z-1}}}{1 + e^{\\beta \\frac{Z + 1}{Z-1}}})^j}}\n\\end{equation}\\]\nFor \\(Z=10\\) and \\(\\beta = 1\\), the above yields the following transition matrix,\n\\[\\begin{equation}\nM \\, = \\, \\begin{pmatrix}\n1 - \\frac{\\rho_{CD}}{2 - 1} & \\frac{\\rho_{CD}}{2 - 1} &\\\\\n\\frac{\\rho_{DC}}{2 - 1} & 1 - \\frac{\\rho_{DC}}{2 - 1}\\\\\n\\end{pmatrix}\n\\approx \\begin{pmatrix}\n0.295 & 0.705 &\\\\\n0.000 & 1.000\\\\\n\\end{pmatrix}\n\\end{equation}\\]\nNote how in the above fixation rate calculations how we used the fermi_learning function to calculate the probability of a player with strategy \\(D\\) adopting strategy \\(C\\) (and likewise for the probability of a player with \\(C\\) adopting \\(D\\)). This function has special properties which aid us in calculating the fixation rate.\nNotice how the ratio of the two adoption rates takes the form, \\(\\frac{1 + e^x}{1 + e^{-x}}\\). It is not too hard to verify that \\(\\frac{1 + e^x}{1 + e^{-x}} = e^x\\).\nWe utilities this property to considerably improve the numerical stability of our algorithm for building a transition matrix. For this reason, we do not use fermi_learning in our algorithm at all.\nWe can similarly note that \\(\\frac{1}{1 + e^{-x}} = 1 - \\frac{1}{1 + e^{x}}\\), i.e. the two adoption rates are complementary probabilities.\n\npayoffs = np.array([[[2, 0],\n                     [3, 1]],\n                   ])\nZ = 10\nβ = 1\nmodels = {\"payoffs\": payoffs,\n          \"Z\": Z,\n          \"β\": β,\n          \"strategy_set\": [\"C\", \"D\"],\n         }\nresult = build_transition_matrix(models)\n\n\nρ_CD = 1 / (1 + sum((1 + np.exp(- β * (Z + 1) / (Z-1)))**j \n                    / (1 + np.exp(β * (Z + 1) / (Z-1)))**j\n                    for j in range(1, Z)))\nρ_DC = 1 / (1 + sum((1 + np.exp(β * (Z + 1) / (Z-1)))**j\n                    / (1 + np.exp(- β * (Z + 1) / (Z-1)))**j \n                    for j in range(1, Z)))\n\n\nρ_CD_alt = 1 / (1 + sum(np.exp(- j * β * (Z + 1) / (Z-1))\n                        for j in range(1, Z)))\nρ_DC_alt = 1 / (1 + sum(np.exp(j * β * (Z + 1) / (Z-1))\n                        for j in range(1, Z)))\n\n\nfastcore.test.test_close(ρ_CD, ρ_CD_alt)\nfastcore.test.test_close(ρ_DC, ρ_DC_alt)\n\n\nfastcore.test.test_close(result['transition_matrix'],\n                         np.array([[1- ρ_CD, ρ_CD],\n                                   [ρ_DC, 1 - ρ_DC]]))\n\n\n\nExample 3\nHere is an additional example for the 3 by 3 matrix we discussed when testing other functions.\nThis time, we make sure we get the correct probabilities for each transition.\n\npayoffs = np.array([[[51, 0.6, 51],\n                     [114.3, 57.75, 39.38],\n                     [51, 0.99798, 51]],\n                   ])\n\n\nexpected = np.array([[[0.495, 0.5, 0.005],\n                     [0, 1, 0],\n                     [0.005, 0, 0.995]],\n                   ])\n\n\nZ = 100\nβ = 1\nmodels = {\"payoffs\": payoffs,\n          \"Z\": Z,\n          \"β\": β,\n          \"strategy_set\": [\"AS\", \"AU\", \"PS\"],\n         }\nresult = build_transition_matrix(models)\nfastcore.test.test_close(result['transition_matrix'], expected)\n\n\n\n\nFind ergodic strategy distribution\nStep 2 is to find the ergodic distribution for the Evolutionary Game using the transition matrix we constructed in step 1.\nLet \\(M\\) denote the transition matrix, and \\(\\omega_t\\) be the column vector describing the proportions with which each strategy is played in the population.\nWe can describe the evolution of this system with \\(\\omega_{t+1} = M^T \\omega_t\\), i.e. the proportion of players that use a given strategy in the next round will be equal to the sum of the proportions of players for each strategy who adopted that strategy in the current round. Equivalently, we can also consider \\(\\omega_t\\) as describing the probabilities that the system at time t is in each of the monomorphic states.\nAs each of the monomporphic states described in the transition matrix is reachable from any other with some probability and since the transition probabilities only depend on the current state, what we have is a markov chain which is irreducible.\nThe ergodicity theorem guarantees that such irreducible and aperiodic markov chains have an ergodic distribution that the system converges to, no matter where it starts. An ergodic distribution (also called a stationary distribution), \\(\\omega^*\\) satisfies \\(\\omega^* = M^T \\omega^*\\) [1] [2] [3].\nOur ergodic distribution, \\(\\omega^*\\), is therefore defined as the normalised right-hand eigenvector with eigenvalue 1 of the transposed transition matrix, \\(M^T\\) (or equivalently, if we defined \\(\\omega\\) as a row vector instead, \\(\\omega^*\\) would be the left-hand eigenvector with eigenvalue 1 of transition matrix, \\(M\\); numerical computing packages usually return the right-hand eigenvectors more directly, which is why I used the other formalism).\nWe use standard linear algebra methods from the numpy package to find this eigenvector. These numerical methods will usually not return an eigenvector which is normalised to sum to 1, so we must normalise the eigenvector we are given. See their documentation to learn more about these numerical methods.\n\nExamples and Tests\nLet our transition matrix, \\(M\\) be\n\\[\\begin{equation}\nM = \\begin{pmatrix}\n\\frac{3}{4} & \\frac{1}{4} \\\\\n\\frac{1}{4} & \\frac{3}{4} \\\\\n\\end{pmatrix}\n\\end{equation}\\]\nNote that \\(M^T\\) is a stochastic matrix because each column of the transposed matrix would sum to \\(1\\) (in general the rows of the transposed matrix are unlikely to sum to 1, but choosing an example like the above makes it easy to compute the eigenvectors).\nIt’s not too hard to verify that the characteristic polynomial of \\(M^T\\) can be factored into \\((\\lambda - 1)(\\lambda - \\frac{1}{2})\\), so we have two eigenvalues, \\(1\\) and \\(\\frac{1}{2}\\).\nIt’s not too hard to verify that column vector \\([1, 1]\\) is the eigenvector of \\(M^T\\) with eigenvalue \\(1\\) .\nNow that we know the weights placed on each strategy, we can compute the strategy distribution by normalising our eigenvector.\nThe ergodic distribution i \\(\\omega^* = [\\frac{1}{2}, \\frac{1}{2}]\\).\n\nM = np.array([[[3/4, 1/4],\n               [1/4, 3/4]],\n             ])\nmodels = {\"transition_matrix\": M}\nresult = find_ergodic_distribution(models)\n\n\nfastcore.test.test_eq(result['ergodic'],\n                      np.array([[1/2, 1/2]]))\n\n\n# #| hide\n# # Here is some code which illustrates how one could use sympy to find the relevant eigenvectors \n# # using symbolic methods (but please note that even sympy must resort to numerical methods if\n# # the matrices are bigger than 5 by 5 in size, due to the fundamental lack of exact solutions to \n# # polynomial equations with order greater than 5)\n# import sympy\n# for m in M:\n#     # Sympy needs integers or expressions to work\n#     # Integers is usually safer\n#     m = np.array(1000 * m, dtype=int)\n#     M_symbolic = sympy.Matrix(m)\n#     for result in M_symbolic.eigenvects():\n#         lamda, multiplicity, evs = result\n        \n#         # print(\"lambda: \" , lamda,\n#         #           \"multiplicity: \", multiplicity,\n#         #           \"eigenvectors: \", evs)\n\nHere is another quick illustrative example.\nLet our transition matrix, \\(M\\) be\n\\[\\begin{equation}\nM = \\begin{pmatrix}\n\\frac{3}{4} & \\frac{1}{4} \\\\\n\\frac{3}{4} & \\frac{1}{4} \\\\\n\\end{pmatrix}\n\\end{equation}\\]\n\\(M^T\\) is a stochastic matrix. It is easy to verify that \\([\\frac{3}{4}, \\frac{1}{4}]\\) is the normalised eigenvector with eigenvalue 1.\n\nM = np.array([[[3/4, 1/4],\n               [3/4, 1/4]],\n             ])\nmodels = {\"transition_matrix\": M}\nresult = find_ergodic_distribution(models)\n\n\nfastcore.test.test_eq(result['ergodic'],\n                      np.array([[3/4, 1/4]]))\n\n\nsource\n\n\n\nfind_ergodic_distribution\n\n find_ergodic_distribution (models:dict)\n\nFind the ergodic distribution of a markov chain with the given transition matrix.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary that contains the parameters in ModelTypeEGT\n\n\n\n\n\nRun full markov chain algorithm\nFinally, here is a helper function to both build the transition matrix for the model and find its ergodic distribution.\n\nsource\n\n\nmarkov_chain\n\n markov_chain (models:dict)\n\nFind the ergodic distribution of the evolutionary game given by each model in models.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary that contains the parameters in ModelTypeEGT"
  },
  {
    "objectID": "methods.html#multiple-populations",
    "href": "methods.html#multiple-populations",
    "title": "Methods in Evolutionary Game Theory",
    "section": "Multiple Populations",
    "text": "Multiple Populations\n\nBuilding blocks\nI will now describe the building blocks for an algorithm for building the transition matrix for a evolutionary game with multiple populations.\nWhen we have multiple populations, it is very easy to have many possible transitions. For this reason, it is important that we have a way to programatically handle them.\nMy algorithm allows one to build these transition matrices using only the following information (in addition to other parameters needed in the single population case): - (i) Payoffs - (ii) Each sector’s strategies - (iii) Allowed sectors for each player - (iv) Sampling rule (optional) - (v) profile filters (optional)\n\nin particular allows for a great deal of flexibility in setting up a game. It is possible to capture games where the number of players may vary or where the interaction contains players who may be sampled from one of several poulations. This flexibility will allow us to study a wide range of models from the literature. By default, (iv) and (v) are already quite general, though one can provide their own sampling rules and profile filters should they want even greater flexibility.\n\n\nPayoffs\npayoffs is a nested dictionary which is 2 levels deep. On the first level, each key is a string of the form “…n3-n2-n1”. n1 is an integer which encodes the strategy that player 1 uses in the interaction. n2 and n3 are the same. We have as many integers as there are possible players in the interaction. This key therefore captures the strategy profile employed by the players.\nAt the second level, we have keys for each player, “P1”, “P2”, “P3”, …\nEach value is a 1D numpy array containing the player’s payoffs (relevant to the specified strategy profile) for each model we are solving the game for.\nThere are number of important hints to follow when writing your payoffs for use in my multiple populations algorithm. - Players are allowed to be from any sector. Note that I assume strategies are coded for each sector in order. So, Sector 1’s strategies are coded from 1 to num_s1_strategies, Sector 2’s are coded from num_s1_strategies + 1 to num_s1_strategies + num_s2 strategies, and so on. In this way the strategy code tells us which sector the strategy is from and which strategy they follow. - I use a 0 to indicate that the player is not involved in the current interaction. Intuitively you may prefer to think of the player as doing nothing and being from Sector 0. This allows us to flexibly allow the possibility of an uncertain number of players in each interaction. - In some games, it is possible that there are a large number of possible strategy profiles (e.g. a game with 3 sectors, 3 strategies, and up to 4 players in each interaction would have 10**4 possible strategy profiles). However, very few of those strategy profiles will be relevant to building the transition matrix, especially if the order of the players does not matter, and if some players must belong to certain sectors. The number of strategy profiles will often be much smaller than the number of parameter combinations (what I refer to as models) we wish to solve for.\n\n\nSector strategies\nA dictionary with keys for each sector and values as lists of integers which encode the sector’s strategies. Recall that sector 1’s strategies start from 1, sector 2’s strategies start from num_s1_strategies + 1 and so on.\nWe will use the sector strategies to generate the set of recurrent states, the states that the system visits in the limit of rare mutations. Such states have every member of a sector (also often reffered to as a population) using the same strategy, i.e. they are monomorphic. These are the states we need to the build the transition matrix for.\n\n\nAllowed_sectors\nallowed_sectors specifies which sectors each player can be from and therefore specifies all possible interactions in the game.\nIt is a dictionary where the keys are players, e.g. “P1” and the values are lists of sectors, e.g. [“S1”, “S2”]. This tells the algorithm which interactions are possible in the game.\nTo specify that the player may not be present in an interaction, you can specify “S0” in the list.\nNote: sectors are perhaps more commonly reffered to as populations or subpopulations in the literature. For the sake of brevity, I use sectors instead when naming variables.\n\n\nSampling rule\nA sampling rule tells us the likelihood that a strategy profile will be selected given the current state of the system and the number of mutants (of the specified type). The sampling also needs to know which player represents the agent who is comparing the two strategies under consideration as this player does not need to be sampled.\nHowever, if this agent could have been one of several players (they would be playing the same strategy in the strategy profile), then the sampling rule should also multiply the likelihood by the probability that the agent would have been chosen as the current player.\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\nTests for sample_profile\nThe sample_profile default method is very general. It will calculate the likelihood of the profile by assuming that all allowed sectors for each player are uniformly sampled from (unless sector weights are provided) and consider the likelihood of sampling a mutant from the sector which has a mutant.\n\n\nTest 1\nI conduct a number of tests below on a simple game where every player in an interaction is fixed to a particular sector. In such games, the likelihood of each profile being chosen is is always 1, no matter how many mutants there are, as no individual plays against a player from the same sector.\n\n\nTest 2\nThere are several errors a user may encounter if they give invalid values.\n\n\nTest 3\nNow, consider a more complicated example where we have 2 players that belong to the same sector and one player who is fixed to another sector. Now the profile likelihoods depend on the number of mutants.\n\n\nTest 4\nI also test it for a game where all 3 players can be from the same two sectors.\n\n\n\nProfile filters\nProfile filters work by filtering a list of profiles for only those profiles which meet the required conditions. Se create_profiles to read up how we create a list of profiles, and profile_filter for different profile filters.\nWe can also use the apply_profile_filters function which by default filters our profiles so that we only keep those profiles which are relevant to the transition and are consistent with the given allowed_sectors.\n\n\nCreate all recurrent states\n\nsource\n\n\ncreate_recurrent_states\n\n create_recurrent_states (models)\n\nCreate all recurrent-states for the set of models.\nHere is a quick test for create_recurrent_states\n\nresult = create_recurrent_states({\"sector_strategies\": {\"S1\": [1, 2],\n                                                        \"S2\": [3, 4],\n                                                        \"S3\": [5, 6]}})\nexpected = ['5-3-1',\n            '5-3-2',\n            '5-4-1',\n            '5-4-2',\n            '6-3-1',\n            '6-3-2',\n            '6-4-1',\n            '6-4-2']\nfastcore.test.test_eq(result, expected)\n\n\n\nCheck transition is valid\nHere is a method for checking that a given transition is valid.\n\nsource\n\n\nvalid_transition\n\n valid_transition (ind1:str, ind2:str)\n\nCheck if the transition from ind1->ind2 is valid i.e. that only one population undergoes a change in strategy.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nind1\nstr\nThe index of the current state, expressed in the form “{strategy_code}-{strategy_code}-{strategy_code}”\n\n\nind2\nstr\nThe index of the next state, expressed in the same form as ind1\n\n\nReturns\nbool\nTrue if the transition is valid, false otherwise\n\n\n\nTests for valid_transition\n\n\nA multimethod for computing the likelihoods of different strategy profiles\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\nTests for compute_profile_dist\n\nTest 1\nI consider a model with 2 sectors, who each have 2 strategies and 10 members, and play a game with 2 players who can each be from either sector.\nWe therefore specify the sectors sizes, Z, the sector_strategies and the allowed_sectors\nprofiles_filtered will be all profiles in such a game relevant to a transition between Sector 1 playing their first strategy and Sector 1 playing their second strategy. The members of Sector 2 play their first strategy.\nI encode these recurrent states as “3-1” and “3-2” respectively.\nI also mark “S1” as the affected_sector, as well as the mutant_strategy and current_strategy.\nprofiles_filtered then includes: all possible “x-y” where x,y in {1,2,3}, since there will be no “S2” players playing strategy 4.\nWe also have to specify the chosen_strategy to indicate whether we are interested in the profile likelihoods from the perspective of a mutant player or a current player.\nThe tests consider different values of n_mutants.\n\n\nTest 2\nI next consider the same model but this time each interaction has 5 players.\nIn such cases, it is desirable to use the multiplayer-symmetric method for computing the likelihood of the relevant profiles.\nAs before, we specify the sectors sizes, Z, the sector_strategies and the allowed_sectors\nprofiles_filtered will be all profiles in such a game relevant to a transition between Sector 1 playing their first strategy and Sector 1 playing their second strategy. The members of Sector 2 play their first strategy.\nI encode these recurrent states as “3-1” and “3-2” respectively.\nI also mark “S1” as the affected_sector, as well as the mutant_strategy and current_strategy.\nprofiles_filtered then includes: all possible “x-y” where x,y in {1,2,3}, since there will be no “S2” players playing strategy 4.\nWhile we need all of these profiles for this new method, the profiles we only compute the likelihoods for a subset. There is only one relevant profile per unique strategy count. The profile chosen is the first such profile when iterating through profiles_filtered (care must be taken to ensure that the payoffs are computed in a similar way - we will have a method to ensure this).\nWe also have to specify the chosen_strategy to indicate whether we are interested in the profile likelihoods from the perspective of a mutant player or a current player.\nThe tests verifies that the result of this method is the same as if aggregated the likelihoods using the default method.\n\n\n\n\nA multimethod for computing each strategy’s success\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\nTests for compute_success\n\nTest 1\nFor simplicity, I assume payoffs are always 1. Naturally, the success of each strategy will also be 1.\nI consider a model with 2 sectors, who each have 2 strategies and 10 members, and play a game with 2 players who can each be from either sector.\nWe therefore specify the sectors sizes, Z, the sector_strategies and the allowed_sectors\nprofiles_filtered will be all profiles in such a game relevant to a transition between Sector 1 playing their first strategy and Sector 1 playing their second strategy. The members of Sector 2 play their first strategy.\nI encode these recurrent states as “3-1” and “3-2” respectively.\nI also mark “S1” as the affected_sector, as well as the mutant_strategy and current_strategy.\nprofiles_filtered then includes: all possible “x-y” where x,y in {1,2,3}, since there will be no “S2” players playing strategy 4.\nWe also have to specify the chosen_strategy to indicate whether we are interested in the profile likelihoods from the perspective of a mutant player or a current player.\nThe tests consider different values of n_mutants.\n\n\nTest 2\nI also consider more general payoffs where many entries are unique. However, the order of strategies does not matter.\nWhen the order does not matter, it is very easy to calulate an expression for what the success of each strategy should be for the 2 player interactions considered in this example.\n\n\nTest 3\nWe can find a similar expression for when the order does matter.\n\n\n\n\nInfer number of models\n\nsource\n\n\nvals\n\n vals (d:dict)\n\nReturn the values of a dictionary.\n\nsource\n\n\ninfer_n_models\n\n infer_n_models (models)\n\nInfer the number of models from the model payoffs.\n\nTests for infer_n_models\n\nprofiles_filtered = ['1-1', '1-2', '1-3',\n                     '2-1', '2-2', '2-3',\n                     '3-1', '3-2', '3-3']\npayoffs = {}\nfor profile in profiles_filtered:\n  payoffs[profile] = {}\n  for player in allowed_sectors.keys():\n    payoffs[profile][player] = np.repeat(np.random.beta(1, 1), 5)\ninfer_n_models({'payoffs': payoffs})\n\n5\n\n\n\n\n\nAn algorithm for building the transition matrix\nWe now have the methods we need for building the transition matrix for a game with an arbitrary number of sectors and various interactions between those sectors.\nThe algorithm goes as follows:\nFor each possible transition - Check if the transition is valid - If self-transition, assign the value 1 - If not, skip - Filter profiles down to only those which are relevant - Compute average payoffs using the payoffs and those profiles - Compute the fixation rate - Compute transition probabilities\n\n\n\nsource\n\nbuild_transition_matrix\n\n build_transition_matrix (models:dict)\n\nBuild a transition matrix between all monomorphic states when there are multiple populations.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary that contains the parameters in ModelTypeEGTMultiple\n\n\n\n\n\n\n\nTests and examples for build_transition_matrix for multiple populations\n\nTest 1\nI first consider a random payoff matrix and check that the resulting transition matrices have rows which sum to 1.\nAs we are working with multiple populations, our models variable needs to declare this with the dispatch-type key.\n\n\nTest 2\nHere is an example of how to build a transition matrix when we have 2 sectors, playing 2 strategies each, and engage in 2 player interactions with players being from either sector.\nIn the limit of small mutation rates, the system spends almost all its time in states where each population plays one strategy. Moreover, only a mutant for one population has the opportunity to fixate in that population. This means we only need to consider transitions where the strategy played by one population has changed. Transitions where both populations would have to change strategy occur with probability 0.\nIn an earlier test, we showed that one could compute the success of each strategy in a pairwise contest analytically, even in the presence of other sectors with fixed populations.\nOnce we have the successes for each n_mutant value, we can rely on our well-tested fixation_rate function to help compute our expected transition probabilities.\n\n\nTest 3\nThis example comes from a paper by Encarnacao et al. 2016.\nThey have a 3 sector model and report fixation probabilities for a particular scenario. Can we replicate it?\n\nsource\n\n\n\n\npayoffs_encanacao_2016\n\n payoffs_encanacao_2016 (models)\n\n\nmodels['transition_matrix'] * 50 * 2\n\narray([[[98.58484409,  0.43868208,  0.30980716,  0.        ,\n          0.66666667,  0.        ,  0.        ,  0.        ],\n        [ 0.96080835, 96.4805528 ,  0.        ,  0.73408493,\n          0.        ,  1.82455392,  0.        ,  0.        ],\n        [ 1.22164065,  0.        , 97.32807231,  0.96080835,\n          0.        ,  0.        ,  0.48947868,  0.        ],\n        [ 0.        ,  0.60342679,  0.43868208, 97.44630095,\n          0.        ,  0.        ,  0.        ,  1.51159018],\n        [ 0.66666667,  0.        ,  0.        ,  0.        ,\n         97.63844005,  0.96080835,  0.73408493,  0.        ],\n        [ 0.        ,  0.14274942,  0.        ,  0.        ,\n          0.43868208, 98.00649791,  0.        ,  1.41207058],\n        [ 0.        ,  0.        ,  0.88124961,  0.        ,\n          0.60342679,  0.        , 96.79725701,  1.71806658],\n        [ 0.        ,  0.        ,  0.        ,  0.21292021,\n          0.        ,  0.24196967,  0.1635232 , 99.38158692]]])\n\n\nThe ergodic distribution of states looks remarkably similar to the results reported in the paper.\nUnfortunately, there are no exact results available to compare against. However, the direction of change and relative sizes of the bars in the bar chat are very similar. The sum of the last 4 very closely matches the bar for total public cooperators which sits level to 0.8 on the chart."
  },
  {
    "objectID": "analytical_conditions.html",
    "href": "analytical_conditions.html",
    "title": "Analytical conditions",
    "section": "",
    "text": "An equilibrium is collectively preferred if the sum of the payoffs to all players is greater than the alternative.\n\\(\\Pi\\)(Always Safe, Always Safe) > \\(\\Pi\\)(Always Unsafe, Always Unsafe)\nUsing the payoffs defined in gh_pages_example.payoffs we can rewrite the corresponding equality in terms of \\((1 - p)\\).\n\\[\\begin{equation}\np_{risk} = (1 - p) = 1 - \\frac{\\pi_{11} + \\frac{B}{2 W}}{ \\pi_{22} + \\frac{s B}{2 W}}\n\\end{equation}\\]\nWe will often use the boundary of this inequality in our analysis.\n\nsource\n\n\n\n\n threshold_society_prefers_safety_dsair (models)\n\nThe threhsold value of AI risk for which society prefers firms to be Always Safe in the DSAIR model.\n\n\n\nAn equilibrium is selected by evolution in 2-by-2 matrix games if it is risk dominant. Risk dominance requires that:\n\\(\\Pi\\)(Always Safe, Always Safe) + \\(\\Pi\\)(Always Safe, Always Unsafe) > \\(\\Pi\\)(Always Unsafe, Always Safe) + \\(\\Pi\\)(Always Unsafe, Always Unsafe)\nUsing the payoffs defined in gh_pages_example.payoffs we can rewrite the corresponding equality in terms of \\((1 - p)\\).\n\\[\\begin{equation}\np_{risk} = (1 - p) = 1 - \\frac{\\pi_{11} + \\pi_{12} + \\frac{B}{2 W}}{\\pi_{21} + \\pi_{22} + \\frac{3 s B}{2 W}}\n\\end{equation}\\]\nWe will often use this boundary in our analysis.\n\nsource\n\n\n\n\n threshold_risk_dominant_safety_dsair (models)\n\nThe threshold value of AI risk for which Always Safe is risk dominant against Always Unsafe in the DSAIR model.\n\n\n\n\nsource\n\n\n\n\n threshold_ps_r_d_au (models)\n\nThe analytic threshold for when strategy PS risk dominates AU in a DSAIR model with peer punishment, expressed in terms of p_risk\nHistogram shows that the simplification is great for the majority of the time, though on some parts of the state space it fails as an approximation."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Armstrong, S., Bostrom, N., & Shulman, C. (2016). Racing to the precipice: A model of artificial intelligence development. AI & SOCIETY, 31(2), 201–206. https://doi.org/10.1007/s00146-015-0590-y\nAskell, A., Brundage, M., & Hadfield, G. (2019). The Role of Cooperation in Responsible AI Development. ArXiv:1907.04534 [Cs]. http://arxiv.org/abs/1907.04534\nBostrom, N. (2014). Superintelligence: Paths, dangers, strategies (First edition). Oxford University Press.\nCave, S., & Ó hÉigeartaigh, S. S. (2018). An AI Race for Strategic Advantage: Rhetoric and Risks. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 36–40. https://doi.org/10.1145/3278721.3278780\nCimpeanu, T., Santos, F. C., Pereira, L. M., Lenaerts, T., & Han, T. A. (2020). AI Development Race Can Be Mediated on Heterogeneous Networks. ArXiv:2012.15234 [Cs]. http://arxiv.org/abs/2012.15234\nDafoe, A. (2018). AI Governance: A Research Agenda. The University of Oxford. https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf\nGruetzemacher, R., Dorner, F., Bernaola-Alvarez, N., Giattino, C., & Manheim, D. (2020). Forecasting AI Progress: A Research Agenda. ArXiv:2008.01848 [Cs]. http://arxiv.org/abs/2008.01848\nGruetzemacher, R., & Whittlestone, J. (2020). The Transformative Potential of Artificial Intelligence. ArXiv:1912.00747 [Cs]. http://arxiv.org/abs/1912.00747\nHan, T. A., Lenaerts, T., Santos, F. C., & Pereira, L. M. (2022). Voluntary safety commitments provide an escape from over-regulation in AI development. Technology in Society, 68, 101843. https://doi.org/10.1016/j.techsoc.2021.101843\nHan, T. A., Pereira, L. M., & Lenaerts, T. (2017). Evolution of commitment and level of participation in public goods games. Autonomous Agents and Multi-Agent Systems, 31(3), 561–583. https://doi.org/10.1007/s10458-016-9338-4\nHan, T. A., Pereira, L. M., & Lenaerts, T. (2019). Modelling and Influencing the AI Bidding War: A Research Agenda. Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 5–11. https://doi.org/10.1145/3306618.3314265\nHan, T. A., Pereira, L. M., Lenaerts, T., & Santos, F. C. (2021). Mediating artificial intelligence developments through negative and positive incentives. PLOS ONE, 16(1). https://doi.org/10.1371/journal.pone.0244592\nHan, T. A., Pereira, L. M., Santos, F. C., & Lenaerts, T. (2020). To Regulate or Not: A Social Dynamics Analysis of an Idealised AI Race. Journal of Artificial Intelligence Research, 69, 881–921. https://doi.org/10.1613/jair.1.12225\nInstitute, F. of L. (2017). AI Principles. https://futureoflife.org/ai-principles/\nKrakovna, V., Uesato, J., Mikulik, V., Rahtz, M., Everitt, T., Kumar, R., Kenton, Z., Leike, J., & Legg, S. (2020, April 21). Specification gaming: The flip side of AI ingenuity. Deepmind. https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity\nLaCroix, T., & Mohseni, A. (2021). The Tragedy of the AI Commons. ArXiv:2006.05203 [Cs]. http://arxiv.org/abs/2006.05203\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Keeling, J., Gimeno, F., Lago, A. D., Hubert, T., Choy, P., & de, C. (n.d.). Competition-Level Code Generation with AlphaCode. 73.\nNaudé, W., & Dimitri, N. (2020). The race for an artificial general intelligence: Implications for public policy. AI & SOCIETY, 35(2), 367–379. https://doi.org/10.1007/s00146-019-00887-x\nOgbo, N. B., Elragig, A., & Han, T. A. (2021). Evolution of coordination in pairwise and multi-player interactions via prior commitments. Adaptive Behavior, 1059712321993166. https://doi.org/10.1177/1059712321993166\nO’Keefe, C., Cihon, P., Garfinkel, B., Flynn, C., Leung, J., & Dafoe, A. (2020). The Windfall Clause: Distributing the Benefits of AI for the Common Good. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 327–331. https://doi.org/10.1145/3375627.3375842\nRussell, S. J. (2019). Human compatible: Artificial intelligence and the problem of control. Penguin Publishing Group.\nTaddeo, M., & Floridi, L. (2018). Regulate artificial intelligence to avert cyber arms race. Nature, 556(7701), 296–298. https://doi.org/10.1038/d41586-018-04602-6\nTzachor, A., Whittlestone, J., Sundaram, L., & hÉigeartaigh, S. Ó. (2020). Artificial intelligence in a crisis needs ethics with urgency. Nature Machine Intelligence, 2(7), 365–366. https://doi.org/10.1038/s42256-020-0195-0\nWhittlestone, J., & Clark, J. (2021). Why and How Governments Should Monitor AI Development. ArXiv:2108.12427 [Cs]. http://arxiv.org/abs/2108.12427\nWhittlestone, J., Nyrup, R., Alexandrova, A., & Cave, S. (2019). The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions. Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 195–200. https://doi.org/10.1145/3306618.3314289\nYeung, D. W. K., & Petrosyan, L. A. (2016). Subgame Consistent Cooperation (Vol. 47). Springer Singapore. https://doi.org/10.1007/978-981-10-1545-8"
  },
  {
    "objectID": "Payoffs/payoffs1.html",
    "href": "Payoffs/payoffs1.html",
    "title": "Payoff Matrices (part 1)",
    "section": "",
    "text": "np.set_printoptions(suppress=True) # don't use scientific notation"
  },
  {
    "objectID": "Payoffs/payoffs1.html#dsair-model-paramaters",
    "href": "Payoffs/payoffs1.html#dsair-model-paramaters",
    "title": "Payoff Matrices (part 1)",
    "section": "DSAIR Model Paramaters",
    "text": "DSAIR Model Paramaters\n\n\n\n\n\n\n\n\n\n\nkeyword\nvalue type\nrange\noptional\ndescription\n\n\n\n\nb\nNDArray\nb > 0\n\nThe size of the per round benefit of leading the AI development race\n\n\nc\nNDArray\nc > 0\n\nThe cost of implementing safety recommendations per round\n\n\ns\nNDArray\ns > 1\n\nThe speed advantage from choosing to ignore safety recommendations\n\n\np\nNDArray\n[0, 1]\n\nThe probability that unsafe firms avoid an AI disaster\n\n\nB\nNDArray\nB >> b\n\nThe size of the prize from winning the AI development race\n\n\nW\nNDArray\n\\[[10, 10^6]\\]\n\nThe anticipated timeline until the development race has a winner if everyone behaves safely\n\n\npfo\nNDArray\n[0, 1]\nYes\nThe probability that firms who ignore safety precautions are found out\n\n\nepsilon\nNDArray\nϵ > 0\nYes\nThe cost of setting up a voluntary commitment\n\n\nω\nNDArray\n[0, 1]\nYes\nNoise in arranging an agreement, with some probability they fail to succeed in making an agreement\n\n\n\n\nsource\n\nModelTypeDSAIR\n\n ModelTypeDSAIR (b:gh_pages_example.types.Array1D,\n                 c:gh_pages_example.types.Array1D,\n                 s:gh_pages_example.types.Array1D,\n                 p:gh_pages_example.types.Array1D,\n                 B:gh_pages_example.types.Array1D,\n                 W:gh_pages_example.types.Array1D,\n                 pfo:gh_pages_example.types.Array1D=None,\n                 α:gh_pages_example.types.Array1D=None,\n                 γ:gh_pages_example.types.Array1D=None,\n                 epsilon:gh_pages_example.types.Array1D=None,\n                 ω:gh_pages_example.types.Array1D=None,\n                 collective_risk:gh_pages_example.types.Array1D=None)\n\nThis is the schema for the inputs to a DSAIR model.\nNote: This schema is not enforced and is here purely for documentation purposes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\nArray1D\n\nbenefit: The size of the per round benefit of leading the AI development race, b>0\n\n\nc\nArray1D\n\ncost: The cost of implementing safety recommendations per round, c>0\n\n\ns\nArray1D\n\nspeed: The speed advantage from choosing to ignore safety recommendations, s>1\n\n\np\nArray1D\n\navoid_risk: The probability that unsafe firms avoid an AI disaster, p ∈ [0, 1]\n\n\nB\nArray1D\n\nprize: The size of the prize from winning the AI development race, B>>b\n\n\nW\nArray1D\n\ntimeline: The anticipated timeline until the development race has a winner if everyone behaves safely, W ∈ [10, 10**6]\n\n\npfo\nArray1D\nNone\ndetection risk: The probability that firms who ignore safety precautions are found out, pfo ∈ [0, 1]\n\n\nα\nArray1D\nNone\nthe cost of rewarding/punishing a peer\n\n\nγ\nArray1D\nNone\nthe effect of a reward/punishment on a developer’s speed\n\n\nepsilon\nArray1D\nNone\ncommitment_cost: The cost of setting up and maintaining a voluntary commitment, ϵ > 0\n\n\nω\nArray1D\nNone\nnoise: Noise in arranging an agreement, with some probability they fail to succeed in making an agreement, ω ∈ [0, 1]\n\n\ncollective_risk\nArray1D\nNone\nThe likelihood that a disaster affects all actors\n\n\n\n\nsource\n\n\nArray1D\n\n Array1D (ModelVector:nptyping.base_meta_classes.NDArray)\n\nAn alias for a 1D numpy array.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nModelVector\nNDArray\nA 1D numpy array suitable for stacks of scalar parameter values\n\n\n\n\nsource\n\n\nbuild_DSAIR\n\n build_DSAIR (b:Union[float,list[float],numpy.ndarray,dict]=4,\n              c:Union[float,list[float],numpy.ndarray,dict]=1,\n              s:Union[float,list[float],numpy.ndarray,dict]={'start': 1,\n              'stop': 5.1, 'step': 0.1},\n              p:Union[float,list[float],numpy.ndarray,dict]={'start': 0,\n              'stop': 1.02, 'step': 0.02},\n              B:Union[float,list[float],numpy.ndarray,dict]=10000,\n              W:Union[float,list[float],numpy.ndarray,dict]=100,\n              pfo:Union[float,list[float],numpy.ndarray,dict]=0,\n              α:Union[float,list[float],numpy.ndarray,dict]=0,\n              γ:Union[float,list[float],numpy.ndarray,dict]=0,\n              epsilon:Union[float,list[float],numpy.ndarray,dict]=0,\n              ω:Union[float,list[float],numpy.ndarray,dict]=0, collective_\n              risk:Union[float,list[float],numpy.ndarray,dict]=0,\n              β:Union[float,list[float],numpy.ndarray,dict]=0.01,\n              Z:int=100, strategy_set:list[str]=['AS', 'AU'],\n              exclude_args:list[str]=['Z', 'strategy_set'],\n              override:bool=False, drop_args:list[str]=['override',\n              'exclude_args', 'drop_args'])\n\nInitialise baseline DSAIR models for all combinations of the provided parameter valules. By default, we create models for replicating Figure 1 of Han et al. 2021.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\ntyping.Union[float, list[float], numpy.ndarray, dict]\n4\nbenefit: The size of the per round benefit of leading the AI development race, b>0\n\n\nc\ntyping.Union[float, list[float], numpy.ndarray, dict]\n1\ncost: The cost of implementing safety recommendations per round, c>0\n\n\ns\ntyping.Union[float, list[float], numpy.ndarray, dict]\n{‘start’: 1, ‘stop’: 5.1, ‘step’: 0.1}\nspeed: The speed advantage from choosing to ignore safety recommendations, s>1\n\n\np\ntyping.Union[float, list[float], numpy.ndarray, dict]\n{‘start’: 0, ‘stop’: 1.02, ‘step’: 0.02}\navoid_risk: The probability that unsafe firms avoid an AI disaster, p ∈ [0, 1]\n\n\nB\ntyping.Union[float, list[float], numpy.ndarray, dict]\n10000\nprize: The size of the prize from winning the AI development race, B>>b\n\n\nW\ntyping.Union[float, list[float], numpy.ndarray, dict]\n100\ntimeline: The anticipated timeline until the development race has a winner if everyone behaves safely, W ∈ [10, 10**6]\n\n\npfo\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\ndetection risk: The probability that firms who ignore safety precautions are found out, pfo ∈ [0, 1]\n\n\nα\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\nthe cost of rewarding/punishing a peer\n\n\nγ\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\nthe effect of a reward/punishment on a developer’s speed\n\n\nepsilon\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\ncommitment_cost: The cost of setting up and maintaining a voluntary commitment, ϵ > 0\n\n\nω\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\nnoise: Noise in arranging an agreement, with some probability they fail to succeed in making an agreement, ω ∈ [0, 1]\n\n\ncollective_risk\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0\nThe likelihood that a disaster affects all actors\n\n\nβ\ntyping.Union[float, list[float], numpy.ndarray, dict]\n0.01\nlearning_rate: the rate at which players imitate each other\n\n\nZ\nint\n100\npopulation_size: the number of players in the evolutionary game\n\n\nstrategy_set\nlist\n[‘AS’, ‘AU’]\nthe set of available strategies\n\n\nexclude_args\nlist\n[‘Z’, ‘strategy_set’]\na list of arguments that should be returned as they are\n\n\noverride\nbool\nFalse\nwhether to build the grid if it is very large\n\n\ndrop_args\nlist\n[‘override’, ‘exclude_args’, ‘drop_args’]\na list of arguments to drop from the final result\n\n\nReturns\ndict\n\nA dictionary containing items from ModelTypeDSAIR and ModelTypeEGT"
  },
  {
    "objectID": "Payoffs/payoffs1.html#dsair-payoff-matrix-short-run",
    "href": "Payoffs/payoffs1.html#dsair-payoff-matrix-short-run",
    "title": "Payoff Matrices (part 1)",
    "section": "DSAIR Payoff Matrix (Short Run)",
    "text": "DSAIR Payoff Matrix (Short Run)\n\n\n\nStrategy\nSafe\nUnsafe\n\n\n\n\nSafe\n\\[\\frac{b}{2} - c\\]\n\\[\\frac{b}{s+1} - c\\]\n\n\nUnsafe\n\\[b \\frac{s}{s+1}\\]\n\\[\\frac{b}{2} \\]\n\n\n\n\nsource\n\npayoffs_sr\n\n payoffs_sr (models:dict)\n\nThe short run payoffs for the DSAIR game.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary containing the items in ModelTypeDSAIR\n\n\nReturns\ndict\nThe models dictionary with added payoff matrix payoffs_sr"
  },
  {
    "objectID": "Payoffs/payoffs1.html#dsair-payoff-matrix-short-run-with-probability-of-being-found-out",
    "href": "Payoffs/payoffs1.html#dsair-payoff-matrix-short-run-with-probability-of-being-found-out",
    "title": "Payoff Matrices (part 1)",
    "section": "DSAIR Payoff Matrix (Short Run) with probability of being found out",
    "text": "DSAIR Payoff Matrix (Short Run) with probability of being found out\n\n\n\n\n\n\n\n\nStrategy\nSafe\nUnsafe\n\n\n\n\nSafe\n\\[\\frac{b}{2} - c\\]\n\\[(1 - p_{fo}) \\frac{b}{s+1} + p_{fo} b - c\\]\n\n\nUnsafe\n\\[ (1 - p_{fo}) b \\frac{s}{s+1}\\]\n\\[(1 - p_{fo}^2) \\frac{b}{2} \\]\n\n\n\n\nsource\n\npayoffs_sr_pfo_extension\n\n payoffs_sr_pfo_extension (models)\n\nThe short run payoffs for the DSAIR game with a chance of unsafe behaviour being spotted."
  },
  {
    "objectID": "Payoffs/payoffs1.html#dsair-payoff-matrix-long-run",
    "href": "Payoffs/payoffs1.html#dsair-payoff-matrix-long-run",
    "title": "Payoff Matrices (part 1)",
    "section": "DSAIR Payoff Matrix (Long Run)",
    "text": "DSAIR Payoff Matrix (Long Run)\nDenote \\(\\pi\\) as one of the short run payoff matrices discussed above with rows and columns indexed by letters A, B, …\n\n\n\n\n\n\n\n\nStrategy\nAlways Safe\nAlways Unsafe\n\n\n\n\nAlways Safe\n\\[πAA + \\frac{B}{2W}\\]\n\\[πAB\\]\n\n\nAlways Unsafe\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\n\n\nNote: In a model where we suffer a collective risk of an AI disaster if the winner is unsafe, payoffs for firms who play safe when facing an unsafe firm are also multiplied by \\(p\\).\n\nsource\n\npayoffs_lr\n\n payoffs_lr (models:dict)\n\nThe long run average payoffs for the DSAIR game.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary containing the items in ModelTypeDSAIR\n\n\nReturns\ndict\nThe models dictionary with added payoff matrix payoffs"
  },
  {
    "objectID": "Payoffs/payoffs1.html#dsair-payoff-matrix-with-punishments-long-run",
    "href": "Payoffs/payoffs1.html#dsair-payoff-matrix-with-punishments-long-run",
    "title": "Payoff Matrices (part 1)",
    "section": "DSAIR Payoff Matrix with punishments (Long Run)",
    "text": "DSAIR Payoff Matrix with punishments (Long Run)\nDenote \\(\\pi\\) as one of the short run payoff matrices discussed above with rows and columns indexed by letters A, B, …\nAlways Safe and Always Unsafe play as they usually do.\nPunish Unsafe always plays Safe. However, they will pay a cost to punish their co-player if the co-player plays Unsafe.\n\n\n\n\n\n\n\n\n\nStrategy\nAlways Safe\nAlways Unsafe\nPunish Unsafe\n\n\n\n\nAlways Safe\n\\[πAA + \\frac{B}{2W}\\]\n\\[πAB\\]\n\\[πAA + \\frac{B}{2W}\\]\n\n\nAlways Unsafe\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\npunished_payoff\n\n\nPunish Unsafe\n\\[πAA + \\frac{B}{2W}\\]\nsanctioner_payoff\n\\[πAA + \\frac{B}{2W}\\]\n\n\n\n\nsource\n\npunished_and_sanctioned_payoffs\n\n punished_and_sanctioned_payoffs (models:dict)\n\nCompute the payoffs for the punished and sanctioner players in a DSAIR model with peer punishment.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary containing the items in ModelTypeDSAIR\n\n\nReturns\ndict\nThe models dictionary with added payoff matrix payoffs:\n\n\n\nBelow, I test that we produce expected results for the punished and sanctioned payoffs.\n\nmodels = build_DSAIR(b=4,\n                     c=1,\n                     p=0.25,\n                     s=1.5,\n                     B=10**4,\n                     W=10**2,\n                     pfo=0,\n                     α=np.array([0]),\n                     γ=np.array([0]),\n                     β=0.01,\n                     Z=100,\n                     strategy_set=[\"AS\", \"AU\", \"PS\"],\n                     collective_risk=0)\n\nresults = thread_macro(models,\n                       payoffs_sr,\n                       punished_and_sanctioned_payoffs)\n\nexpected_result = (1/4 * 3 / 200 * (12/5 + 10**4 + 197/3 * 12/5))\ntest_eq(results['punished_payoff'], expected_result)\n\n\nmodels = build_DSAIR(b=4,\n                     c=1,\n                     p=0.25,\n                     s=1.5,\n                     B=10**4,\n                     W=10**2,\n                     pfo=0,\n                     α= np.arange(0, 3, 0.1),\n                     γ= np.arange(0, 3, 0.1),\n                     β=0.01,\n                     Z=100,\n                     strategy_set=[\"AS\", \"AU\", \"PS\"],\n                     collective_risk=0)\n\nresults = thread_macro(models,\n                       payoffs_sr,\n                       punished_and_sanctioned_payoffs)\n\n\ndef expected_fn1(α, γ):\n    p_punish = np.where((3/2 - γ) * (100 - 1) > (1 - α) * (100 - 3/2),\n                        1/4,\n                        1)\n    origin_speed = np.where((3/2 - γ) * (100 - 1) > (1 - α) * (100 - 3/2),\n                         3/2, \n                         1)\n    win_speed = np.where((3/2 - γ) * (100 - 1) > (1 - α) * (100 - 3/2),\n                         (3/2 - γ), \n                         (1 - α))\n    Bp = np.where((3/2 - γ) * (100 - 1) > (1 - α) * (100 - 3/2),\n                  10**4,\n                  np.where((3/2 - γ) * (100 - 1) == (1 - α) * (100 - 3/2),\n                           10**4 / 2,\n                           0))\n    sum_of_speeds = np.maximum(1e-20, (3/2 - γ) + (1 - α))\n    b_p = np.where((3/2 > γ) & (1 > α),\n                   4 * (3/2 - γ) / sum_of_speeds,\n                   np.where((3/2 > γ),\n                            4,\n                            0))\n    R_inv = (np.maximum(0, win_speed) \n                          / (100 - origin_speed + np.maximum(0, win_speed)))\n    punished_payoff = (p_punish * R_inv * (12/5 + Bp)\n                       + p_punish * b_p\n                       - p_punish * b_p * R_inv\n                      )\n    return punished_payoff\n\n\ntest_close(results['punished_payoff'][:, 0, 0],\n           expected_fn1(results['α'], results['γ']))\n\n\ndef expected_fn2(α, γ):\n    origin_speed = np.where((3/2 - γ) * (100 - 1) > (1 - α) * (100 - 3/2),\n                         3/2, \n                         1)\n    win_speed = np.where((3/2 - γ) * (100 - 1) > (1 - α) * (100 - 3/2),\n                         (3/2 - γ), \n                         (1 - α))\n    Bs = np.where((3/2 - γ) * (100 - 1) < (1 - α) * (100 - 3/2),\n                  10**4,\n                  np.where((3/2 - γ) * (100 - 1) == (1 - α) * (100 - 3/2),\n                           10**4 / 2,\n                           0))\n    sum_of_speeds = np.maximum(1e-20, (3/2 - γ) + (1 - α))\n    b_s = np.where((3/2 > γ) & (1 > α),\n                   4 * (1 - α) / sum_of_speeds,\n                   np.where((1 > α),\n                            4,\n                            0))\n    R_inv = (np.maximum(0, win_speed) \n             / (100 - origin_speed + np.maximum(0, win_speed)))\n    punished_payoff = (R_inv * (3/5 + Bs)\n                       + (b_s - 1)\n                       - (b_s - 1) * R_inv\n                      )\n    return punished_payoff\n\n\ntest_close(results['sanctioner_payoff'][:, 0, 0],\n           expected_fn2(results['α'], results['γ']))\n\n\nsource\n\n\npayoffs_lr_peer_punishment\n\n payoffs_lr_peer_punishment (models:dict)\n\nThe long run average payoffs for the DSAIR game with peer punishment.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary containing the items in ModelTypeDSAIR\n\n\nReturns\ndict\nThe models dictionary with added payoff matrix payoffs:\n\n\n\n\n\nExpressions for the sanctioner and punished payoffs\nFor convenience we denote a number of new variables to simplify the expressions for the sanctioner and punished payoffs.\n\\[\\begin{equation}\n\\text{sanctioner payoff} = \\frac{1}{R} (\\pi AB + B_s + (R-1) (b_s - c))\\\\\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{punished payoff} = \\frac{p_{punish}}{R} (πBA + B_p + (R-1) b_p)\\\\\n\\end{equation}\\]\nNote: In a model where we suffer a collective risk of an AI disaster if the winner is unsafe, payoffs for firms who play safe when facing an unsafe firm are also multiplied by \\(p_{punish}\\).\nWe can read the above payoffs as telling us the average payoffs over the R rounds of the race for each firm, assuming the punishment is levied at the end of the first round and the remaining \\(R - 1\\) rounds are played with the punishment in effect.\nNote that \\(s_{\\beta}\\) denotes the new speed of the firm who is punished and \\(s_{\\alpha}\\) as the speed of the firm who levies the punishment.\nBelow we denote the four possible outcomes (ignoring disaster) of a race between a sanctioner and a punished firm:\n\\[\\begin{equation}\n\\text{punished wins} = (s_{\\beta} > 0) \\, \\& \\, (\\frac{W-s}{s_{\\beta}} <= \\frac{W-1}{s_{\\alpha}})\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{sanctioner wins} = (s_{\\alpha} > 0) \\, \\& \\, (\\frac{W-1}{s_{\\alpha}} <= \\frac{W-s}{s_{\\beta}})\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{draw} = (s_{\\beta} > 0) \\, \\& \\, (\\frac{W-s}{s_{\\beta}} = \\frac{W-1}{s_{\\alpha}})\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{no winner} = (s_{\\beta} <= 0)  \\, \\& \\,  (s_{\\alpha} <= 0)\n\\end{equation}\\]\nWe can use the above expressions to define the following variables:\n\\(p_{punish}\\) is the probability of avoiding an AI disaster if a punishment is levied and depends on who wins the race.\n\\[\\begin{equation}\np_{punish} = \\begin{cases} 0 & \\text{sanctioner wins | no winner} \\\\\np & otherwise\n\\end{cases}\n\\end{equation}\\]\n\\(R\\) is the number of rounds that the race lasts for; the race ends when the first firm reaches the finish line.\n\\[\\begin{equation}\nR = \\begin{cases} \\infty & \\text{no winner} \\\\\n\\frac{W - 1}{s_{\\alpha}} & \\text{sanctioner wins} \\\\\n\\frac{W - s}{s_{\\beta}} & \\text{punished wins | draw} \\\\\n\\end{cases}\n\\end{equation}\\]\n\\(B_s\\) is the prize that the sanctioner receives at the end of the race.\n\\[\\begin{equation}\nB_s = \\begin{cases} B & \\text{sanctioner wins} \\\\\n\\frac{B}{2} & \\text{draw} \\\\\n0 & otherwise \\\\\n\\end{cases}\n\\end{equation}\\]\n\\(B_p\\) is the prize that the punished receives at the end of the race.\n\\[\\begin{equation}\nB_p = \\begin{cases} B & \\text{punished wins} \\\\\n\\frac{B}{2} & \\text{draw} \\\\\n0 & otherwise \\\\\n\\end{cases}\n\\end{equation}\\]\n\\(b_s\\) is the benefit the sanctioner receives each round, they only gain a benefit if their speed is positive but gain the whole benefit if they are the only firm with positive speed.\n\\[\\begin{equation}\nb_s = \\begin{cases} p_{fo} b + (1-p_{fo}) b \\frac{s_{\\alpha}}{s_{\\alpha} + s_{\\beta}} & s_{\\alpha}, s_{\\beta} > 0\\\\\nb & s_{\\alpha} > 0 >= s_{\\beta} \\\\\n0 & s_{\\alpha} <= 0 \\\\\n\\end{cases}\n\\end{equation}\\]\n\\(b_p\\) is the benefit the punished receives each round, they only gain a benefit if their speed is positive but gain the whole benefit if they are the only firm with positive speed.\n\\[\\begin{equation}\nb_p = \\begin{cases} (1-p_{fo}) b \\frac{s_{\\beta}}{s_{\\alpha} + s_{\\beta}} & s_{\\alpha}, s_{\\beta} > 0\\\\\nb & s_{\\beta} > 0 >= s_{\\alpha} \\\\\n0 & s_{\\beta} <= 0 \\\\\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "Payoffs/payoffs1.html#dsair-payoff-matrix-with-rewards-long-run",
    "href": "Payoffs/payoffs1.html#dsair-payoff-matrix-with-rewards-long-run",
    "title": "Payoff Matrices (part 1)",
    "section": "DSAIR Payoff Matrix with rewards (Long Run)",
    "text": "DSAIR Payoff Matrix with rewards (Long Run)\nDenote \\(\\pi\\) as one of the short run payoff matrices discussed above with rows and columns indexed by letters A, B, …\nAlways Safe and Always Unsafe play as they usually do.\nReward Safe always plays Safe. However, they will pay a cost to reward their co-player if the co-player plays Safe.\n\n\n\n\n\n\n\n\n\nStrategy\nAlways Safe\nAlways Unsafe\nReward Safe\n\n\n\n\nAlways Safe\n\\[πAA + \\frac{B}{2W}\\]\n\\[πAB\\]\n\\[πAA + \\frac{B (1 + s_{\\beta})}{W}\\]\n\n\nAlways Unsafe\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\n\nReward Safe\n\\[ πAA \\]\n\\[ πAB \\]\n\\[πAA + \\frac{B (1 + s_{\\beta} - s_{\\alpha})}{2W}\\]\n\n\n\n\nsource\n\npayoffs_lr_peer_reward\n\n payoffs_lr_peer_reward (models:dict)\n\nThe long run average payoffs for the DSAIR game with peer punishment.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary containing the items in ModelTypeDSAIR\n\n\nReturns\ndict\nThe models dictionary with added payoff matrix payoffs:"
  },
  {
    "objectID": "Payoffs/payoffs1.html#dsair-payoff-matrix-with-voluntary-commitments-long-run",
    "href": "Payoffs/payoffs1.html#dsair-payoff-matrix-with-voluntary-commitments-long-run",
    "title": "Payoff Matrices (part 1)",
    "section": "DSAIR Payoff Matrix with voluntary commitments (Long Run)",
    "text": "DSAIR Payoff Matrix with voluntary commitments (Long Run)\nDenote \\(\\pi\\) as one of the short run payoff matrices discussed above with rows and columns indexed by letters A, B, …\nThe strategies below are less obvious than in earlier models. Always Safe Out and Always Unsafe Out are the same strategies we are used to.\nAlways Safe In is willing to form a commitment to play Safe. Otherwise, they will always play Unsafe.\nAlways Unsafe In is willing to form a commitment but will violate it by always playing Unsafe. This way, they anticipate that they can encourage other firms to play safe and so pull ahead of them in the race.\nPunish Violator is willing to form a commitment to play Safe. Otherwise, they will always play Unsafe. If the coparty to the commitment violates the commitment by playing Unsafe, then this player pays a cost to levy a punishment on the violator.\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nAlways Safe Out\nAlways Unsafe Out\nAlways Safe In\nAlways Unsafe In\nPunish Violator\n\n\n\n\nAlways Safe Out\n\\[πAA + \\frac{B}{2W}\\]\n\\[πAB\\]\n\\[πAB\\]\n\\[πAB\\]\n\\[πAB\\]\n\n\nAlways Unsafe Out\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\n\nAlways Safe In\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\\[πAA + \\frac{B}{2W} - \\epsilon\\]\n\\[πAB - \\epsilon\\]\n\\[πAA + \\frac{B}{2W} - \\epsilon\\]\n\n\nAlways Unsafe In\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\\[p \\, (s \\frac{B}{W} + πBA) - \\epsilon\\]\n\\[p \\, (s \\frac{B}{2W} + πBB) - \\epsilon\\]\npunished_payoff - ϵ\n\n\nPunish Violator\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\\[p \\, (s \\frac{B}{2W} + πBB)\\]\n\\[πAA + \\frac{B}{2W} - \\epsilon\\]\nsanctioner_payoff - ϵ\n\\[πAA + \\frac{B}{2W} - \\epsilon\\]\n\n\n\nThe punished and sanctioner payoffs above are exactly the same as in the model with punishments above, so I do not repeat this here.\n\nsource\n\npayoffs_lr_voluntary\n\n payoffs_lr_voluntary (models:dict)\n\nThe long run average payoffs for the DSAIR game with voluntary commitments.\n\n\n\n\nType\nDetails\n\n\n\n\nmodels\ndict\nA dictionary containing the items in ModelTypeDSAIR\n\n\nReturns\ndict\nThe models dictionary with added payoff matrix payoffs:"
  },
  {
    "objectID": "Payoffs/payoffs1.html#dsair-payoff-matrix-long-run-with-collective-risk",
    "href": "Payoffs/payoffs1.html#dsair-payoff-matrix-long-run-with-collective-risk",
    "title": "Payoff Matrices (part 1)",
    "section": "DSAIR Payoff Matrix (Long Run) with collective risk",
    "text": "DSAIR Payoff Matrix (Long Run) with collective risk\nDenote \\(\\pi\\) as one of the short run payoff matrices discussed above with rows and columns indexed by letters A, B, …\n\n\n\n\n\n\n\n\nStrategy\nAlways Safe\nAlways Unsafe\n\n\n\n\nAlways Safe\n\\[πAA + \\frac{B}{2W}\\]\n\\[p \\, πAB\\]\n\n\nAlways Unsafe\n\\[p \\, (s \\frac{B}{W} + πBA)\\]\n\\[p^2 \\, (s \\frac{B}{2W} + πBB)\\]\n\n\n\n\ndef payoffs_lr_collective(models:dict, # A dictionary containing the items in `ModelTypeDSAIR`\n              ) -> dict : # The `models` dictionary with added payoff matrix `payoffs`:\n    \"\"\"Long run average payoffs for the DSAIR model with collective risk.\"\"\"\n    # All 1D arrays must be promoted to 3D Arrays for broadcasting\n    s,b,c, p, B, W = [models[k][:, None, None]\n                      for k in ['s', 'b', 'c', 'p', 'B', 'W']]\n    risk_shared = models[\"collective_risk\"][:, None, None]\n    πAA,πAB,πBA,πBB = [models['payoffs_sr'][:, i:i+1, j:j+1]\n                       for i in range(2) for j in range(2)]\n    πAA = πAA + B/(2*W)\n    πAB = πAB * (1 - (1-p)*risk_shared)\n    πBA = p*(s*B/W + πBA)\n    πBB = p*(s*B/(2*W) + πBB) * (1 - (1-p)*risk_shared)\n    matrix = np.block([[πAA, πAB],\n                       [πBA, πBB]])\n    return {**models, 'payoffs':matrix}"
  },
  {
    "objectID": "Payoffs/payoffs2.html",
    "href": "Payoffs/payoffs2.html",
    "title": "Payoff Matrices (part 2)",
    "section": "",
    "text": "np.set_printoptions(suppress=True)  # don't use scientific notation\nsource"
  },
  {
    "objectID": "Payoffs/payoffs2.html#vasconselos-et-al.-2014",
    "href": "Payoffs/payoffs2.html#vasconselos-et-al.-2014",
    "title": "Payoff Matrices (part 2)",
    "section": "Vasconselos et al. 2014",
    "text": "Vasconselos et al. 2014\nThey introduce a model of a Collective Risk Dilemma. It is a variant of the public goods game where players must achieve a target level of contributions to avoid risking a disaster which destroys the group’s endowments.\nWe compute payoffs when players contribute \\(0\\) or a fixed \\(c\\) proportion of their endowment as a contribution in a game with up to \\(n\\) participants. To do this, we compute the payoffs as a function of the number of contributors, then use that function for each relevant strategy profile.\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\nHere are a few simple tests of the payoff primitives for their model.\n\nmodels = {'payoffs_state': {'strategy_counts': {\"2\": 2,\n                                                \"4\": 4}},\n          'c': 0.5,\n          'T': 2,\n          'b_r': 4,\n          'b_p': 2,\n          'r': 0.5,\n          'payoffs_key': 'vasconcelos_2014_primitives'}\nmodels = build_payoffs(models)\nfastcore.test.test_eq(models['payoff_primitives'],\n                      {'1': 4,\n                       '2': 2,\n                       '3': 2,\n                       '4': 1})\nmodels = {**models,\n          'payoffs_state': {'strategy_counts': {\"2\": 0,\n                                                \"4\": 1}}, }\nmodels = build_payoffs(models)\nfastcore.test.test_eq(models['payoff_primitives'],\n                      {'1': 2,\n                       '2': 1,\n                       '3': 1,\n                       '4': 0.5})\n\nWe quickly check that we can generate payoffs for each of the 5**5 possible interactions in their model.\n\nmodels = {'c': 0.5,\n          'T': 2,\n          'b_r': 4,\n          'b_p': 2,\n          'r': 0.5,\n          'payoffs_key': 'vasconcelos_2014'}\nmodels = build_payoffs(models)\nfastcore.test.test_eq(len(models['payoffs']), 5**5)\n\nIf we are unwilling to use the 5**5 possible strategy profiles for computing the transition matrices for the evolutionary system, we can always restrict our attention to the payoffs given the number of contributors from each sector. We often use hypergeometric sampling anyways when computing the success of each strategy in the evolutionary system."
  },
  {
    "objectID": "Payoffs/payoffs2.html#general-payoff-wrapper",
    "href": "Payoffs/payoffs2.html#general-payoff-wrapper",
    "title": "Payoff Matrices (part 2)",
    "section": "General Payoff Wrapper",
    "text": "General Payoff Wrapper\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)"
  },
  {
    "objectID": "Payoffs/payoffs2.html#stochastic-payoffs",
    "href": "Payoffs/payoffs2.html#stochastic-payoffs",
    "title": "Payoff Matrices (part 2)",
    "section": "Stochastic Payoffs",
    "text": "Stochastic Payoffs\n\nStochastic payoffs\nWe can compute the payoffs of stochastic games with state-action transition matrix, \\(M\\), and state-action utilities, \\(u\\), and discount factor, \\(\\delta\\), as follows:\n\\(v = (1 - \\delta) v^0 (I - \\delta M)^{-1}\\)\n\\(payoffs = v \\cdot u\\)\nWhen \\(\\delta \\rightarrow 1\\), we instead compute \\(v\\) as the eigenvector of \\(M\\) with associated eigenvalue \\(1\\).\n\\(M\\) is the product of a transition matrix and a matrix containing the probabilities with which each action profile occurs (i.e. a matrix of player (mixed) strategies). \\(M\\) has size \\(2mk + 1\\), where \\(m\\) is the number of states and \\(k\\) is the number of strategies available to each player.\nWe first need to define our flow payoffs, that is, at each state-action combination, what are the payoffs to each type of player.\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\nTests for “flow_payoffs_wrapper” method of build_payoffs\nHere is an example of flow payoffs.\n\n@method(build_payoffs, 'vasconcelos_2014_flow')\ndef build_payoffs(models: dict):\n    names = ['payoffs_state', 'c', 'T', 'b_r', 'b_p', 'r', 'g']\n    payoffs_state, c, T, b_r, b_p, r, g = [models[k] for k in names]\n    strategy_counts = payoffs_state['strategy_counts']\n    state = payoffs_state['state']\n    reward_bonus = g if state=='1' else 1\n    n_r = strategy_counts.get(\"2\", 0)\n    n_p = strategy_counts.get(\"4\", 0)\n    risk = r * (n_r * c * b_r + n_p * c * b_p < T)\n    payoffs = {\"1\": (1 - risk) * b_r * reward_bonus,  # rich_free_rider\n               \"2\": (1 - risk) * c * b_r * reward_bonus,  # rich_contributor\n               \"3\": (1 - risk) * b_p * reward_bonus,  # poor_free_rider\n               \"4\": (1 - risk) * c * b_p * reward_bonus}  # poor_contributor\n    return {**models, \"flow_payoffs\": payoffs}\n\n\nmodels = {\"allowed_sectors\": {\"P1\": [\"S1\", \"S2\"],\n                              \"P2\": [\"S1\", \"S2\"]},\n          \"sector_strategies\": {\"S1\": [1, 2],\n                                \"S2\": [3, 4]},\n          \"profiles_rule\": \"allowed_sectors\",}\naction_profiles = create_profiles(models)[\"profiles\"]\nn_states = 2\nstate_actions = []\nfor profile in action_profiles:\n    for state in range(n_states):\n        state_actions.append(f\"{state}:{profile}\")\n\nmodels = {\"payoffs_flow_key\": \"vasconcelos_2014_flow\",\n          \"payoffs_key\": \"flow_payoffs_wrapper\",\n          \"state_actions\": state_actions,\n          'c': 0.5,\n          'T': 2,\n          'b_r': 4,\n          'b_p': 2,\n          'r': 0.8,\n          'g': 2,\n          }\nflow_payoffs = build_payoffs(models)['flow_payoffs']\n\n\n\nState transition functions\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\nsource\n\n\nbuild_state_transitions\n\n build_state_transitions (models)\n\n\nTests for build_state_transitions\n\nmodels = {\"allowed_sectors\": {\"P1\": [\"S1\", \"S2\"],\n                              \"P2\": [\"S1\", \"S2\"]},\n          \"sector_strategies\": {\"S1\": [1, 2],\n                                \"S2\": [3, 4]},\n          \"profiles_rule\": \"allowed_sectors\", }\naction_profiles = create_profiles(models)[\"profiles\"]\nn_states = 2\nstate_actions = [f\"{state}:{a}\"\n                 for a in action_profiles\n                 for state in range(n_states)]\nmodels = {'n_states':n_states,\n          'state_actions': state_actions,\n          'state_transition_key': 'ex1'}\nresult = build_state_transitions(models)['state_transitions']\nexpected = {'0:1-1': {'0': 1, '1': 0},\n '1:1-1': {'0': 1, '1': 0},\n '0:1-2': {'0': 0, '1': 1},\n '1:1-2': {'0': 0, '1': 1},\n '0:1-3': {'0': 1, '1': 0},\n '1:1-3': {'0': 1, '1': 0},\n '0:1-4': {'0': 0, '1': 1},\n '1:1-4': {'0': 0, '1': 1},\n '0:2-1': {'0': 0, '1': 1},\n '1:2-1': {'0': 0, '1': 1},\n '0:2-2': {'0': 0, '1': 1},\n '1:2-2': {'0': 0, '1': 1},\n '0:2-3': {'0': 0, '1': 1},\n '1:2-3': {'0': 0, '1': 1},\n '0:2-4': {'0': 0, '1': 1},\n '1:2-4': {'0': 0, '1': 1},\n '0:3-1': {'0': 1, '1': 0},\n '1:3-1': {'0': 1, '1': 0},\n '0:3-2': {'0': 0, '1': 1},\n '1:3-2': {'0': 0, '1': 1},\n '0:3-3': {'0': 1, '1': 0},\n '1:3-3': {'0': 1, '1': 0},\n '0:3-4': {'0': 0, '1': 1},\n '1:3-4': {'0': 0, '1': 1},\n '0:4-1': {'0': 0, '1': 1},\n '1:4-1': {'0': 0, '1': 1},\n '0:4-2': {'0': 0, '1': 1},\n '1:4-2': {'0': 0, '1': 1},\n '0:4-3': {'0': 0, '1': 1},\n '1:4-3': {'0': 0, '1': 1},\n '0:4-4': {'0': 0, '1': 1},\n '1:4-4': {'0': 0, '1': 1}}\nfastcore.test.test_eq(result, expected)\n\n\nmodels = {\"allowed_sectors\": {\"P1\": [\"S1\", \"S2\"],\n                              \"P2\": [\"S1\", \"S2\"]},\n          \"sector_strategies\": {\"S1\": [\"1\", \"2\"],\n                                \"S2\": [\"3\", \"4\"]},\n          \"profiles_rule\": \"anonymous\", }\naction_profiles = create_profiles(models)[\"profiles\"]\nn_states = 2\nstate_actions = [f\"{state}:{a}\"\n                 for a in action_profiles\n                 for state in range(n_states)]\nmodels = {'n_states':n_states,\n          'state_actions': state_actions,\n          'state_transition_key': 'ex1'}\nresult = build_state_transitions(models)['state_transitions']\nexpected = {'0:4-4': {'0': 0, '1': 1},\n '1:4-4': {'0': 0, '1': 1},\n '0:4-3': {'0': 0, '1': 1},\n '1:4-3': {'0': 0, '1': 1},\n '0:3-3': {'0': 1, '1': 0},\n '1:3-3': {'0': 1, '1': 0},\n '0:4-2': {'0': 0, '1': 1},\n '1:4-2': {'0': 0, '1': 1},\n '0:3-2': {'0': 0, '1': 1},\n '1:3-2': {'0': 0, '1': 1},\n '0:2-2': {'0': 0, '1': 1},\n '1:2-2': {'0': 0, '1': 1},\n '0:4-1': {'0': 0, '1': 1},\n '1:4-1': {'0': 0, '1': 1},\n '0:3-1': {'0': 1, '1': 0},\n '1:3-1': {'0': 1, '1': 0},\n '0:2-1': {'0': 0, '1': 1},\n '1:2-1': {'0': 0, '1': 1},\n '0:1-1': {'0': 1, '1': 0},\n '1:1-1': {'0': 1, '1': 0}}\nfastcore.test.test_eq(result, expected)\n\n\n\nStrategy construction\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\nsource\n\n\nbuild_strategies\n\n build_strategies (models)\n\nBuild a dictionary containing the specified strategies in models\n\nTests for build_strategy\n\nmodels = {\"allowed_sectors\": {\"P1\": [\"S1\", \"S2\"],\n                              \"P2\": [\"S1\", \"S2\"]},\n          \"sector_strategies\": {\"S1\": [\"1\", \"2\"],\n                                \"S2\": [\"3\", \"4\"]},\n          \"profiles_rule\": \"anonymous\", }\naction_profiles = create_profiles(models)[\"profiles\"]\nn_states = 2\nstate_actions = [f\"{state}:{a}\"\n                 for a in action_profiles\n                 for state in range(n_states)]\nstrategy_keys = [\"ex1_rich_cooperator\",\n                 \"ex1_rich_defector\",\n                 \"ex1_poor_cooperator\",\n                 \"ex1_poor_defector\",]\nstrategies = {f\"{i+1}\": {s: build_strategy({\"strategy_key\": strategy_key,\n                                            \"state_action\": s})\n                         for s in state_actions}\n              for i, strategy_key in enumerate(strategy_keys)}\nexpected = {'1': {'0:4-4': {'A1': 0.05, 'A2': 0.95},\n  '1:4-4': {'A1': 0.05, 'A2': 0.95},\n  '0:4-3': {'A1': 0.05, 'A2': 0.95},\n  '1:4-3': {'A1': 0.05, 'A2': 0.95},\n  '0:3-3': {'A1': 0.95, 'A2': 0.05},\n  '1:3-3': {'A1': 0.95, 'A2': 0.05},\n  '0:4-2': {'A1': 0.05, 'A2': 0.95},\n  '1:4-2': {'A1': 0.05, 'A2': 0.95},\n  '0:3-2': {'A1': 0.05, 'A2': 0.95},\n  '1:3-2': {'A1': 0.05, 'A2': 0.95},\n  '0:2-2': {'A1': 0.05, 'A2': 0.95},\n  '1:2-2': {'A1': 0.05, 'A2': 0.95},\n  '0:4-1': {'A1': 0.05, 'A2': 0.95},\n  '1:4-1': {'A1': 0.05, 'A2': 0.95},\n  '0:3-1': {'A1': 0.95, 'A2': 0.05},\n  '1:3-1': {'A1': 0.95, 'A2': 0.05},\n  '0:2-1': {'A1': 0.05, 'A2': 0.95},\n  '1:2-1': {'A1': 0.05, 'A2': 0.95},\n  '0:1-1': {'A1': 0.95, 'A2': 0.05},\n  '1:1-1': {'A1': 0.95, 'A2': 0.05}},\n '2': {'0:4-4': {'A1': 0.05, 'A2': 0.95},\n  '1:4-4': {'A1': 0.05, 'A2': 0.95},\n  '0:4-3': {'A1': 0.05, 'A2': 0.95},\n  '1:4-3': {'A1': 0.05, 'A2': 0.95},\n  '0:3-3': {'A1': 0.05, 'A2': 0.95},\n  '1:3-3': {'A1': 0.05, 'A2': 0.95},\n  '0:4-2': {'A1': 0.05, 'A2': 0.95},\n  '1:4-2': {'A1': 0.05, 'A2': 0.95},\n  '0:3-2': {'A1': 0.05, 'A2': 0.95},\n  '1:3-2': {'A1': 0.05, 'A2': 0.95},\n  '0:2-2': {'A1': 0.05, 'A2': 0.95},\n  '1:2-2': {'A1': 0.05, 'A2': 0.95},\n  '0:4-1': {'A1': 0.05, 'A2': 0.95},\n  '1:4-1': {'A1': 0.05, 'A2': 0.95},\n  '0:3-1': {'A1': 0.05, 'A2': 0.95},\n  '1:3-1': {'A1': 0.05, 'A2': 0.95},\n  '0:2-1': {'A1': 0.05, 'A2': 0.95},\n  '1:2-1': {'A1': 0.05, 'A2': 0.95},\n  '0:1-1': {'A1': 0.05, 'A2': 0.95},\n  '1:1-1': {'A1': 0.05, 'A2': 0.95}},\n '3': {'0:4-4': {'A3': 0.05, 'A4': 0.95},\n  '1:4-4': {'A3': 0.05, 'A4': 0.95},\n  '0:4-3': {'A3': 0.05, 'A4': 0.95},\n  '1:4-3': {'A3': 0.05, 'A4': 0.95},\n  '0:3-3': {'A3': 0.95, 'A4': 0.05},\n  '1:3-3': {'A3': 0.95, 'A4': 0.05},\n  '0:4-2': {'A3': 0.05, 'A4': 0.95},\n  '1:4-2': {'A3': 0.05, 'A4': 0.95},\n  '0:3-2': {'A3': 0.05, 'A4': 0.95},\n  '1:3-2': {'A3': 0.05, 'A4': 0.95},\n  '0:2-2': {'A3': 0.05, 'A4': 0.95},\n  '1:2-2': {'A3': 0.05, 'A4': 0.95},\n  '0:4-1': {'A3': 0.05, 'A4': 0.95},\n  '1:4-1': {'A3': 0.05, 'A4': 0.95},\n  '0:3-1': {'A3': 0.95, 'A4': 0.05},\n  '1:3-1': {'A3': 0.95, 'A4': 0.05},\n  '0:2-1': {'A3': 0.05, 'A4': 0.95},\n  '1:2-1': {'A3': 0.05, 'A4': 0.95},\n  '0:1-1': {'A3': 0.95, 'A4': 0.05},\n  '1:1-1': {'A3': 0.95, 'A4': 0.05}},\n '4': {'0:4-4': {'A3': 0.05, 'A4': 0.95},\n  '1:4-4': {'A3': 0.05, 'A4': 0.95},\n  '0:4-3': {'A3': 0.05, 'A4': 0.95},\n  '1:4-3': {'A3': 0.05, 'A4': 0.95},\n  '0:3-3': {'A3': 0.05, 'A4': 0.95},\n  '1:3-3': {'A3': 0.05, 'A4': 0.95},\n  '0:4-2': {'A3': 0.05, 'A4': 0.95},\n  '1:4-2': {'A3': 0.05, 'A4': 0.95},\n  '0:3-2': {'A3': 0.05, 'A4': 0.95},\n  '1:3-2': {'A3': 0.05, 'A4': 0.95},\n  '0:2-2': {'A3': 0.05, 'A4': 0.95},\n  '1:2-2': {'A3': 0.05, 'A4': 0.95},\n  '0:4-1': {'A3': 0.05, 'A4': 0.95},\n  '1:4-1': {'A3': 0.05, 'A4': 0.95},\n  '0:3-1': {'A3': 0.05, 'A4': 0.95},\n  '1:3-1': {'A3': 0.05, 'A4': 0.95},\n  '0:2-1': {'A3': 0.05, 'A4': 0.95},\n  '1:2-1': {'A3': 0.05, 'A4': 0.95},\n  '0:1-1': {'A3': 0.05, 'A4': 0.95},\n  '1:1-1': {'A3': 0.05, 'A4': 0.95}}}\nfastcore.test.test_eq(strategies, expected)\n\n\nmodels = {\"allowed_sectors\": {\"P1\": [\"S1\", \"S2\"],\n                              \"P2\": [\"S1\", \"S2\"]},\n          \"sector_strategies\": {\"S1\": [\"1\", \"2\"],\n                                \"S2\": [\"3\", \"4\"]},\n          \"profiles_rule\": \"anonymous\", }\naction_profiles = create_profiles(models)[\"profiles\"]\nn_states = 2\nstate_actions = [f\"{state}:{a}\"\n                 for a in action_profiles\n                 for state in range(n_states)]\nstrategy_keys = [\"ex1_rich_cooperator\",\n                 \"ex1_rich_defector\",\n                 \"ex1_poor_cooperator\",\n                 \"ex1_poor_defector\",]\nmodels = {**models,\n          \"strategy_keys\": strategy_keys,\n          \"state_actions\": state_actions}\nstrategies = build_strategies(models)['strategies']\nexpected = {'1': {'0:4-4': {'A1': 0.05, 'A2': 0.95},\n  '1:4-4': {'A1': 0.05, 'A2': 0.95},\n  '0:4-3': {'A1': 0.05, 'A2': 0.95},\n  '1:4-3': {'A1': 0.05, 'A2': 0.95},\n  '0:3-3': {'A1': 0.95, 'A2': 0.05},\n  '1:3-3': {'A1': 0.95, 'A2': 0.05},\n  '0:4-2': {'A1': 0.05, 'A2': 0.95},\n  '1:4-2': {'A1': 0.05, 'A2': 0.95},\n  '0:3-2': {'A1': 0.05, 'A2': 0.95},\n  '1:3-2': {'A1': 0.05, 'A2': 0.95},\n  '0:2-2': {'A1': 0.05, 'A2': 0.95},\n  '1:2-2': {'A1': 0.05, 'A2': 0.95},\n  '0:4-1': {'A1': 0.05, 'A2': 0.95},\n  '1:4-1': {'A1': 0.05, 'A2': 0.95},\n  '0:3-1': {'A1': 0.95, 'A2': 0.05},\n  '1:3-1': {'A1': 0.95, 'A2': 0.05},\n  '0:2-1': {'A1': 0.05, 'A2': 0.95},\n  '1:2-1': {'A1': 0.05, 'A2': 0.95},\n  '0:1-1': {'A1': 0.95, 'A2': 0.05},\n  '1:1-1': {'A1': 0.95, 'A2': 0.05}},\n '2': {'0:4-4': {'A1': 0.05, 'A2': 0.95},\n  '1:4-4': {'A1': 0.05, 'A2': 0.95},\n  '0:4-3': {'A1': 0.05, 'A2': 0.95},\n  '1:4-3': {'A1': 0.05, 'A2': 0.95},\n  '0:3-3': {'A1': 0.05, 'A2': 0.95},\n  '1:3-3': {'A1': 0.05, 'A2': 0.95},\n  '0:4-2': {'A1': 0.05, 'A2': 0.95},\n  '1:4-2': {'A1': 0.05, 'A2': 0.95},\n  '0:3-2': {'A1': 0.05, 'A2': 0.95},\n  '1:3-2': {'A1': 0.05, 'A2': 0.95},\n  '0:2-2': {'A1': 0.05, 'A2': 0.95},\n  '1:2-2': {'A1': 0.05, 'A2': 0.95},\n  '0:4-1': {'A1': 0.05, 'A2': 0.95},\n  '1:4-1': {'A1': 0.05, 'A2': 0.95},\n  '0:3-1': {'A1': 0.05, 'A2': 0.95},\n  '1:3-1': {'A1': 0.05, 'A2': 0.95},\n  '0:2-1': {'A1': 0.05, 'A2': 0.95},\n  '1:2-1': {'A1': 0.05, 'A2': 0.95},\n  '0:1-1': {'A1': 0.05, 'A2': 0.95},\n  '1:1-1': {'A1': 0.05, 'A2': 0.95}},\n '3': {'0:4-4': {'A3': 0.05, 'A4': 0.95},\n  '1:4-4': {'A3': 0.05, 'A4': 0.95},\n  '0:4-3': {'A3': 0.05, 'A4': 0.95},\n  '1:4-3': {'A3': 0.05, 'A4': 0.95},\n  '0:3-3': {'A3': 0.95, 'A4': 0.05},\n  '1:3-3': {'A3': 0.95, 'A4': 0.05},\n  '0:4-2': {'A3': 0.05, 'A4': 0.95},\n  '1:4-2': {'A3': 0.05, 'A4': 0.95},\n  '0:3-2': {'A3': 0.05, 'A4': 0.95},\n  '1:3-2': {'A3': 0.05, 'A4': 0.95},\n  '0:2-2': {'A3': 0.05, 'A4': 0.95},\n  '1:2-2': {'A3': 0.05, 'A4': 0.95},\n  '0:4-1': {'A3': 0.05, 'A4': 0.95},\n  '1:4-1': {'A3': 0.05, 'A4': 0.95},\n  '0:3-1': {'A3': 0.95, 'A4': 0.05},\n  '1:3-1': {'A3': 0.95, 'A4': 0.05},\n  '0:2-1': {'A3': 0.05, 'A4': 0.95},\n  '1:2-1': {'A3': 0.05, 'A4': 0.95},\n  '0:1-1': {'A3': 0.95, 'A4': 0.05},\n  '1:1-1': {'A3': 0.95, 'A4': 0.05}},\n '4': {'0:4-4': {'A3': 0.05, 'A4': 0.95},\n  '1:4-4': {'A3': 0.05, 'A4': 0.95},\n  '0:4-3': {'A3': 0.05, 'A4': 0.95},\n  '1:4-3': {'A3': 0.05, 'A4': 0.95},\n  '0:3-3': {'A3': 0.05, 'A4': 0.95},\n  '1:3-3': {'A3': 0.05, 'A4': 0.95},\n  '0:4-2': {'A3': 0.05, 'A4': 0.95},\n  '1:4-2': {'A3': 0.05, 'A4': 0.95},\n  '0:3-2': {'A3': 0.05, 'A4': 0.95},\n  '1:3-2': {'A3': 0.05, 'A4': 0.95},\n  '0:2-2': {'A3': 0.05, 'A4': 0.95},\n  '1:2-2': {'A3': 0.05, 'A4': 0.95},\n  '0:4-1': {'A3': 0.05, 'A4': 0.95},\n  '1:4-1': {'A3': 0.05, 'A4': 0.95},\n  '0:3-1': {'A3': 0.05, 'A4': 0.95},\n  '1:3-1': {'A3': 0.05, 'A4': 0.95},\n  '0:2-1': {'A3': 0.05, 'A4': 0.95},\n  '1:2-1': {'A3': 0.05, 'A4': 0.95},\n  '0:1-1': {'A3': 0.05, 'A4': 0.95},\n  '1:1-1': {'A3': 0.05, 'A4': 0.95}}}\nfastcore.test.test_eq(strategies, expected)\n\n\n\nStochastic payoffs test\n\nmodels = {\"allowed_sectors\": {\"P1\": [\"S1\", \"S2\"],\n                              \"P2\": [\"S1\", \"S2\"]},\n          \"sector_strategies\": {\"S1\": [\"1\", \"2\"],\n                                \"S2\": [\"3\", \"4\"]},\n          \"profiles_rule\": \"anonymous\", }\naction_profiles = create_profiles(models)[\"profiles\"]\nn_states = 2\nstate_actions = [f\"{state}:{a}\"\n                 for a in action_profiles\n                 for state in range(n_states)]\n\nstrategy_keys = [\"ex1_rich_cooperator\",\n                 \"ex1_rich_defector\",\n                 \"ex1_poor_cooperator\",\n                 \"ex1_poor_defector\",]\nmodels = {**models,\n          \"strategy_keys\": strategy_keys,\n          \"state_actions\": state_actions}\nstrategies = build_strategies(models)['strategies']\nstrategy_profile = \"1-2-3\"\nmodels = {\"payoffs_flow_key\": \"vasconcelos_2014_flow\",\n          \"payoffs_key\": \"flow_payoffs_wrapper\",\n          \"state_actions\": state_actions,\n          \"strategies\": strategies,\n          \"strategy_profile\": strategy_profile,\n          'n_states':n_states,\n          'state_transition_key': 'ex1',\n          'compute_transition_key': \"anonymous_actions\",\n          'c': 0.5,\n          'T': 2,\n          'b_r': 4,\n          'b_p': 2,\n          'r': 0.8,\n          'g': 2,\n          }\n\nmodels = build_state_transitions(models)\nmodels = build_payoffs(models)\nmodels = {**models,\n          \"payoffs_key\": \"stochastic-no-discounting\"}\nresults = build_payoffs(models)\nexpected = {'1': 1.5979057591623032,\n '2': 0.7989528795811516,\n '3': 0.7989528795811516,\n '4': 0.3994764397905758}\nfastcore.test.test_eq(results['profile_payoffs'], expected)\n\n\nmodels = {\"allowed_sectors\": {\"P1\": [\"S1\", \"S2\"],\n                              \"P2\": [\"S1\", \"S2\"]},\n          \"sector_strategies\": {\"S1\": [\"1\", \"2\"],\n                                \"S2\": [\"3\", \"4\"]},\n          \"profiles_rule\": \"anonymous\", }\naction_profiles = create_profiles(models)[\"profiles\"]\nn_states = 2\nstate_actions = [f\"{state}:{a}\"\n                 for a in action_profiles\n                 for state in range(n_states)]\n\nstrategy_keys = [\"ex1_rich_cooperator\",\n                 \"ex1_rich_defector\",\n                 \"ex1_poor_cooperator\",\n                 \"ex1_poor_defector\",]\nmodels = {**models,\n          \"strategy_keys\": strategy_keys,\n          \"state_actions\": state_actions}\nstrategies = build_strategies(models)['strategies']\nstrategy_profile = \"1-2-3\"\nmodels = {**models,\n          \"payoffs_flow_key\": \"vasconcelos_2014_flow\",\n          \"payoffs_key\": \"flow_payoffs_wrapper\",\n          \"state_actions\": state_actions,\n          \"strategies\": strategies,\n          \"strategy_profile\": strategy_profile,\n          'n_states':n_states,\n          'state_transition_key': 'ex1',\n          'compute_transition_key': \"anonymous_actions\",\n          'c': 0.5,\n          'T': 2,\n          'b_r': 4,\n          'b_p': 2,\n          'r': 0.8,\n          'g': 2,\n          }\nmodels = build_state_transitions(models)\nmodels = build_payoffs(models)\nmodels = {**models,\n          \"payoffs_key\": \"payoff_function_wrapper\",\n          \"profile_payoffs_key\": \"stochastic-no-discounting\"}\nresults = build_payoffs(models)\nresults['payoffs']\n\n{'4-4': {'P1': 0.3994764397905758, 'P2': 0.3994764397905758},\n '4-3': {'P1': 0.7989528795811516, 'P2': 0.3994764397905758},\n '3-3': {'P1': 0.7989528795811516, 'P2': 0.7989528795811516},\n '4-2': {'P1': 0.7989528795811516, 'P2': 0.3994764397905758},\n '3-2': {'P1': 0.7989528795811516, 'P2': 0.7989528795811516},\n '2-2': {'P1': 0.7989528795811516, 'P2': 0.7989528795811516},\n '4-1': {'P1': 1.5979057591623032, 'P2': 0.3994764397905758},\n '3-1': {'P1': 1.5979057591623032, 'P2': 0.7989528795811516},\n '2-1': {'P1': 1.5979057591623032, 'P2': 0.7989528795811516},\n '1-1': {'P1': 1.5979057591623032, 'P2': 1.5979057591623032}}"
  },
  {
    "objectID": "Payoffs/index.html",
    "href": "Payoffs/index.html",
    "title": "Payoffd",
    "section": "",
    "text": "Click through to any of these notebooks for EGT payoff matrices.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nPayoff Matrices (part 1)\n\n\nThis module contains payoff matrices for different evolutionary games\n\n\n\n\nPayoff Matrices (part 2)\n\n\nThis module contains payoff matrices for different evolutionary games\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "model_utils.html",
    "href": "model_utils.html",
    "title": "Model Utilities",
    "section": "",
    "text": "source\n\n\n\n model_builder (saved_args:dict, exclude_args:list[str]=[],\n                override:bool=False, drop_args:list[str]=['override',\n                'exclude_args', 'drop_args'])\n\nBuild models for all combinations of the valules in saved_args.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsaved_args\ndict\n\na dictionary containing parameters we want to vary or hold fixed\n\n\nexclude_args\nlist\n[]\na list of arguments that should be returned as they are\n\n\noverride\nbool\nFalse\nwhether to build the grid if it is very large\n\n\ndrop_args\nlist\n[‘override’, ‘exclude_args’, ‘drop_args’]\na list of arguments to drop from the final result\n\n\nReturns\ndict\n\nA dictionary containing items for the desired models"
  },
  {
    "objectID": "model_utils.html#efficiently-find-profiles-for-hypergeometric-sampling",
    "href": "model_utils.html#efficiently-find-profiles-for-hypergeometric-sampling",
    "title": "Model Utilities",
    "section": "Efficiently find profiles for hypergeometric sampling",
    "text": "Efficiently find profiles for hypergeometric sampling\n\nfind_unique_allocations\nWe want to lexicographically sort our 2D array as we want to sort them so that those thoses with the highest counts for the lowest strategies (according to their numerical code) for the earliest subgroups considered (players are split into subgroups based on their allowed sectors, and subgroups are ordered by their lowest player index).\nWe can achieve this if we have strategy counts for each subgroup in a list, with counts for earlier subgroups first. We then build a list of these lists, one list for each possible way of achieving the same combined strategy count if we added each strategy count for each subgroup together.\nSo, I test that we can correctly convert a list of list of arrays to our desired 2D array.\nWe then perform our lexicographic sort and take only the last row of the sorted array. This is the combination of strategy counts which satisfies the properties we specified above.\n\nsource\n\n\nfind_unique_allocations\n\n find_unique_allocations (n, k)\n\nFind all combinations with replacement of ‘n’ of the first k integers where the sum of the integers is k.\n\n\n\n\nDetails\n\n\n\n\nn\nThe number of items to allocate\n\n\nk\nThe number of bins that items can be allocated to\n\n\n\n\nTests for find_unique_allocations\n\nfastcore.test.test_eq(find_unique_allocations(2, 4),\n                      [[0, 0, 0, 2],\n                       [0, 0, 1, 1],\n                       [0, 0, 2, 0],\n                       [0, 1, 0, 1],\n                       [0, 1, 1, 0],\n                       [0, 2, 0, 0],\n                       [1, 0, 0, 1],\n                       [1, 0, 1, 0],\n                       [1, 1, 0, 0],\n                       [2, 0, 0, 0], ])\n\nfastcore.test.test_eq(find_unique_allocations(4, 3),\n                      [[0, 0, 4],\n                       [0, 1, 3],\n                       [0, 2, 2],\n                       [0, 3, 1],\n                       [0, 4, 0],\n                       [1, 0, 3],\n                       [1, 1, 2],\n                       [1, 2, 1],\n                       [1, 3, 0],\n                       [2, 0, 2],\n                       [2, 1, 1],\n                       [2, 2, 0],\n                       [3, 0, 1],\n                       [3, 1, 0],\n                       [4, 0, 0], ])\n\n\n\nTests for building blocks of find_unique_counts\n\ncurrent_sub_group = [[\"2\", \"3\"], 2]\ncount_groups = [[[0, 1, 0, 0], [[1, 0, 0, 0]]],\n                [[1, 0, 0, 0], [[0, 1, 0, 0]]],\n                ]\ncategories, n_draws = current_sub_group\nn_categories = len(categories)\ncounts = find_unique_allocations(n_draws, n_categories)\nn_all_categories = len(count_groups[0][0])\nallocations = np.zeros((len(counts), n_all_categories))\nfor category, count in zip(categories, np.array(counts).T):\n    allocations[:, int(category)] = count\n\nfastcore.test.test_eq(allocations,\n                      [[0, 0, 0, 2],\n                       [0, 0, 1, 1],\n                       [0, 0, 2, 0], ])\n\n\ncurrent_sub_group = [[\"1\", \"3\"], 2]\ncount_groups = [[[0, 1, 0, 0], [[1, 0, 0, 0]]],\n                [[1, 0, 0, 0], [[0, 1, 0, 0]]],\n                ]\ncategories, n_draws = current_sub_group\nn_categories = len(categories)\ncounts = find_unique_allocations(n_draws, n_categories)\nn_all_categories = len(count_groups[0][0])\nallocations = np.zeros((len(counts), n_all_categories))\nfor category, count in zip(categories, np.array(counts).T):\n    allocations[:, int(category)] = count\n\nfastcore.test.test_eq(allocations,\n                      [[0, 0, 0, 2],\n                       [0, 1, 0, 1],\n                       [0, 2, 0, 0], ])\n\n\nfastcore.test.test_eq(np.sum([[1, 1], np.array([1, 2])], axis=1),\n                      [2, 3])\nfastcore.test.test_eq(\":\".join(map(str, map(int, [1, 2, 3.0]))),\n                      \"1:2:3\")\n\n\nrepeated_counts = [[np.array([1, 2, 3]),\n                    np.array([2, 3, 4]), ],\n                   [np.array([2, 3, 4]),\n                    np.array([1, 2, 3]), ]]\ny = np.vstack([np.hstack(seq) for seq in repeated_counts])\nfastcore.test.test_eq(y,\n                      np.array([[1, 2, 3, 2, 3, 4],\n                                [2, 3, 4, 1, 2, 3]]))\nfastcore.test.test_eq(np.lexsort(y.T[::-1]),\n                      [0, 1])\nfastcore.test.test_eq(np.lexsort(y.T[::-1])[-1],\n                      1)\nfastcore.test.test_eq(y[np.lexsort(y.T[::-1])[-1]],\n                      [2, 3, 4, 1, 2, 3])\nfastcore.test.test_eq(np.split(y[np.lexsort(y.T[::-1])[-1]], 2),\n                      [[2, 3, 4], [1, 2, 3]])\n\n\n\n\nfind_unique_counts\n\nsource\n\n\nfind_unique_counts\n\n find_unique_counts (count_groups, current_sub_group)\n\nBuild a list of all unique count groups given an allocation and a list of existing unique counts.\n\nTests for find_unique_counts\n\ncurrent_sub_group = [[\"2\", \"3\"], 2]\ncount_groups = [[[0, 0, 0, 0]], ]\nresult = find_unique_counts(count_groups, current_sub_group)\nexpected = [[[0, 0, 0, 0], [0, 0, 0, 2]],\n            [[0, 0, 0, 0], [0, 0, 1, 1]],\n            [[0, 0, 0, 0], [0, 0, 2, 0]]]\n\n\ncurrent_sub_group = [[\"2\", \"3\"], 2]\ncount_groups = [[[1, 0, 0, 0], [0, 1, 0, 0]], ]\nresult = find_unique_counts(count_groups, current_sub_group)\nexpected = [[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 2]],\n            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 1]],\n            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 2, 0]]]\n\ncurrent_sub_group = [[\"2\", \"3\"], 2]\n# Notice that the given count groups are redundant since if we had created them\n# using `find_unique_counts`, then we would have only kept the bottom one.\ncount_groups = [[[0, 1, 0, 0], [1, 0, 0, 0]],\n                [[1, 0, 0, 0], [0, 1, 0, 0]],\n                ]\nresult = find_unique_counts(count_groups, current_sub_group)\nexpected = [[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 2]],\n            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 1]],\n            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 2, 0]]]\n\n\nallowed_sectors = {\"P1\": [\"S1\"],\n                   \"P2\": [\"S1\", \"S2\"], }\nsector_strategies = {\"S1\": [\"1\", \"2\"],\n                     \"S2\": [\"3\", \"4\"],\n                     \"S3\": [\"5\", \"6\"]}\n\nrules = collections.defaultdict(list)\nfor player, sectors in allowed_sectors.items():\n    available_strategies = []\n    for sector in sectors:\n        available_strategies += sector_strategies[sector]\n    available_strategies = np.unique(available_strategies)\n    rule = \":\".join(available_strategies)\n    rules[rule].append(player)\n\nsubgroups = [[str.split(rule, ':'), len(players)]\n             for rule, players in rules.items()]\n\nfastcore.test.test_eq(subgroups,\n                      [[['1', '2'], 1], [['1', '2', '3', '4'], 1]])\n\n\nsorted([\"P10\", \"P1\",\"P2\"])\nplayer_strings = [\"P10\", \"P1\",\"P2\"]\nplayer_numbers = [int(\"\".join(string[1:]))\n                  for string in player_strings]\nplayers = [f\"P{x}\" for x in sorted(player_numbers)]\nfastcore.test.test_eq(players, [\"P1\", \"P2\",\"P10\"])"
  },
  {
    "objectID": "model_utils.html#methods-for-creating-strategy-profile-codes-for-a-set-of-models.",
    "href": "model_utils.html#methods-for-creating-strategy-profile-codes-for-a-set-of-models.",
    "title": "Model Utilities",
    "section": "Methods for creating strategy profile codes for a set of models.",
    "text": "Methods for creating strategy profile codes for a set of models.\nI also need a method for creating all possible player profiles. I need to use this method before the profile filters can do any filtering.\nWe can also apply our profile filters to this list to know exactly which profiles we need to write payoffs for.\nI also use this method to validate that we have passed sufficient information to payoffs, and to warn us about what is missing.\nNote that it creates strategy profiles where some players may not be present (as represented by the strategy 0). It is useful to think of 0 as representing a “null sector” as it captures the possibility that a player isn’t sampled from any of the sectors.\nIf one instead wants to capture the possibility that particular sectors may be less likely to participate, they may achieve this by creating a new strategy for that sector to represent a member from that sector as not participating in the interaction. It is also possible to achieve a similar effect by using sector_weights.\nI prefer to reserve 0 as a code for a missing player, so I index strategies from 1. However, note that if you wish to index your strategies from 0 instead, the profile filters are flexible enough to allow you to do so without issue. If you wish to later add the possibility of missing players, you just need to assign it a new code which you use to indicate the relevant payoffs.\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\nTests for create_profiles\nHere are tests for the default method, where all possible profiles are created for a given number of strategies and players.\n\nresult = create_profiles({\"n_players\": 2, \"n_strategies\": 2})\nfastcore.test.test_eq(result['profiles'],\n                      ['0-0', '0-1', '0-2',\n                       '1-0', '1-1', '1-2',\n                       '2-0', '2-1', '2-2'])\n\nresult = create_profiles({\"n_players\": 2, \"n_strategies\": [2, 1]})\nfastcore.test.test_eq(result['profiles'],\n                      [\"0-0\", \"0-1\", \"0-2\", \"0-3\",\n                       \"1-0\", \"1-1\", \"1-2\", \"1-3\",\n                       \"2-0\", \"2-1\", \"2-2\", \"2-3\",\n                       \"3-0\", \"3-1\", \"3-2\", \"3-3\", ])\n\nfastcore.test.test_eq(create_profiles({\"n_players\": 2,\n                                       \"n_strategies\": [2, 2]})['profiles'],\n                      create_profiles({\"n_players\": 2,\n                                       \"n_strategies\": 4})['profiles'])\n\nmodels = {\"allowed_sectors\": {\"P1\": [\"S1\"],\n                              \"P2\": [\"S2\"]},\n          \"sector_strategies\": {\"S1\": [1, 2],\n                                \"S2\": [3, 4]}}\nfastcore.test.test_eq(create_profiles(models)['profiles'],\n                      create_profiles({\"n_players\": 2,\n                                       \"n_strategies\": 4})['profiles'])\n\nI’ll test the \"from_strategy_count\" method. Notice that it grows quickly for larger strategy counts with more choices. As a rule of thumb, this will become untenable with more than 10 players, especially when there are more choices to choose between.\n\nstrategy_count = {\"1\": 2, \"2\": 2}\nmodels = {\"strategy_count\": strategy_count,\n          \"profiles_rule\": \"from_strategy_count\"}\nresults = create_profiles(models)\nlength = (math.factorial(sum(strategy_count.values()))\n          / math.prod([math.factorial(v) for v in strategy_count.values()]))\nfastcore.test.test_eq(length, len(results['profiles']))\nexpected = [\"1-1-2-2\",\n            \"1-2-1-2\",\n            \"1-2-2-1\",\n            \"2-1-1-2\",\n            \"2-1-2-1\",\n            \"2-2-1-1\",]\nfastcore.test.test_eq(expected, results['profiles'])\n\nstrategy_count = {\"1\": 2, \"2\": 2, \"3\": 4, \"4\": 3}\nmodels = {\"strategy_count\": strategy_count,\n          \"profiles_rule\": \"from_strategy_count\"}\nresults = create_profiles(models)\nlength = (math.factorial(sum(strategy_count.values()))\n          / math.prod([math.factorial(v) for v in strategy_count.values()]))\nfastcore.test.test_eq(length, len(results['profiles']))\n\nWe also include tests for the create_profiles “anonymous” method. which only creates one profile for each unique count of strategies.\n\nallowed_sectors = {\"P1\": [\"S1\"],\n                   \"P2\": [\"S1\", \"S2\"], }\n# Third sector irrelevant given allowed sectors\nsector_strategies = {\"S1\": [\"1\", \"2\"],\n                     \"S2\": [\"3\", \"4\"],\n                     \"S3\": [\"5\", \"6\"]}\nmodels = {\"allowed_sectors\": allowed_sectors,\n          \"sector_strategies\": sector_strategies,\n          \"profiles_rule\": \"anonymous\", }\nprofiles = create_profiles(models)['profiles']\nexpected = ['4-2', '3-2', '2-2', '2-1', '4-1', '3-1', '1-1']\nfastcore.test.test_eq(profiles, expected)\nfastcore.test.test_eq(sorted(profiles),\n                      ['1-1', '2-1', '2-2', '3-1', '3-2', '4-1', '4-2'])\n\n\nallowed_sectors = {\"P1\": [\"S1\"],\n                   \"P2\": [\"S1\", \"S2\"], }\n# Third sector irrelevant given allowed sectors\nsector_strategies = {\"S1\": [\"1\", \"0\"],\n                     \"S2\": [\"3\", \"4\"],\n                     \"S3\": [\"5\", \"6\"]}\nmodels = {\"allowed_sectors\": allowed_sectors,\n          \"sector_strategies\": sector_strategies,\n          \"profiles_rule\": \"anonymous\", }\nprofiles = create_profiles(models)['profiles']\nexpected = ['4-1', '3-1', '1-1', '1-0', '4-0', '3-0', '0-0']\nfastcore.test.test_eq(profiles, expected)\nfastcore.test.test_eq(sorted(profiles),\n                      ['0-0', '1-0', '1-1', '3-0', '3-1', '4-0', '4-1'])\n\n\nallowed_sectors = {\"P1\": [\"S1\"],\n                   \"P2\": [\"S1\", \"S2\"],\n                   \"P3\": [\"S1\", \"S2\"],\n                   \"P4\": [\"S1\", \"S2\"],\n                   \"P5\": [\"S1\", \"S2\"],\n                   \"P6\": [\"S1\", \"S2\"],\n                   \"P7\": [\"S1\", \"S2\"],\n                   \"P8\": [\"S1\", \"S2\"],\n                   \"P9\": [\"S1\", \"S2\"],\n                   \"P10\": [\"S1\", \"S2\"], }\n# Third sector irrelevant given allowed sectors\nsector_strategies = {\"S1\": [\"1\", \"2\"],\n                     \"S2\": [\"3\", \"4\"],\n                     \"S3\": [\"5\", \"6\"]}\nmodels = {\"allowed_sectors\": allowed_sectors,\n          \"sector_strategies\": sector_strategies,\n          \"profiles_rule\": \"anonymous\", }\nprofiles = create_profiles(models)['profiles']\nexpected = ['1-1-1-1-1-1-1-1-1-1',\n            '2-1-1-1-1-1-1-1-1-1',\n            '2-2-1-1-1-1-1-1-1-1',\n            '2-2-2-1-1-1-1-1-1-1',\n            '2-2-2-2-1-1-1-1-1-1',\n            '2-2-2-2-2-1-1-1-1-1',\n            '2-2-2-2-2-2-1-1-1-1',\n            '2-2-2-2-2-2-2-1-1-1',\n            '2-2-2-2-2-2-2-2-1-1',\n            '2-2-2-2-2-2-2-2-2-1',\n            '2-2-2-2-2-2-2-2-2-2',\n            '3-1-1-1-1-1-1-1-1-1',\n            '3-2-1-1-1-1-1-1-1-1',\n            '3-2-2-1-1-1-1-1-1-1',\n            '3-2-2-2-1-1-1-1-1-1',\n            '3-2-2-2-2-1-1-1-1-1',\n            '3-2-2-2-2-2-1-1-1-1',\n            '3-2-2-2-2-2-2-1-1-1',\n            '3-2-2-2-2-2-2-2-1-1',\n            '3-2-2-2-2-2-2-2-2-1',\n            '3-2-2-2-2-2-2-2-2-2',\n            '3-3-1-1-1-1-1-1-1-1',\n            '3-3-2-1-1-1-1-1-1-1',\n            '3-3-2-2-1-1-1-1-1-1',\n            '3-3-2-2-2-1-1-1-1-1',\n            '3-3-2-2-2-2-1-1-1-1',\n            '3-3-2-2-2-2-2-1-1-1',\n            '3-3-2-2-2-2-2-2-1-1',\n            '3-3-2-2-2-2-2-2-2-1',\n            '3-3-2-2-2-2-2-2-2-2',\n            '3-3-3-1-1-1-1-1-1-1',\n            '3-3-3-2-1-1-1-1-1-1',\n            '3-3-3-2-2-1-1-1-1-1',\n            '3-3-3-2-2-2-1-1-1-1',\n            '3-3-3-2-2-2-2-1-1-1',\n            '3-3-3-2-2-2-2-2-1-1',\n            '3-3-3-2-2-2-2-2-2-1',\n            '3-3-3-2-2-2-2-2-2-2',\n            '3-3-3-3-1-1-1-1-1-1',\n            '3-3-3-3-2-1-1-1-1-1',\n            '3-3-3-3-2-2-1-1-1-1',\n            '3-3-3-3-2-2-2-1-1-1',\n            '3-3-3-3-2-2-2-2-1-1',\n            '3-3-3-3-2-2-2-2-2-1',\n            '3-3-3-3-2-2-2-2-2-2',\n            '3-3-3-3-3-1-1-1-1-1',\n            '3-3-3-3-3-2-1-1-1-1',\n            '3-3-3-3-3-2-2-1-1-1',\n            '3-3-3-3-3-2-2-2-1-1',\n            '3-3-3-3-3-2-2-2-2-1',\n            '3-3-3-3-3-2-2-2-2-2',\n            '3-3-3-3-3-3-1-1-1-1',\n            '3-3-3-3-3-3-2-1-1-1',\n            '3-3-3-3-3-3-2-2-1-1',\n            '3-3-3-3-3-3-2-2-2-1',\n            '3-3-3-3-3-3-2-2-2-2',\n            '3-3-3-3-3-3-3-1-1-1',\n            '3-3-3-3-3-3-3-2-1-1',\n            '3-3-3-3-3-3-3-2-2-1',\n            '3-3-3-3-3-3-3-2-2-2',\n            '3-3-3-3-3-3-3-3-1-1',\n            '3-3-3-3-3-3-3-3-2-1',\n            '3-3-3-3-3-3-3-3-2-2',\n            '3-3-3-3-3-3-3-3-3-1',\n            '3-3-3-3-3-3-3-3-3-2',\n            '4-1-1-1-1-1-1-1-1-1',\n            '4-2-1-1-1-1-1-1-1-1',\n            '4-2-2-1-1-1-1-1-1-1',\n            '4-2-2-2-1-1-1-1-1-1',\n            '4-2-2-2-2-1-1-1-1-1',\n            '4-2-2-2-2-2-1-1-1-1',\n            '4-2-2-2-2-2-2-1-1-1',\n            '4-2-2-2-2-2-2-2-1-1',\n            '4-2-2-2-2-2-2-2-2-1',\n            '4-2-2-2-2-2-2-2-2-2',\n            '4-3-1-1-1-1-1-1-1-1',\n            '4-3-2-1-1-1-1-1-1-1',\n            '4-3-2-2-1-1-1-1-1-1',\n            '4-3-2-2-2-1-1-1-1-1',\n            '4-3-2-2-2-2-1-1-1-1',\n            '4-3-2-2-2-2-2-1-1-1',\n            '4-3-2-2-2-2-2-2-1-1',\n            '4-3-2-2-2-2-2-2-2-1',\n            '4-3-2-2-2-2-2-2-2-2',\n            '4-3-3-1-1-1-1-1-1-1',\n            '4-3-3-2-1-1-1-1-1-1',\n            '4-3-3-2-2-1-1-1-1-1',\n            '4-3-3-2-2-2-1-1-1-1',\n            '4-3-3-2-2-2-2-1-1-1',\n            '4-3-3-2-2-2-2-2-1-1',\n            '4-3-3-2-2-2-2-2-2-1',\n            '4-3-3-2-2-2-2-2-2-2',\n            '4-3-3-3-1-1-1-1-1-1',\n            '4-3-3-3-2-1-1-1-1-1',\n            '4-3-3-3-2-2-1-1-1-1',\n            '4-3-3-3-2-2-2-1-1-1',\n            '4-3-3-3-2-2-2-2-1-1',\n            '4-3-3-3-2-2-2-2-2-1',\n            '4-3-3-3-2-2-2-2-2-2',\n            '4-3-3-3-3-1-1-1-1-1',\n            '4-3-3-3-3-2-1-1-1-1',\n            '4-3-3-3-3-2-2-1-1-1',\n            '4-3-3-3-3-2-2-2-1-1',\n            '4-3-3-3-3-2-2-2-2-1',\n            '4-3-3-3-3-2-2-2-2-2',\n            '4-3-3-3-3-3-1-1-1-1',\n            '4-3-3-3-3-3-2-1-1-1',\n            '4-3-3-3-3-3-2-2-1-1',\n            '4-3-3-3-3-3-2-2-2-1',\n            '4-3-3-3-3-3-2-2-2-2',\n            '4-3-3-3-3-3-3-1-1-1',\n            '4-3-3-3-3-3-3-2-1-1',\n            '4-3-3-3-3-3-3-2-2-1',\n            '4-3-3-3-3-3-3-2-2-2',\n            '4-3-3-3-3-3-3-3-1-1',\n            '4-3-3-3-3-3-3-3-2-1',\n            '4-3-3-3-3-3-3-3-2-2',\n            '4-3-3-3-3-3-3-3-3-1',\n            '4-3-3-3-3-3-3-3-3-2',\n            '4-4-1-1-1-1-1-1-1-1',\n            '4-4-2-1-1-1-1-1-1-1',\n            '4-4-2-2-1-1-1-1-1-1',\n            '4-4-2-2-2-1-1-1-1-1',\n            '4-4-2-2-2-2-1-1-1-1',\n            '4-4-2-2-2-2-2-1-1-1',\n            '4-4-2-2-2-2-2-2-1-1',\n            '4-4-2-2-2-2-2-2-2-1',\n            '4-4-2-2-2-2-2-2-2-2',\n            '4-4-3-1-1-1-1-1-1-1',\n            '4-4-3-2-1-1-1-1-1-1',\n            '4-4-3-2-2-1-1-1-1-1',\n            '4-4-3-2-2-2-1-1-1-1',\n            '4-4-3-2-2-2-2-1-1-1',\n            '4-4-3-2-2-2-2-2-1-1',\n            '4-4-3-2-2-2-2-2-2-1',\n            '4-4-3-2-2-2-2-2-2-2',\n            '4-4-3-3-1-1-1-1-1-1',\n            '4-4-3-3-2-1-1-1-1-1',\n            '4-4-3-3-2-2-1-1-1-1',\n            '4-4-3-3-2-2-2-1-1-1',\n            '4-4-3-3-2-2-2-2-1-1',\n            '4-4-3-3-2-2-2-2-2-1',\n            '4-4-3-3-2-2-2-2-2-2',\n            '4-4-3-3-3-1-1-1-1-1',\n            '4-4-3-3-3-2-1-1-1-1',\n            '4-4-3-3-3-2-2-1-1-1',\n            '4-4-3-3-3-2-2-2-1-1',\n            '4-4-3-3-3-2-2-2-2-1',\n            '4-4-3-3-3-2-2-2-2-2',\n            '4-4-3-3-3-3-1-1-1-1',\n            '4-4-3-3-3-3-2-1-1-1',\n            '4-4-3-3-3-3-2-2-1-1',\n            '4-4-3-3-3-3-2-2-2-1',\n            '4-4-3-3-3-3-2-2-2-2',\n            '4-4-3-3-3-3-3-1-1-1',\n            '4-4-3-3-3-3-3-2-1-1',\n            '4-4-3-3-3-3-3-2-2-1',\n            '4-4-3-3-3-3-3-2-2-2',\n            '4-4-3-3-3-3-3-3-1-1',\n            '4-4-3-3-3-3-3-3-2-1',\n            '4-4-3-3-3-3-3-3-2-2',\n            '4-4-3-3-3-3-3-3-3-1',\n            '4-4-3-3-3-3-3-3-3-2',\n            '4-4-4-1-1-1-1-1-1-1',\n            '4-4-4-2-1-1-1-1-1-1',\n            '4-4-4-2-2-1-1-1-1-1',\n            '4-4-4-2-2-2-1-1-1-1',\n            '4-4-4-2-2-2-2-1-1-1',\n            '4-4-4-2-2-2-2-2-1-1',\n            '4-4-4-2-2-2-2-2-2-1',\n            '4-4-4-2-2-2-2-2-2-2',\n            '4-4-4-3-1-1-1-1-1-1',\n            '4-4-4-3-2-1-1-1-1-1',\n            '4-4-4-3-2-2-1-1-1-1',\n            '4-4-4-3-2-2-2-1-1-1',\n            '4-4-4-3-2-2-2-2-1-1',\n            '4-4-4-3-2-2-2-2-2-1',\n            '4-4-4-3-2-2-2-2-2-2',\n            '4-4-4-3-3-1-1-1-1-1',\n            '4-4-4-3-3-2-1-1-1-1',\n            '4-4-4-3-3-2-2-1-1-1',\n            '4-4-4-3-3-2-2-2-1-1',\n            '4-4-4-3-3-2-2-2-2-1',\n            '4-4-4-3-3-2-2-2-2-2',\n            '4-4-4-3-3-3-1-1-1-1',\n            '4-4-4-3-3-3-2-1-1-1',\n            '4-4-4-3-3-3-2-2-1-1',\n            '4-4-4-3-3-3-2-2-2-1',\n            '4-4-4-3-3-3-2-2-2-2',\n            '4-4-4-3-3-3-3-1-1-1',\n            '4-4-4-3-3-3-3-2-1-1',\n            '4-4-4-3-3-3-3-2-2-1',\n            '4-4-4-3-3-3-3-2-2-2',\n            '4-4-4-3-3-3-3-3-1-1',\n            '4-4-4-3-3-3-3-3-2-1',\n            '4-4-4-3-3-3-3-3-2-2',\n            '4-4-4-3-3-3-3-3-3-1',\n            '4-4-4-3-3-3-3-3-3-2',\n            '4-4-4-4-1-1-1-1-1-1',\n            '4-4-4-4-2-1-1-1-1-1',\n            '4-4-4-4-2-2-1-1-1-1',\n            '4-4-4-4-2-2-2-1-1-1',\n            '4-4-4-4-2-2-2-2-1-1',\n            '4-4-4-4-2-2-2-2-2-1',\n            '4-4-4-4-2-2-2-2-2-2',\n            '4-4-4-4-3-1-1-1-1-1',\n            '4-4-4-4-3-2-1-1-1-1',\n            '4-4-4-4-3-2-2-1-1-1',\n            '4-4-4-4-3-2-2-2-1-1',\n            '4-4-4-4-3-2-2-2-2-1',\n            '4-4-4-4-3-2-2-2-2-2',\n            '4-4-4-4-3-3-1-1-1-1',\n            '4-4-4-4-3-3-2-1-1-1',\n            '4-4-4-4-3-3-2-2-1-1',\n            '4-4-4-4-3-3-2-2-2-1',\n            '4-4-4-4-3-3-2-2-2-2',\n            '4-4-4-4-3-3-3-1-1-1',\n            '4-4-4-4-3-3-3-2-1-1',\n            '4-4-4-4-3-3-3-2-2-1',\n            '4-4-4-4-3-3-3-2-2-2',\n            '4-4-4-4-3-3-3-3-1-1',\n            '4-4-4-4-3-3-3-3-2-1',\n            '4-4-4-4-3-3-3-3-2-2',\n            '4-4-4-4-3-3-3-3-3-1',\n            '4-4-4-4-3-3-3-3-3-2',\n            '4-4-4-4-4-1-1-1-1-1',\n            '4-4-4-4-4-2-1-1-1-1',\n            '4-4-4-4-4-2-2-1-1-1',\n            '4-4-4-4-4-2-2-2-1-1',\n            '4-4-4-4-4-2-2-2-2-1',\n            '4-4-4-4-4-2-2-2-2-2',\n            '4-4-4-4-4-3-1-1-1-1',\n            '4-4-4-4-4-3-2-1-1-1',\n            '4-4-4-4-4-3-2-2-1-1',\n            '4-4-4-4-4-3-2-2-2-1',\n            '4-4-4-4-4-3-2-2-2-2',\n            '4-4-4-4-4-3-3-1-1-1',\n            '4-4-4-4-4-3-3-2-1-1',\n            '4-4-4-4-4-3-3-2-2-1',\n            '4-4-4-4-4-3-3-2-2-2',\n            '4-4-4-4-4-3-3-3-1-1',\n            '4-4-4-4-4-3-3-3-2-1',\n            '4-4-4-4-4-3-3-3-2-2',\n            '4-4-4-4-4-3-3-3-3-1',\n            '4-4-4-4-4-3-3-3-3-2',\n            '4-4-4-4-4-4-1-1-1-1',\n            '4-4-4-4-4-4-2-1-1-1',\n            '4-4-4-4-4-4-2-2-1-1',\n            '4-4-4-4-4-4-2-2-2-1',\n            '4-4-4-4-4-4-2-2-2-2',\n            '4-4-4-4-4-4-3-1-1-1',\n            '4-4-4-4-4-4-3-2-1-1',\n            '4-4-4-4-4-4-3-2-2-1',\n            '4-4-4-4-4-4-3-2-2-2',\n            '4-4-4-4-4-4-3-3-1-1',\n            '4-4-4-4-4-4-3-3-2-1',\n            '4-4-4-4-4-4-3-3-2-2',\n            '4-4-4-4-4-4-3-3-3-1',\n            '4-4-4-4-4-4-3-3-3-2',\n            '4-4-4-4-4-4-4-1-1-1',\n            '4-4-4-4-4-4-4-2-1-1',\n            '4-4-4-4-4-4-4-2-2-1',\n            '4-4-4-4-4-4-4-2-2-2',\n            '4-4-4-4-4-4-4-3-1-1',\n            '4-4-4-4-4-4-4-3-2-1',\n            '4-4-4-4-4-4-4-3-2-2',\n            '4-4-4-4-4-4-4-3-3-1',\n            '4-4-4-4-4-4-4-3-3-2',\n            '4-4-4-4-4-4-4-4-1-1',\n            '4-4-4-4-4-4-4-4-2-1',\n            '4-4-4-4-4-4-4-4-2-2',\n            '4-4-4-4-4-4-4-4-3-1',\n            '4-4-4-4-4-4-4-4-3-2',\n            '4-4-4-4-4-4-4-4-4-1',\n            '4-4-4-4-4-4-4-4-4-2']\nfastcore.test.test_eq(sorted(profiles), expected)"
  },
  {
    "objectID": "model_utils.html#profile-filters",
    "href": "model_utils.html#profile-filters",
    "title": "Model Utilities",
    "section": "Profile filters",
    "text": "Profile filters\nThe profile filters work out which strategy profiles are relevant to the game in question. I already pass in two filters by default which keep only the strategy profiles which are relevant to the transition the algorithm is curently calculating the fixation probability for, and are possible given the allowed_sectors for the players.\nAdditional filters could be defined and passed in when desired (e.g. a filter to keep only strategy profiles which are unique to rearrangement)\nTechnical note: A similar effect could be achieved by having a sampling rule which returned 0 as the likelihood for irrelevant or impossible strategy profiles. However, it usually makes sense to keep these restrictions separate: profile filters can be reused for many possible games, but sampling rules may vary more often. There is also a slight performance advantage to having a profile filter, since it reduces the number of times the sampling rule must be called.\n\nProfile filter methods\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\nWe also need a filter which yields the relevant profiles for each transition considered.\nGiven two states and their transition we first check that it is valid, and if we so we know the sector affected. Only that sector may choose different strategies, so the others are fixed.\nNote that the recurrent states of the game describes each strategy employed by each sector. This is written in the same form as we write the strategy profile, “{strategy_code}-{strategy_code}-{strategy_code}” and uses the same codes.\nNote: Below, my filter only keeps strategies which are relevant to the two recurrent states relevant to the transition. If we need additional strategies, this filter will not be sufficient (this might be the case if we allow a social learning rule which explores more than one mutant strategy at a time)\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\nI have also written a function for applying multiple filters.\n\nsource\n\n\napply_profile_filters\n\n apply_profile_filters (models)\n\nApply all profile filters listed in profile_filters in models.\n\nTests for profile_filter\n\n\nTest 1\nLet’s test the \"allowed_sectors\" filter rule.\nFirst I test a game with 3 players but only two sectors.\nEach player is fixed to a specific sector, but two players belong to the same sector.\nIn this case the rule should filter to only those profiles where players use\nthe strategies available to the sectors they can play as.\nI then test a 3 player game with 3 sectors. Each player is fixed to a particular sector.\n\n\nTest 2\nNow, let’s test the \"relevant_to_transition\" filter rule.\nFirst I test a game with 3 players but only two populations.\nIn this case the rule should filter to only the relevant profiles where strategy 2 is missing.\nRecall that strategy 2 is the first strategy available to a player from sector 2.\nI then test a larger game with 3 players and 3 sectors. The list is long, so I only check that the number of profiles kept is what we expect.\n\n\nTest 3\nI now test the \"allowed_sectors\" and “relevant_to_transition” rules when used together.\nThe game is as before with 3 players and 3 sectors. Here, we can check that the list of profiles is as expected.\nThe order we apply these two filter rules should not matter.\nWe can use the apply_profile_filters function to achieve the same result.\nLet’s also check the eariler game with 2 sectors and 3 players.\nNotice that in this game, transitions which affect the sector with multiple players require us to look at more profiles.\n\n\n\nAn additional method for create_profiles which depends on profile_filter\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\nHere is a quick test of the “allowed_sectors” method\n\nallowed_sectors = {\"P1\": [\"S1\"],\n                   \"P2\": [\"S1\", \"S2\"], }\n# Third sector irrelevant given allowed sectors\nsector_strategies = {\"S1\": [\"1\", \"2\"],\n                     \"S2\": [\"3\", \"4\"],\n                     \"S3\": [\"5\", \"6\"]}\nmodels = {\"allowed_sectors\": allowed_sectors,\n          \"sector_strategies\": sector_strategies,\n          \"profiles_rule\": \"allowed_sectors\"}\nprofiles = create_profiles(models)['profiles']\nexpected = ['1-1', \"1-2\", '2-1', '2-2', '3-1', '3-2', '4-1', '4-2']\nfastcore.test.test_eq(sorted(profiles), expected)\n\n/tmp/ipykernel_3459/3464752654.py:24: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if (ind not in allowed_inds) and (str(ind) not in allowed_inds):"
  },
  {
    "objectID": "plots_utils.html",
    "href": "plots_utils.html",
    "title": "Plot utilities",
    "section": "",
    "text": "source\n\nplot_strategy_distribution\n\n plot_strategy_distribution (data, strategy_set, x='pr', x_label='Risk of\n                             an AI disaster, pr', title='Strategy\n                             distribution')\n\nPlot the strategy distribution as we vary x.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nThe dataset containing data on parameters and the strategy distribution\n\n\nstrategy_set\n\n\nThe strategies to plot from the dataset\n\n\nx\nstr\npr\nThe parameter to place on the x-axis of the plot\n\n\nx_label\nstr\nRisk of an AI disaster, pr\nthe x-axis label\n\n\ntitle\nstr\nStrategy distribution\nthe plot title\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nplot_heatmap\n\n plot_heatmap (table, figure_object=None, xlabel='x', ylabel='y',\n               zlabel='z', cmap='inferno', zmin=0, zmax=1)\n\nPlot heatmap using the index, columns, and values from table.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntable\n\n\nA pivot table, created using pandas.pivot function\n\n\nfigure_object\nNoneType\nNone\n\n\n\nxlabel\nstr\nx\n\n\n\nylabel\nstr\ny\n\n\n\nzlabel\nstr\nz\n\n\n\ncmap\nstr\ninferno\n\n\n\nzmin\nint\n0\n\n\n\nzmax\nint\n1\n\n\n\n\n3D quiver plot\n\nax = plt.figure().add_subplot(projection='3d')\n\n# Make the grid\nx, y, z = np.meshgrid(np.arange(0, 1, 0.15),\n                      np.arange(0, 1, 0.15),\n                      np.arange(0, 1, 0.15))\n\n# Make the direction data for the arrows\nu = np.sin(np.pi * x) * np.cos(np.pi * y) * np.cos(np.pi * z)\nv = -np.cos(np.pi * x) * np.sin(np.pi * y) * np.cos(np.pi * z)\nw = (np.sqrt(2.0 / 3.0) * np.cos(np.pi * x) * np.cos(np.pi * y) *\n     np.sin(np.pi * z))\n\nax.quiver(x, y, z, u, v, w, length=0.1, normalize=True)\nplt.show()\n\n\n\n\n\ndf = pandas.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/vortex.csv\")\n\nfig = go.Figure(data = go.Cone(\n    x=df['x'],\n    y=df['y'],\n    z=df['z'],\n    u=df['u'],\n    v=df['v'],\n    w=df['w'],\n    colorscale='Blues',\n    sizemode=\"absolute\",\n    sizeref=40))\n\nfig.update_layout(scene=dict(aspectratio=dict(x=1, y=1, z=0.8),\n                             camera_eye=dict(x=1.2, y=1.2, z=0.6)))\n\nfig.show()\n\n\n                                                \n\n\n\ndef payoffs_encanacao_2016(models):\n    names = ['b_r', 'b_s', 'c_s', 'c_t', 'σ']\n    b_r, b_s, c_s, c_t, σ = [models[k] for k in names]\n    payoffs = {}\n    n_players = 3\n    n_sectors = 3\n    n_strategies_per_sector = [2, 2, 2]\n    n_strategies_total = 6\n    index_min = \"0-0-0\" # All players are from the first sector, playing that sector's first strategy\n    index_max = \"5-5-5\" # All players are from the third sector, playing that sector's second strategy\n    # Note: The seperator makes it easy to represent games where n_strategies_total >= 10.\n    \n    # It is also trivial to define a vector which maps these indexes to strategy profiles\n    # As sector order is fixed we could neglect to mention suscripts for each sector\n    strategy_names = [\"D\", \"C\", \"D\", \"C\", \"D\", \"C\"]\n    \n    zero = np.zeros(b_r.shape[0])\n    # As in the main text\n    payoffs[\"C-C-C\"] = {\"P3\": b_r-2*c_s,\n                        \"P2\": σ+b_s-c_t,\n                        \"P1\": σ+b_s}\n    payoffs[\"C-C-D\"] = {\"P3\": -c_s,\n                        \"P2\": b_s-c_t,\n                        \"P1\": zero}\n    payoffs[\"C-D-C\"] = {\"P3\": b_r-c_s,\n                        \"P2\": zero,\n                        \"P1\": b_s}\n    payoffs[\"C-D-D\"] = {\"P3\": zero,\n                        \"P2\": σ,\n                        \"P1\": σ}\n    payoffs[\"D-C-C\"] = {\"P3\": zero,\n                        \"P2\": σ-c_t,\n                        \"P1\": σ}\n    payoffs[\"D-C-D\"] = {\"P3\": zero,\n                        \"P2\": -c_t,\n                        \"P1\": zero}\n    payoffs[\"D-D-C\"] = {\"P3\": zero,\n                        \"P2\": zero,\n                        \"P1\": zero}\n    payoffs[\"D-D-D\"] = {\"P3\": zero,\n                        \"P2\": σ,\n                        \"P1\": σ}\n    \n    # The following indexes capture all strategy profiles where each player is fixed to a unique sector\n    # (and player order does not matter, so we need only consider one ordering of sectors).\n    payoffs[\"4-2-0\"] = payoffs[\"D-D-D\"]\n    payoffs[\"4-2-1\"] = payoffs[\"D-D-C\"]\n    payoffs[\"4-3-0\"] = payoffs[\"D-C-D\"]\n    payoffs[\"4-3-1\"] = payoffs[\"D-C-C\"]\n    payoffs[\"5-2-0\"] = payoffs[\"C-D-D\"]\n    payoffs[\"5-2-1\"] = payoffs[\"C-D-C\"]\n    payoffs[\"5-3-0\"] = payoffs[\"C-C-D\"]\n    payoffs[\"5-3-1\"] = payoffs[\"C-C-C\"]\n    return {**models, \"payoffs\": payoffs}\n\n\ndef gos_encanacao_2016(models):\n    payoffs = models['payoffs']\n    x,y,z = models['current_state']\n    Z1, Z2, Z3 = models['Z']\n    β = models['β']\n    μ = models['μ']\n    fitness_C1 = (y*z*payoffs[\"C-C-C\"][\"P1\"]\n                  + (1-y)*z*payoffs[\"C-D-C\"][\"P1\"]\n                  + y*(1-z)*payoffs[\"D-C-C\"][\"P1\"]\n                  + (1-y)*(1-z)*payoffs[\"D-D-C\"][\"P1\"])\n    fitness_D1 = (y*z*payoffs[\"C-C-D\"][\"P1\"]\n                  + (1-y)*z*payoffs[\"D-D-D\"][\"P1\"]\n                  + y*(1-z)*payoffs[\"D-C-D\"][\"P1\"]\n                  + (1-y)*(1-z)*payoffs[\"D-D-D\"][\"P1\"])\n    fitness_C2 = (x*z*payoffs[\"C-C-C\"][\"P2\"]\n                  + (1-x)*z*payoffs[\"C-C-D\"][\"P2\"]\n                  + x*(1-z)*payoffs[\"D-C-C\"][\"P2\"]\n                  + (1-x)*(1-z)*payoffs[\"D-C-D\"][\"P2\"])\n    fitness_D2 = (x*z*payoffs[\"C-D-C\"][\"P2\"]\n                  + (1-x)*z*payoffs[\"C-D-D\"][\"P2\"]\n                  + x*(1-z)*payoffs[\"D-D-C\"][\"P2\"]\n                  + (1-x)*(1-z)*payoffs[\"D-D-D\"][\"P2\"]) \n    fitness_C3 = (x*y*payoffs[\"C-C-C\"][\"P3\"]\n                  + (1-x)*y*payoffs[\"C-C-D\"][\"P3\"]\n                  + x*(1-y)*payoffs[\"C-D-C\"][\"P3\"]\n                  + (1-x)*(1-y)*payoffs[\"C-D-D\"][\"P3\"])\n    fitness_D3 = (x*y*payoffs[\"D-C-C\"][\"P3\"]\n                  + (1-x)*y*payoffs[\"D-C-D\"][\"P3\"]\n                  + x*(1-y)*payoffs[\"D-D-C\"][\"P3\"]\n                  + (1-x)*(1-y)*payoffs[\"D-D-D\"][\"P3\"])\n    \n    g1 = (np.tanh(β*(fitness_C1 - fitness_D1)/2) * (1 - x) * x\n          + μ * (1 - 2*x))\n    g2 = (np.tanh(β*(fitness_C2 - fitness_D2)/2) * (1 - y) * y\n          + μ * (1 - 2*y))\n    g3 = (np.tanh(β*(fitness_C3 - fitness_D3)/2) * (1 - z) * z\n          + μ * (1 - 2*z))\n    models['u'] = g1\n    models['v'] = g2\n    models['w'] = g3\n    models['gos'] = [g1, g2, g3]\n    return models\n\n\nmodels = model_builder({\"b_r\": 0.8,\n               \"b_s\": 0.4,\n               \"c_s\": 0.15,\n               \"c_t\": 0.15,\n               \"σ\": 0.2,\n               \"x\": np.arange(0, 1, 0.2),\n               \"y\": np.arange(0, 1, 0.2),\n               \"z\": np.arange(0, 1, 0.2),})\nmodels['current_state'] = [models['x'], models['y'], models['z']]\nmodels = {**models, \"Z\": [50, 50, 50], \"β\": 2.5, \"μ\": 0.02}\n\n\nresults =  thread_macro(models,\n                        payoffs_encanacao_2016,\n                        gos_encanacao_2016,\n                       )\n\n\nresults['gos']\n\n[array([ 0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n         0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n         0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n         0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n         0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n        -0.02718699, -0.01182161,  0.00400666,  0.01999334,  0.03582161,\n        -0.01182161,  0.00400666,  0.01999334,  0.03582161,  0.05118699,\n         0.00400666,  0.01999334,  0.03582161,  0.05118699,  0.06582009,\n         0.01999334,  0.03582161,  0.05118699,  0.06582009,  0.07950384,\n         0.03582161,  0.05118699,  0.06582009,  0.07950384,  0.09208323,\n        -0.05478048, -0.03173241, -0.00799001,  0.01599001,  0.03973241,\n        -0.03173241, -0.00799001,  0.01599001,  0.03973241,  0.06278048,\n        -0.00799001,  0.01599001,  0.03973241,  0.06278048,  0.08473013,\n         0.01599001,  0.03973241,  0.06278048,  0.08473013,  0.10525576,\n         0.03973241,  0.06278048,  0.08473013,  0.10525576,  0.12412485,\n        -0.06278048, -0.03973241, -0.01599001,  0.00799001,  0.03173241,\n        -0.03973241, -0.01599001,  0.00799001,  0.03173241,  0.05478048,\n        -0.01599001,  0.00799001,  0.03173241,  0.05478048,  0.07673013,\n         0.00799001,  0.03173241,  0.05478048,  0.07673013,  0.09725576,\n         0.03173241,  0.05478048,  0.07673013,  0.09725576,  0.11612485,\n        -0.05118699, -0.03582161, -0.01999334, -0.00400666,  0.01182161,\n        -0.03582161, -0.01999334, -0.00400666,  0.01182161,  0.02718699,\n        -0.01999334, -0.00400666,  0.01182161,  0.02718699,  0.04182009,\n        -0.00400666,  0.01182161,  0.02718699,  0.04182009,  0.05550384,\n         0.01182161,  0.02718699,  0.04182009,  0.05550384,  0.06808323]),\n array([ 0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n        -0.05385121, -0.04003899, -0.02530128, -0.00986239,  0.00600281,\n        -0.09477681, -0.07405848, -0.05195192, -0.02879359, -0.00499578,\n        -0.10277681, -0.08205848, -0.05995192, -0.03679359, -0.01299578,\n        -0.07785121, -0.06403899, -0.04930128, -0.03386239, -0.01799719,\n         0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n        -0.04003899, -0.02530128, -0.00986239,  0.00600281,  0.021987  ,\n        -0.07405848, -0.05195192, -0.02879359, -0.00499578,  0.0189805 ,\n        -0.08205848, -0.05995192, -0.03679359, -0.01299578,  0.0109805 ,\n        -0.06403899, -0.04930128, -0.03386239, -0.01799719, -0.002013  ,\n         0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n        -0.02530128, -0.00986239,  0.00600281,  0.021987  ,  0.03777354,\n        -0.05195192, -0.02879359, -0.00499578,  0.0189805 ,  0.04266031,\n        -0.05995192, -0.03679359, -0.01299578,  0.0109805 ,  0.03466031,\n        -0.04930128, -0.03386239, -0.01799719, -0.002013  ,  0.01377354,\n         0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n        -0.00986239,  0.00600281,  0.021987  ,  0.03777354,  0.05306118,\n        -0.02879359, -0.00499578,  0.0189805 ,  0.04266031,  0.06559177,\n        -0.03679359, -0.01299578,  0.0109805 ,  0.03466031,  0.05759177,\n        -0.03386239, -0.01799719, -0.002013  ,  0.01377354,  0.02906118,\n         0.02      ,  0.02      ,  0.02      ,  0.02      ,  0.02      ,\n         0.00600281,  0.021987  ,  0.03777354,  0.05306118,  0.06758627,\n        -0.00499578,  0.0189805 ,  0.04266031,  0.06559177,  0.08737941,\n        -0.01299578,  0.0109805 ,  0.03466031,  0.05759177,  0.07937941,\n        -0.01799719, -0.002013  ,  0.01377354,  0.02906118,  0.04358627]),\n array([ 2.00000000e-02,  1.20000000e-02,  4.00000000e-03, -4.00000000e-03,\n        -1.20000000e-02,  2.00000000e-02,  6.00281092e-03, -4.99578362e-03,\n        -1.29957836e-02, -1.79971891e-02,  2.00000000e-02,  2.24494900e-05,\n        -1.39663258e-02, -2.19663258e-02, -2.39775505e-02,  2.00000000e-02,\n        -5.92444497e-03, -2.28866675e-02, -3.08866675e-02, -2.99244450e-02,\n         2.00000000e-02, -1.18216054e-02, -3.17324081e-02, -3.97324081e-02,\n        -3.58216054e-02,  2.00000000e-02,  3.77735375e-02,  4.26603063e-02,\n         3.46603063e-02,  1.37735375e-02,  2.00000000e-02,  3.18964803e-02,\n         3.38447204e-02,  2.58447204e-02,  7.89648028e-03,  2.00000000e-02,\n         2.59643799e-02,  2.49465699e-02,  1.69465699e-02,  1.96437992e-03,\n         2.00000000e-02,  1.99933400e-02,  1.59900100e-02,  7.99000999e-03,\n        -4.00666001e-03,  2.00000000e-02,  1.39998958e-02,  6.99984376e-03,\n        -1.00015624e-03, -1.00001042e-02,  2.00000000e-02,  6.22433481e-02,\n         7.93650221e-02,  7.13650221e-02,  3.82433481e-02,  2.00000000e-02,\n         5.67731510e-02,  7.11597264e-02,  6.31597264e-02,  3.27731510e-02,\n         2.00000000e-02,  5.11869860e-02,  6.27804790e-02,  5.47804790e-02,\n         2.71869860e-02,  2.00000000e-02,  4.54973071e-02,  5.42459607e-02,\n         4.62459607e-02,  2.14973071e-02,  2.00000000e-02,  3.97176253e-02,\n         4.55764379e-02,  3.75764379e-02,  1.57176253e-02,  2.00000000e-02,\n         8.43567940e-02,  1.12535191e-01,  1.04535191e-01,  6.03567940e-02,\n         2.00000000e-02,  7.95038408e-02,  1.05255761e-01,  9.72557613e-02,\n         5.55038408e-02,  2.00000000e-02,  7.44949342e-02,  9.77424013e-02,\n         8.97424013e-02,  5.04949342e-02,  2.00000000e-02,  6.93371837e-02,\n         9.00057756e-02,  8.20057756e-02,  4.53371837e-02,  2.00000000e-02,\n         6.40389879e-02,  8.20584818e-02,  7.40584818e-02,  4.00389879e-02,\n         2.00000000e-02,  1.03467195e-01,  1.41200792e-01,  1.33200792e-01,\n         7.94671946e-02,  2.00000000e-02,  9.93415230e-02,  1.35012285e-01,\n         1.27012285e-01,  7.53415230e-02,  2.00000000e-02,  9.50434934e-02,\n         1.28565240e-01,  1.20565240e-01,  7.10434934e-02,  2.00000000e-02,\n         9.05749165e-02,  1.21862375e-01,  1.13862375e-01,  6.65749165e-02,\n         2.00000000e-02,  8.59387452e-02,  1.14908118e-01,  1.06908118e-01,\n         6.19387452e-02])]\n\n\n\nnp.arange(0, 1, 0.2)\n\narray([0. , 0.2, 0.4, 0.6, 0.8])\n\n\n\nfig = go.Figure(data = go.Cone(\n    x=results['x'],\n    y=results['y'],\n    z=results['z'],\n    u=results['u'],\n    v=results['v'],\n    w=results['w'],\n    # colorscale='Reds',\n    # sizemode=\"absolute\",\n    # sizeref=40\n))\n\nfig.update_layout(\n    \n    scene=dict(aspectratio=dict(x=1, y=1, z=0.8),\n               camera_eye=dict(x=1.2, y=1.2, z=0.6)),\n    height=800,\n)\n\nfig.show()\n\n\n                                                \n\n\n\ndf = pandas.DataFrame({k:v for k,v in results.items() if k in [\"x\", \"y\", \"z\", \"u\", \"v\", \"w\"]})\n\n\nax = plt.figure().add_subplot(projection='3d')\n\n\nx = results['x'].reshape((5, 5, 5))\ny = results['y'].reshape((5, 5, 5))\nz = results['z'].reshape((5, 5, 5))\nu = results['u'].reshape((5, 5, 5))\nv = results['v'].reshape((5, 5, 5))\nw = results['w'].reshape((5, 5, 5))\n\nax.quiver(x, y, z, u, v, w, length=0.1, normalize=True)\nplt.show()"
  },
  {
    "objectID": "data_utils.html",
    "href": "data_utils.html",
    "title": "Data utilities",
    "section": "",
    "text": "source\n\nresults_to_dataframe_egt\n\n results_to_dataframe_egt (results:dict, suppress:bool=True)\n\nConvert results to a dataframe, keeping only items which are valid for a dataframe to have.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nresults\ndict\n\nA dictionary containing items from ModelTypeEGT.\n\n\nsuppress\nbool\nTrue\nSupress the dropped items warning\n\n\n\n\nsource\n\n\nprocess_dsair_data\n\n process_dsair_data (data)\n\nProcess DSAIR model results dataframe."
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utilities",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "utils.html#clojure-multimethods-in-python",
    "href": "utils.html#clojure-multimethods-in-python",
    "title": "Utilities",
    "section": "Clojure multimethods in python",
    "text": "Clojure multimethods in python\nDisclaimer: Unlike the code above, this code is not my invention. All credit goes to Adam Bard for coming up with this (and Guido for writing an earlier implementation). Adam Bard made this code freely available at https://adambard.com/blog/implementing-multimethods-in-python/.\nAlternatively, this package exists but only works based on type hints: https://pypi.org/project/multimethod/#description. In my opinion, the clojure dispatch function approach is far more versatile. Big thanks to Adam Bard for implementing this.\n\nsource\n\nmulti\n\n multi (dispatch_fn)\n\n\nsource\n\n\nmethod\n\n method (dispatch_fn, dispatch_key=None)\n\n\n\nExamples and tests\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)\n\n\n\n\nmulti.._inner\n\n multi.<locals>._inner (*args, **kwargs)"
  },
  {
    "objectID": "utils.html#string_to_tuple-utility",
    "href": "utils.html#string_to_tuple-utility",
    "title": "Utilities",
    "section": "string_to_tuple utility",
    "text": "string_to_tuple utility\n\nsource\n\nstring_to_tuple\n\n string_to_tuple (string)\n\nConvert a string containing only integers and dashes to a tuple of integers in reverse order."
  },
  {
    "objectID": "types.html",
    "href": "types.html",
    "title": "Types",
    "section": "",
    "text": "source\n\nArray1D\n\n Array1D (ModelVector:nptyping.base_meta_classes.NDArray)\n\nAn alias for a 1D numpy array.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nModelVector\nNDArray\nA 1D numpy array suitable for stacks of scalar parameter values\n\n\n\n\nsource\n\n\nArray2D\n\n Array2D (Model2DArray:nptyping.base_meta_classes.NDArray)\n\nAn alias for a 2D numpy array.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nModel2DArray\nNDArray\nA 2D numpy array suitable for stacks of state vectors\n\n\n\n\nsource\n\n\nArray3D\n\n Array3D (Model3DArray:nptyping.base_meta_classes.NDArray)\n\nAn alias for 3D numpy array, last two dimensions of equal size.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nModel3DArray\nNDArray\nA 3D numpy array suitable for stacks of payoff or transition matrices\n\n\n\n\nsource\n\n\nModelTypeDSAIR\n\n ModelTypeDSAIR (b:__main__.Array1D, c:__main__.Array1D,\n                 s:__main__.Array1D, p:__main__.Array1D,\n                 B:__main__.Array1D, W:__main__.Array1D,\n                 pfo:__main__.Array1D=None, α:__main__.Array1D=None,\n                 γ:__main__.Array1D=None, epsilon:__main__.Array1D=None,\n                 ω:__main__.Array1D=None,\n                 collective_risk:__main__.Array1D=None)\n\nThis is the schema for the inputs to a DSAIR model.\nNote: This schema is not enforced and is here purely for documentation purposes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\nArray1D\n\nbenefit: The size of the per round benefit of leading the AI development race, b>0\n\n\nc\nArray1D\n\ncost: The cost of implementing safety recommendations per round, c>0\n\n\ns\nArray1D\n\nspeed: The speed advantage from choosing to ignore safety recommendations, s>1\n\n\np\nArray1D\n\navoid_risk: The probability that unsafe firms avoid an AI disaster, p ∈ [0, 1]\n\n\nB\nArray1D\n\nprize: The size of the prize from winning the AI development race, B>>b\n\n\nW\nArray1D\n\ntimeline: The anticipated timeline until the development race has a winner if everyone behaves safely, W ∈ [10, 10**6]\n\n\npfo\nArray1D\nNone\ndetection risk: The probability that firms who ignore safety precautions are found out, pfo ∈ [0, 1]\n\n\nα\nArray1D\nNone\nthe cost of rewarding/punishing a peer\n\n\nγ\nArray1D\nNone\nthe effect of a reward/punishment on a developer’s speed\n\n\nepsilon\nArray1D\nNone\ncommitment_cost: The cost of setting up and maintaining a voluntary commitment, ϵ > 0\n\n\nω\nArray1D\nNone\nnoise: Noise in arranging an agreement, with some probability they fail to succeed in making an agreement, ω ∈ [0, 1]\n\n\ncollective_risk\nArray1D\nNone\nThe likelihood that a disaster affects all actors"
  }
]