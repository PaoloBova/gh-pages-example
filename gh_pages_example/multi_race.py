# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/Analysis/17_multi_race.ipynb.

# %% auto 0
__all__ = ['market_share', 'market_value', 'catch_up_fn', 'build_payoffs_multi_race_v1', 'build_payoffs']

# %% ../nbs/Analysis/17_multi_race.ipynb 3
from .conditions import *
from .data_utils import *
from .methods import *
from .models import *
from .model_utils import *
from .payoffs import *
from .plot_utils import *
from .types import *
from .utils import *

import copy
import typing
import warnings

import chaospy
import fastcore.test
import ipywidgets
from ipywidgets import interact
import matplotlib as mpl
import matplotlib.pyplot as plt
from nbdev.showdoc import *
import nptyping
import numpy as np
import pandas
import plotly.express as px
import plotly.graph_objects as go
import scipy
import seaborn as sns
import tqdm

# %% ../nbs/Analysis/17_multi_race.ipynb 6
def market_share(t_1, t_2, gamma):
   """Compute market share of the player who arrives at the discovery at time `t_1`."""
   dt = np.abs(t_1 - t_2)
   market_shares = np.where(t_2 <= t_1,
                   0.5*gamma**dt,
                   1 - 0.5*gamma**dt)
   return market_shares
def market_value(b, delta, t, w):
   """Compute the net present value of the market with monetary value `b` to a
   player who enters the market at time `t` given their discount rate, `delta`.
   
   A negative delta is nonsensical so we simply assume a very large benefit
   no matter the value of t. Likewise if delta=1, instead of assuming infinite
   payoffs, we assume a large value for the prize, capped at b*100"""
   return np.where((0 < delta) & (delta < 1),
                  #  b * delta**t / (1 - delta),
                  #  100 * b,
                  w / t * b,
                  w / t * b)
def catch_up_fn(t, w, s_p1, s_p2):
   """Compute the spillover adjusted arrival time if a player from the other
   layer makes a breakthrough at time `t`."""
   return np.where(s_p2 > 0,
                   (t + ((w - t * s_p1) / s_p2)),
                   1e100)

def build_payoffs_multi_race_v1(models):
   """ 
   Builds the payoffs for the multi race model of AI development.
   
   Note: This model assumes only two layers interact with one another at a time.
   Currently, we've hardcoded these parameters, but in future we could refactor
   the parameters to be 2d arrays instead of 1d arrays and use them as needed.

   Note: The interaction should only be between two layers at a time.
   More complex interactions should be decomposed into simpler ones.
   """
   # Extract the parameters from the models dictionary.
   names1 = ["B_1", "B_2", "s_1", "s_2", "p_1", "p_2", "p_both"]
   names2 = [ "W_1", "W_2", "gamma_1", "gamma_2", "delta_1", "delta_2", "alpha_1", "alpha_2"]
   B_1, B_2, s_1, s_2, p_1, p_2, p_both = [models[k] for k in names1]
   W_1, W_2, gamma_1, gamma_2, delta_1, delta_2, alpha_1, alpha_2  = [models[k] for k in names2]
   profile = models["profile"]
   strategy_profile = [int(a) for a in profile.split("-")][::-1]
   M = np.shape(B_1)[0]
   N = len(strategy_profile)
   
   # The following collections of parameters are specified per strategy. 
   # Each layer allows the following strategies:
   # safe, unsafe, safe_phase_1, safe_phase_2
   mapping_s_to_irt = [W_1, W_1/s_1, W_1, W_1/s_1,
                       W_2, W_2/s_2, W_2, W_2/s_2] # short for mapping_strategy_to_isolated_research_time
   ones = np.ones(M)
   mapping_s_to_phase1_speed = [ones, s_1, ones, s_1,
                                ones, s_2, ones, s_2]
   mapping_s_to_phase2_speed = [alpha_1, alpha_1 * s_1, alpha_1 * s_1, alpha_1,
                                alpha_2, alpha_2 * s_2, alpha_2 * s_2, alpha_2]
   maping_s_to_w = [W_1, W_1, W_1, W_1,
                    W_2, W_2, W_2, W_2]
   mapping_s_to_gamma = [gamma_1, gamma_1, gamma_1, gamma_1,
                         gamma_2, gamma_2, gamma_2, gamma_2]
   mapping_s_to_delta = [delta_1, delta_1, delta_1, delta_1,
                         delta_2, delta_2, delta_2, delta_2]
   mapping_s_to_b = [B_1, B_1, B_1, B_1,
                     B_2, B_2, B_2, B_2]
   mapping_s_to_alpha = [alpha_1, alpha_1, alpha_1, alpha_1,
                         alpha_2, alpha_2, alpha_2, alpha_2]
   
   # The following collection of parameters are specified per player
   # We usually do this to make broadcasting our functions over arrays easier
   isolated_research_times = np.array([mapping_s_to_irt[a - 1] for a in strategy_profile]).T
   phase_1_speeds = np.array([mapping_s_to_phase1_speed[a - 1] for a in strategy_profile]).T
   phase_2_speeds = np.array([mapping_s_to_phase2_speed[a - 1] for a in strategy_profile]).T
   research_distances = np.array([maping_s_to_w[a - 1] for a in strategy_profile]).T
   contestability_rates = np.array([mapping_s_to_gamma[a - 1] for a in strategy_profile]).T
   discount_rates = np.array([mapping_s_to_delta[a - 1] for a in strategy_profile]).T
   benefits = np.array([mapping_s_to_b[a - 1] for a in strategy_profile]).T
   safety_boosts = np.array([mapping_s_to_alpha[a - 1] for a in strategy_profile]).T

   # Define layer membership for each player
   layer_membership = np.zeros((M, N))
   for i, a in enumerate(strategy_profile):
      # First 4 strategies belong to layer 1, the rest belong to layer 2
      layer_membership[:, i] = 1 if a > 4 else 0

   # Determine leader and leader arrival time
   leader = np.argmin(isolated_research_times, axis=1)
   is_leader = np.zeros((M, N), dtype=bool)
   is_leader[np.arange(M), leader] = True
   leader_layer = layer_membership[np.arange(M), leader][:, None]
   arrival_time_leader = np.min(isolated_research_times, axis=1, keepdims=True)
   

   # Compute times until each player achieves their AI breakthrough given their strategies
   arrival_times = isolated_research_times
   on_late_layer = layer_membership != leader_layer
   on_leader_layer = layer_membership == leader_layer
   
   catch_up_times =  catch_up_fn(arrival_time_leader,
                                 research_distances,
                                 phase_1_speeds,
                                 phase_2_speeds)
   arrival_times = np.where(on_late_layer, catch_up_times, arrival_times)
   arrival_times_follower_layer = arrival_times[on_late_layer].reshape((M, 2))
   
   # Revise arrival times for players on the leading layer who are behind the other layer   
   arrival_time_follower = np.min(arrival_times_follower_layer, axis=1, keepdims=True)
   after_follower = arrival_times > arrival_time_follower
   catch_up_times_crossover = catch_up_fn(arrival_time_follower,
                                          research_distances,
                                          phase_1_speeds,
                                          phase_2_speeds)
   arrival_times = np.where(on_leader_layer & after_follower, catch_up_times_crossover, arrival_times)
   
   # Compute times until each player would achieve a safe AI given their strategies
   safe_times = research_distances
   safe_time_catch_up = catch_up_fn(arrival_time_leader,
                                    research_distances,
                                    phase_1_speeds,
                                    safety_boosts)
   safe_times = np.where(~on_leader_layer, safe_time_catch_up, safe_times)
   # Revise safe times for players on the leading layer who are behind the other layer   
   safe_times_crossover = catch_up_fn(arrival_time_follower,
                                      research_distances,
                                      phase_1_speeds,
                                      safety_boosts)
   safe_times = np.where(on_leader_layer & after_follower, safe_times_crossover, safe_times)
   
   # Record which players and layers are unsafe
   unsafe_players = safe_times > arrival_times
   unsafe_layer1 = np.any(unsafe_players[:, layer_membership[0, :] == 0], axis=1, keepdims=True)
   unsafe_layer2 = np.any(unsafe_players[:, layer_membership[0, :] == 1], axis=1, keepdims=True)
   
   # Match arrival times to the rival's arrival times for calculating market share
   rival_indices = [i+1 if i % 2 == 0 else i-1 for i in range(N)]
   arrival_times_rival = arrival_times[:, rival_indices]
   
   # Determine harms for each player
   unsafe_scenario_1 = unsafe_players & unsafe_layer1 & ~unsafe_layer2
   unsafe_scenario_2 = unsafe_players & ~unsafe_layer1 & unsafe_layer2
   unsafe_scenario_3 = unsafe_players & unsafe_layer1 & unsafe_layer2
   harms = np.zeros((M, N))
   harms = np.where(unsafe_scenario_1, 1 - p_1[:, None], harms)
   harms = np.where(unsafe_scenario_2, 1 - p_2[:, None], harms)
   harms = np.where(unsafe_scenario_3, 1 - p_1[:, None] * p_2[:, None], harms)
   
   # Given arrival times and safe times, as well as the equations for market
   # value, market share, and risk, we can now compute the payoffs.
   market_shares = market_share(arrival_times, arrival_times_rival, contestability_rates)
   market_values = market_value(benefits, discount_rates, arrival_times, research_distances)
   payoff_values = market_shares * market_values * (1 - harms)

   # Helpful assertions:
   
   # Check that it is impossible for any player to have made their breakthrough
   # in phase 1 without covering their research distance (as this must be true
   # by construction, this only happens if we have a bug in the code).
   
   assert np.all(research_distances >= np.round(arrival_time_leader * phase_1_speeds, 3))
   
   payoffs = {f"P{i+1}": payoff_values[:, i] for i in range(N)}
   return {"payoffs": payoffs,
           "arrival_times": arrival_times,
           "safe_times": safe_times}

@method(build_payoffs, "multi-race-v1")
def build_payoffs(models):
   """Payoff matrix for the Multi Race game."""
   profiles = models["profiles_filtered"]

   models["payoffs"] = {}
   models["arrival_times"] = {}
   models["safe_times"] = {}
   for profile in tqdm.tqdm(profiles):
      models["profile"] = profile
      # Build payoffs for the current profile
      data = build_payoffs_multi_race_v1(models)
      models["payoffs"][profile] = data["payoffs"]
      models["arrival_times"][profile] = data["arrival_times"]
      models["safe_times"][profile] = data["safe_times"]

   return models
